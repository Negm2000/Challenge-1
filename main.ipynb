{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67bb0a3d",
   "metadata": {},
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è (v4: ColumnTransformer)**\n",
    "\n",
    "This notebook implements a robust K-Fold Cross-Validation and Ensembling strategy. This version includes a key fix: **Selective Scaling using `ColumnTransformer`**.\n",
    "\n",
    "**Strategy:**\n",
    "1.  **Feature Engineering:** \n",
    "    * Create an `is_pirate` binary feature (1 if pirate, 0 otherwise).\n",
    "    * Include the `time` column as a feature.\n",
    "    * This results in **37 total features**.\n",
    "2.  **Selective Scaling:** Use `ColumnTransformer` to apply different scaling rules to our 37 features. This logic is applied consistently in HPO, K-Fold, and Final Prediction loops.\n",
    "    * `StandardScaler` for the 31 `joint_` features.\n",
    "    * `MinMaxScaler` for the 4 `pain_survey_` features.\n",
    "    * `MinMaxScaler` for the 1 `time` feature.\n",
    "    * `passthrough` for the 1 `is_pirate` feature.\n",
    "3.  **Hyperparameter Search:** Use Ray Tune & Optuna on a single 80/20 split to find a good set of hyperparameters (`FINAL_CONFIG`).\n",
    "4.  **K-Fold Training:** Train `K=5` models on 5 different folds, using the `FINAL_CONFIG`. Each model is trained with early stopping and saved to disk.\n",
    "5.  **Ensemble Prediction:** Load all 5 models, average their (softmax) probabilities on the test set, and aggregate these probabilities for a final, robust submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03291d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Setup & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba22088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GPU ---\n",
      "PyTorch version: 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Sklearn Imports ---\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Ray[tune] & Optuna Imports ---\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from functools import partial\n",
    "\n",
    "# --- Setup Directories & Device ---\n",
    "logs_dir = \"tensorboard\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8582ef",
   "metadata": {},
   "source": [
    "## üîÑ 2. Data Loading & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44203b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Data ---\n",
      "Loaded X_train_full (shape: (661, 160, 36)) and y_train_full (shape: (661,))\n",
      "Loaded X_test_full (shape: (1324, 160, 36))\n",
      "\n",
      "--- 2. Engineering 'is_pirate' Feature ---\n",
      "Created X_train_full_engineered (shape: (661, 160, 37))\n",
      "Created X_test_full_engineered (shape: (1324, 160, 37))\n",
      "N_FEATURES is now: 37\n",
      "\n",
      "--- 3. Calculating Class Weights ---\n",
      "Class counts (0, 1, 2): [511  94  56]\n",
      "Calculated class weights: tensor([0.0643, 0.3493, 0.5864], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Loading Data ---\")\n",
    "\n",
    "# --- Define File Paths and Features ---\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "try:\n",
    "    # Load features and labels\n",
    "    features_long_df = pd.read_csv(X_TRAIN_PATH)\n",
    "    labels_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "    X_test_long_df = pd.read_csv(X_TEST_PATH)\n",
    "    \n",
    "    # --- Define constants ---\n",
    "    N_TIMESTEPS = 160\n",
    "    JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(31)]\n",
    "    PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "    TIME_FEATURE = ['time'] # <-- MODIFICATION: Add 'time' as a feature\n",
    "    \n",
    "    FEATURES = JOINT_FEATURES + PAIN_FEATURES + TIME_FEATURE\n",
    "    N_FEATURES_ORIGINAL = len(FEATURES) # This is 31 + 4 + 1 = 36\n",
    "    LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "    N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "    # --- Reshape function ---\n",
    "    def reshape_data(df, features_list, n_timesteps):\n",
    "        df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "        data_2d = df_pivot.values\n",
    "        n_samples = data_2d.shape[0]\n",
    "        data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "        return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "    # --- Load and reshape X_train_full (36 features) ---\n",
    "    X_train_full = reshape_data(\n",
    "        features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())], \n",
    "        FEATURES, \n",
    "        N_TIMESTEPS\n",
    "    )\n",
    "    \n",
    "    # --- Load and reshape X_test (36 features) ---\n",
    "    X_test_full = reshape_data(\n",
    "        X_test_long_df, FEATURES, N_TIMESTEPS\n",
    "    )\n",
    "\n",
    "    # --- Load and prepare y_train_full ---\n",
    "    y_train_full_df = labels_df.sort_values(by='sample_index')\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(LABEL_MAPPING.keys()))\n",
    "    y_train_full = le.transform(y_train_full_df['label'])\n",
    "    \n",
    "    print(f\"Loaded X_train_full (shape: {X_train_full.shape}) and y_train_full (shape: {y_train_full.shape})\")\n",
    "    print(f\"Loaded X_test_full (shape: {X_test_full.shape})\")\n",
    "\n",
    "    # --- 2. Engineer 'is_pirate' Feature (for Train) ---\n",
    "    print(\"\\n--- 2. Engineering 'is_pirate' Feature ---\")\n",
    "    static_cols = ['sample_index', 'n_legs', 'n_hands', 'n_eyes']\n",
    "    static_df = features_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    \n",
    "    pirate_filter = (\n",
    "        (static_df['n_legs'] == 'one+peg_leg') |\n",
    "        (static_df['n_hands'] == 'one+hook_hand') |\n",
    "        (static_df['n_eyes'] == 'one+eye_patch')\n",
    "    )\n",
    "    pirate_indices = static_df[pirate_filter].index\n",
    "    sample_indices_ordered = sorted(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())]['sample_index'].unique())\n",
    "    is_pirate_map = np.array([1 if idx in pirate_indices else 0 for idx in sample_indices_ordered])\n",
    "    pirate_feature_broadcast = np.tile(is_pirate_map.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    \n",
    "    # Concatenate with X_train_full\n",
    "    X_train_full_engineered = np.concatenate([X_train_full, pirate_feature_broadcast], axis=2)\n",
    "    \n",
    "    # --- 3. Engineer 'is_pirate' Feature (for Test) ---\n",
    "    static_df_test = X_test_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    pirate_filter_test = (\n",
    "        (static_df_test['n_legs'] == 'one+peg_leg') |\n",
    "        (static_df_test['n_hands'] == 'one+hook_hand') |\n",
    "        (static_df_test['n_eyes'] == 'one+eye_patch')\n",
    "    )\n",
    "    pirate_indices_test = static_df_test[pirate_filter_test].index\n",
    "    sample_indices_test_ordered = sorted(X_test_long_df['sample_index'].unique())\n",
    "    is_pirate_map_test = np.array([1 if idx in pirate_indices_test else 0 for idx in sample_indices_test_ordered])\n",
    "    pirate_feature_broadcast_test = np.tile(is_pirate_map_test.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    \n",
    "    # Concatenate with X_test_full\n",
    "    X_test_full_engineered = np.concatenate([X_test_full, pirate_feature_broadcast_test], axis=2)\n",
    "    \n",
    "    N_FEATURES_NEW = X_train_full_engineered.shape[2] # This will be 36 + 1 = 37\n",
    "    print(f\"Created X_train_full_engineered (shape: {X_train_full_engineered.shape})\")\n",
    "    print(f\"Created X_test_full_engineered (shape: {X_test_full_engineered.shape})\")\n",
    "    print(f\"N_FEATURES is now: {N_FEATURES_NEW}\")\n",
    "\n",
    "    # --- 4. Calculate Class Weights ---\n",
    "    print(\"\\n--- 3. Calculating Class Weights ---\")\n",
    "    class_counts_series = labels_df['label'].value_counts()\n",
    "    counts_ordered = class_counts_series.reindex(LABEL_MAPPING.keys()).values\n",
    "    class_weights_tensor = 1.0 / torch.tensor(counts_ordered, dtype=torch.float)\n",
    "    class_weights_tensor = class_weights_tensor / class_weights_tensor.sum() # Normalize weights\n",
    "    class_weights_tensor = class_weights_tensor.to(device)\n",
    "    \n",
    "    print(f\"Class counts (0, 1, 2): {counts_ordered}\")\n",
    "    print(f\"Calculated class weights: {class_weights_tensor}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find a required file. {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f8bc4",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a7c2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(X_3d, y=None, window_size=100, stride=20):\n",
    "    \"\"\"\n",
    "    Takes 3D data (n_samples, n_timesteps, n_features)\n",
    "    and creates overlapping windows.\n",
    "    \"\"\"\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    # This new array tracks which original sample each window came from.\n",
    "    window_indices = [] \n",
    "    \n",
    "    n_samples, n_timesteps, n_features = X_3d.shape\n",
    "    \n",
    "    # Iterate over each original sample\n",
    "    for i in range(n_samples):\n",
    "        sample = X_3d[i]\n",
    "        \n",
    "        # Slide a window over this sample\n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            window = sample[idx : idx + window_size]\n",
    "            new_X.append(window)\n",
    "            window_indices.append(i) # Track the original sample index (0, 1, 2...)\n",
    "            \n",
    "            if y is not None:\n",
    "                new_y.append(y[i]) # The label is the same for all windows\n",
    "                \n",
    "            idx += stride\n",
    "            \n",
    "    if y is not None:\n",
    "        # Return new X, new y, and the index mapping\n",
    "        return np.array(new_X), np.array(new_y), np.array(window_indices)\n",
    "    else:\n",
    "        # Return new X and the index mapping\n",
    "        return np.array(new_X), np.array(window_indices)\n",
    "    \n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    \"\"\"Creates a PyTorch DataLoader with optimized settings.\"\"\"\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=int(batch_size), # Ensure batch_size is an int\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=None,\n",
    "    )\n",
    "\n",
    "def recurrent_summary(model, input_size):\n",
    "    \"\"\"Custom summary function that correctly counts parameters for RNN/GRU/LSTM layers.\"\"\"\n",
    "    output_shapes = {}\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1  # Replace batch dimension with -1\n",
    "\n",
    "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
    "                    shape2 = list(output[1][0].shape)\n",
    "                else:  # RNN/GRU case: h_n only\n",
    "                    shape2 = list(output[1].shape)\n",
    "                shape2[1] = -1\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "        return hook\n",
    "\n",
    "    try:\n",
    "        device_summary = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device_summary = torch.device(\"cpu\")\n",
    "\n",
    "    dummy_input = torch.randn(1, *input_size).to(device_summary)\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(dummy_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faed930",
   "metadata": {},
   "source": [
    "## üß† 4. Model & Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3cadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic RNN classifier (RNN, LSTM, GRU).\n",
    "    Uses the last hidden state for classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "\n",
    "        # Dropout is only applied between layers (if num_layers > 1)\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        if self.bidirectional:\n",
    "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
    "        else:\n",
    "            classifier_input_size = hidden_size\n",
    "\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" x shape: (batch_size, seq_length, input_size) \"\"\"\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0] # Use only the hidden state, not the cell state\n",
    "\n",
    "        # Get the last layer's hidden state\n",
    "        if self.bidirectional:\n",
    "            # Reshape to (num_layers, num_directions, batch, hidden_size)\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "            # Concat the last fwd and bwd hidden states\n",
    "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
    "        else:\n",
    "            # Just take the last layer's hidden state\n",
    "            hidden_to_classify = hidden[-1]\n",
    "\n",
    "        logits = self.classifier(hidden_to_classify)\n",
    "        return logits\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            \n",
    "            # Add L1/L2 regularization if provided\n",
    "            if l1_lambda > 0 or l2_lambda > 0:\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "                loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Unscale gradients before clipping\n",
    "        scaler.unscale_(optimizer) \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5) \n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Warning: NaN loss detected in batch {batch_idx}. Skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    if not all_targets:\n",
    "        return 0.0, 0.0 # Return 0 if all batches were nan\n",
    "\n",
    "    epoch_loss = running_loss / len(np.concatenate(all_targets))\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset.tensors[1])\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
    "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "\n",
    "def objective_function_cv(config, X_full_engineered, y_full, class_weights_tensor):\n",
    "    \"\"\"\n",
    "    Robust Objective: Runs 3-Fold CV for EVERY trial.\n",
    "    Reports the AVERAGE validation F1 across folds to Ray Tune.\n",
    "    \"\"\"\n",
    "    # Define CV strategy inside the trial (e.g., 3-Fold is usually enough for HPO)\n",
    "    N_HPO_FOLDS = 3 \n",
    "    skf = StratifiedKFold(n_splits=N_HPO_FOLDS, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # Prepare data loaders for all folds UP FRONT to save time in the loop\n",
    "    fold_loaders = []\n",
    "    \n",
    "    # --- MODIFICATION: Define feature indices based on our 37 features ---\n",
    "    # X_full_engineered has 37 features: \n",
    "    # 0-30: joint_ (31 features) -> StandardScaler\n",
    "    # 31-34: pain_ (4 features) -> MinMaxScaler\n",
    "    # 35: time (1 feature) -> MinMaxScaler\n",
    "    # 36: is_pirate (1 feature) -> passthrough\n",
    "    joint_indices = list(range(31))\n",
    "    pain_indices = list(range(31, 35))\n",
    "    time_index = [35]\n",
    "    pirate_index = [36]\n",
    "    \n",
    "    # We need to scale inside the folds to avoid leakage\n",
    "    for train_idx, val_idx in skf.split(X_full_engineered, y_full):\n",
    "        \n",
    "        # 1. Split\n",
    "        X_train_fold = X_full_engineered[train_idx]\n",
    "        y_train_fold = y_full[train_idx]\n",
    "        X_val_fold = X_full_engineered[val_idx]\n",
    "        y_val_fold = y_full[val_idx]\n",
    "        \n",
    "        # 2. --- MODIFICATION: Selective Scale using ColumnTransformer ---\n",
    "        preprocessor_fold = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('joint_scaler', StandardScaler(), joint_indices),\n",
    "                ('pain_scaler', MinMaxScaler(), pain_indices),\n",
    "                ('time_scaler', MinMaxScaler(), time_index),\n",
    "                ('pirate_passthrough', 'passthrough', pirate_index)\n",
    "            ],\n",
    "            remainder='drop' \n",
    "        )\n",
    "        \n",
    "        # Reshape to 2D for fitting/transforming\n",
    "        ns, ts, f = X_train_fold.shape\n",
    "        X_train_2d = X_train_fold.reshape(ns * ts, f)\n",
    "        ns_val, ts_val, f_val = X_val_fold.shape\n",
    "        X_val_2d = X_val_fold.reshape(ns_val * ts_val, f_val)\n",
    "\n",
    "        # Fit on 2D train data\n",
    "        preprocessor_fold.fit(X_train_2d)\n",
    "\n",
    "        # Transform 2D train and val data\n",
    "        X_train_scaled_2d = preprocessor_fold.transform(X_train_2d)\n",
    "        X_val_scaled_2d = preprocessor_fold.transform(X_val_2d)\n",
    "        \n",
    "        # Reshape back to 3D\n",
    "        # The feature dimension (-1) will correctly be 37\n",
    "        X_train_final = X_train_scaled_2d.reshape(ns, ts, -1) \n",
    "        X_val_final = X_val_scaled_2d.reshape(ns_val, ts_val, -1)\n",
    "        # --- End of Scaling Modification ---\n",
    "        \n",
    "        # 3. Windowing\n",
    "        X_train_w, y_train_w, _ = create_sliding_windows(\n",
    "            X_train_final, y_train_fold, config[\"window_size\"], config[\"stride\"]\n",
    "        )\n",
    "        X_val_w, y_val_w, _ = create_sliding_windows(\n",
    "            X_val_final, y_val_fold, config[\"window_size\"], config[\"stride\"]\n",
    "        )\n",
    "        \n",
    "        # 4. Loaders\n",
    "        train_ds = TensorDataset(torch.from_numpy(X_train_w).float(), torch.from_numpy(y_train_w).long())\n",
    "        val_ds = TensorDataset(torch.from_numpy(X_val_w).float(), torch.from_numpy(y_val_w).long())\n",
    "        \n",
    "        t_loader = make_loader(train_ds, config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "        v_loader = make_loader(val_ds, config[\"batch_size\"], shuffle=False, drop_last=False)\n",
    "        \n",
    "        fold_loaders.append((t_loader, v_loader))\n",
    "\n",
    "    # Initialize K models and K optimizers\n",
    "    models = []\n",
    "    optimizers = []\n",
    "    scalers = []\n",
    "    \n",
    "    # MODIFICATION: Get input_size from the data\n",
    "    INPUT_SIZE = X_full_engineered.shape[2] # This will be 37\n",
    "\n",
    "    for _ in range(N_HPO_FOLDS):\n",
    "        model = RecurrentClassifier(\n",
    "            input_size=INPUT_SIZE, # <-- MODIFICATION\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            num_classes=3,\n",
    "            dropout_rate=config[\"dropout_rate\"],\n",
    "            bidirectional=config[\"bidirectional\"],\n",
    "            rnn_type=config[\"rnn_type\"]\n",
    "        ).to(device)\n",
    "        \n",
    "        if torch.__version__[0] >= \"2\": model = torch.compile(model)\n",
    "        \n",
    "        optim = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "        scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "        \n",
    "        models.append(model)\n",
    "        optimizers.append(optim)\n",
    "        scalers.append(scaler)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    EPOCHS = 100 # Reduce epochs slightly since we are doing 3x the work\n",
    "    \n",
    "    # --- The Parallel Training Loop ---\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        \n",
    "        fold_val_f1s = []\n",
    "        fold_train_losses = []\n",
    "        \n",
    "        # Train each fold for 1 epoch\n",
    "        for i in range(N_HPO_FOLDS):\n",
    "            train_loader, val_loader = fold_loaders[i]\n",
    "            model = models[i]\n",
    "            optimizer = optimizers[i]\n",
    "            scaler = scalers[i]\n",
    "            \n",
    "            t_loss, _ = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "            _, v_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "            \n",
    "            fold_train_losses.append(t_loss)\n",
    "            fold_val_f1s.append(v_f1)\n",
    "        \n",
    "        # Calculate AVERAGE metrics across the 3 folds\n",
    "        avg_val_f1 = np.mean(fold_val_f1s)\n",
    "        avg_train_loss = np.mean(fold_train_losses)\n",
    "        \n",
    "        # Report the AVERAGE to Ray Tune\n",
    "        # If the config is bad on *any* fold, the average drops, and ASHA kills it.\n",
    "        tune.report({\n",
    "            \"val_f1\": avg_val_f1,\n",
    "            \"train_loss\": avg_train_loss\n",
    "        })\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Full training loop with early stopping, model checkpointing, and logging.\n",
    "    \"\"\"\n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    print(f\"Will train for {epochs} epochs with patience={patience} monitoring {evaluation_metric}\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
    "            )\n",
    "\n",
    "        if verbose > 0 and (epoch % verbose == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                  f\"Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n",
    "                  f\"Val: Loss={val_loss:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    if restore_best_weights and patience > 0:\n",
    "        print(f\"Restoring best model from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    if patience == 0:\n",
    "        print(\"Training complete. Saving final model.\")\n",
    "        torch.save(model.state_dict(), model_path.replace(\"_best_model.pt\", \"_final_model.pt\"))\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    \n",
    "    print(f\"--- Finished Training: {experiment_name} ---\")\n",
    "    return model, training_history, best_epoch if 'best_epoch' in locals() else epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f911c",
   "metadata": {},
   "source": [
    "## üß™ 5. Phase 1: Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa548a9",
   "metadata": {},
   "source": [
    "### 5.1. Preprocessing for HPO (ColumnTransformer)\n",
    "\n",
    "This section prepares a single 80/20 split. This split is used by the *original* (now deprecated) single-split HPO function. The *current* HPO function `objective_function_cv` ignores this and performs its own K-Fold splitting and scaling.\n",
    "\n",
    "We update the logic here for consistency, but it is not critical to the `objective_function_cv` workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09a835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Splitting NON-WINDOWED data for HPO ---\n",
      "  X_train_split_full: (528, 160, 37)\n",
      "  X_val_split_full:   (133, 160, 37)\n",
      "\n",
      "--- Applying Selective Scaling for HPO ---\n",
      "Fitted preprocessor on training data shape: (84480, 37)\n",
      "  X_train_full_scaled (final): (528, 160, 37)\n",
      "  X_val_full_scaled (final):   (133, 160, 37)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Split Data (NON-WINDOWED) ---\n",
    "print(\"--- Splitting NON-WINDOWED data for HPO ---\")\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "\n",
    "for train_idx, val_idx in sss.split(X_train_full_engineered, y_train_full):\n",
    "    X_train_split_full = X_train_full_engineered[train_idx]\n",
    "    y_train_split_full = y_train_full[train_idx]\n",
    "    X_val_split_full = X_train_full_engineered[val_idx]\n",
    "    y_val_split_full = y_train_full[val_idx]\n",
    "\n",
    "print(f\"  X_train_split_full: {X_train_split_full.shape}\")\n",
    "print(f\"  X_val_split_full:   {X_val_split_full.shape}\")\n",
    "\n",
    "# --- 2. Scale Features (SELECTIVELY) ---\n",
    "print(\"\\n--- Applying Selective Scaling for HPO ---\")\n",
    "\n",
    "# --- MODIFICATION: Define feature indices based on our 37 features ---\n",
    "joint_indices = list(range(31))\n",
    "pain_indices = list(range(31, 35))\n",
    "time_index = [35]\n",
    "pirate_index = [36]\n",
    "\n",
    "# 1. Create the preprocessor for HPO\n",
    "preprocessor_hpo = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('joint_scaler', StandardScaler(), joint_indices),\n",
    "        ('pain_scaler', MinMaxScaler(), pain_indices),\n",
    "        ('time_scaler', MinMaxScaler(), time_index),\n",
    "        ('pirate_passthrough', 'passthrough', pirate_index)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# 2. Fit preprocessor ONLY on 2D-reshaped training data\n",
    "ns, ts, f = X_train_split_full.shape\n",
    "X_train_2d = X_train_split_full.reshape(ns * ts, f)\n",
    "preprocessor_hpo.fit(X_train_2d)\n",
    "print(f\"Fitted preprocessor on training data shape: {X_train_2d.shape}\")\n",
    "\n",
    "# 3. Transform 2D data for train and val\n",
    "X_train_scaled_2d = preprocessor_hpo.transform(X_train_2d)\n",
    "\n",
    "ns_val, ts_val, f_val = X_val_split_full.shape\n",
    "X_val_2d = X_val_split_full.reshape(ns_val * ts_val, f_val)\n",
    "X_val_scaled_2d = preprocessor_hpo.transform(X_val_2d)\n",
    "\n",
    "# 4. Reshape back to 3D\n",
    "X_train_full_scaled = X_train_scaled_2d.reshape(ns, ts, -1)\n",
    "X_val_full_scaled = X_val_scaled_2d.reshape(ns_val, ts_val, -1)\n",
    "\n",
    "print(f\"  X_train_full_scaled (final): {X_train_full_scaled.shape}\")\n",
    "print(f\"  X_val_full_scaled (final):   {X_val_full_scaled.shape}\")\n",
    "\n",
    "# Verify the pirate feature is still 0/1 (it's now the last feature)\n",
    "# print(f\"Min/Max of pirate feature in scaled train: {X_train_full_scaled[:, :, -1].min()}, {X_train_full_scaled[:, :, -1].max()}\")\n",
    "\n",
    "# Clean up\n",
    "del X_train_2d, X_train_scaled_2d, X_val_2d, X_val_scaled_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526ea0b",
   "metadata": {},
   "source": [
    "### 5.2. HPO Search Execution (Ray Tune + Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42797c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-12 19:05:15</td></tr>\n",
       "<tr><td>Running for: </td><td>00:31:56.08        </td></tr>\n",
       "<tr><td>Memory:      </td><td>11.9/13.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=1<br>Bracket: Iter 80.000: 0.9148854766674107 | Iter 40.000: 0.9136399986472679 | Iter 20.000: 0.9112018391982808<br>Logical resource usage: 8.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th>bidirectional  </th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  l2_lambda</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th>rnn_type  </th><th style=\"text-align: right;\">  stride</th><th style=\"text-align: right;\">  window_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_f1</th><th style=\"text-align: right;\">  train_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_function_cv_0cfcb68d</td><td>RUNNING   </td><td>127.0.0.1:32588</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.597867</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.000315614</td><td style=\"text-align: right;\">0.00152823 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">           20</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">        1877.32 </td><td style=\"text-align: right;\">0.917656</td><td style=\"text-align: right;\"> 2.26439e-05</td></tr>\n",
       "<tr><td>objective_function_cv_65e3d8db</td><td>RUNNING   </td><td>127.0.0.1:4280 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.191544</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000920306</td><td style=\"text-align: right;\">0.00216957 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         974.61 </td><td style=\"text-align: right;\">0.912124</td><td style=\"text-align: right;\"> 0.00322384 </td></tr>\n",
       "<tr><td>objective_function_cv_341cc2b2</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.480971</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">4.2443e-05 </td><td style=\"text-align: right;\">0.000180603</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>objective_function_cv_c572debd</td><td>TERMINATED</td><td>127.0.0.1:33480</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.32484 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.75638e-05</td><td style=\"text-align: right;\">2.11919e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         880.674</td><td style=\"text-align: right;\">0.908057</td><td style=\"text-align: right;\"> 0.0100722  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 19:05:15,979\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-11-12 19:05:15,993\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/Karim Negm/ray_results/pirate_pain_robust_cv_search_v4' in 0.0126s.\n",
      "2025-11-12 19:05:26,827\tINFO tune.py:1041 -- Total run time: 1926.98 seconds (1916.07 seconds for the tuning loop).\n",
      "2025-11-12 19:05:26,828\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2025-11-12 19:05:26,927\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- objective_function_cv_341cc2b2: FileNotFoundError('Could not fetch metrics for objective_function_cv_341cc2b2: both result.json and progress.csv were not found at C:/Users/Karim Negm/ray_results/pirate_pain_robust_cv_search_v4/objective_function_cv_341cc2b2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Search Complete ---\n",
      "\n",
      "Getting best trial from analysis...\n",
      "Best validation F1 score: 0.9177\n",
      "Best hyperparameters found:\n",
      "{'window_size': 20, 'stride': 2, 'rnn_type': 'GRU', 'lr': 0.0015282292191831153, 'batch_size': 64, 'hidden_size': 128, 'num_layers': 3, 'dropout_rate': 0.5978665068940733, 'bidirectional': True, 'l2_lambda': 0.0003156144864649396}\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define the Search Space for Optuna --\n",
    "search_space = {\n",
    "    # Windowing params\n",
    "    \"window_size\": tune.choice([5, 10, 20]),\n",
    "    \"stride\": tune.choice([1, 2, 5]),\n",
    "    \n",
    "    # Model params\n",
    "    \"rnn_type\": tune.choice(['GRU']),\n",
    "    \"lr\": tune.loguniform(1e-5, 5e-3),\n",
    "    \"batch_size\": tune.choice([64, 128, 256]),  \n",
    "    \"hidden_size\": tune.choice([128, 256, 384]),\n",
    "    \"num_layers\": tune.choice([2, 3]),\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.6),\n",
    "    \"bidirectional\": tune.choice([True, False]),\n",
    "    \"l2_lambda\": tune.loguniform(1e-7, 1e-3) # This is weight_decay in AdamW\n",
    "}\n",
    "# --- 2. Define the Optimizer (Optuna) and Scheduler (ASHA) ---\n",
    "optuna_search = OptunaSearch(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\",\n",
    "    grace_period=20,  # Min epochs a trial must run\n",
    "    reduction_factor=2  # How aggressively to stop trials\n",
    ")\n",
    "\n",
    "# --- 3. Initialize Ray ---\n",
    "# Shutdown previous sessions if any (helps in notebooks)\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "ray_logs_path = os.path.abspath(\"./ray_results\")\n",
    "os.makedirs(ray_logs_path, exist_ok=True)\n",
    "os.environ[\"RAY_TEMP_DIR\"] = ray_logs_path\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=16, \n",
    "    num_gpus=1, \n",
    "    ignore_reinit_error=True,\n",
    "    log_to_driver=False # Suppress logs in notebook\n",
    ")\n",
    "\n",
    "def short_trial_name(trial):\n",
    "    \"\"\"Creates a short, unique name for each trial folder.\"\"\"\n",
    "    return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "\n",
    "# --- 4. Run the Tuner --\n",
    "print(\"Starting hyperparameter search...\")\n",
    "\n",
    "# Use tune.with_parameters to pass our NON-WINDOWED, *UNSCALED* numpy arrays\n",
    "# and the class weights to the objective function\n",
    "objective_with_data = tune.with_parameters(\n",
    "    objective_function_cv, \n",
    "    X_full_engineered=X_train_full_engineered, # The full (661, 160, 37) array\n",
    "    y_full=y_train_full,                       # The full labels\n",
    "    class_weights_tensor=class_weights_tensor\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    objective_with_data,\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 0.5}, \n",
    "    config=search_space,\n",
    "    num_samples=20, \n",
    "    search_alg=optuna_search,\n",
    "    scheduler=scheduler,\n",
    "    name=\"pirate_pain_robust_cv_search_v4\",\n",
    "    trial_dirname_creator=short_trial_name,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n--- Search Complete ---\\n\")\n",
    "\n",
    "# --- 5. Get Best Results ---\n",
    "print(\"Getting best trial from analysis...\")\n",
    "best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "if best_trial:\n",
    "    FINAL_CONFIG = best_trial.config\n",
    "    FINAL_BEST_VAL_F1 = best_trial.last_result[\"val_f1\"]\n",
    "    \n",
    "    print(f\"Best validation F1 score: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(FINAL_CONFIG)\n",
    "else:\n",
    "    print(\"ERROR: No trials completed successfully. Using a default config.\")\n",
    "    # Fallback config in case HPO fails\n",
    "    FINAL_CONFIG = {\n",
    "        'window_size': 10, 'stride': 10, 'rnn_type': 'GRU', 'lr': 0.0005,\n",
    "        'batch_size': 64, 'hidden_size': 256, 'num_layers': 3,\n",
    "        'dropout_rate': 0.5, 'bidirectional': True, 'l2_lambda': 1e-06\n",
    "    }\n",
    "    FINAL_BEST_VAL_F1 = 0.0\n",
    "\n",
    "# Clean up HPO data\n",
    "del X_train_full_scaled, y_train_split_full, X_val_full_scaled, y_val_split_full, preprocessor_hpo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c0bb8",
   "metadata": {},
   "source": [
    "## üèÜ 6. Phase 2: K-Fold Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13308e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# --- üèÜ FINAL MODEL CONFIGURATION üèÜ ---\n",
    "# ===================================================================\n",
    "print(\"--- üèÜ Final Configuration Set --- \")\n",
    "print(f\"Best Val F1 from HPO search: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "print(FINAL_CONFIG)\n",
    "\n",
    "# --- Set variables for the K-Fold & submission cells ---\n",
    "FINAL_MODEL_TYPE = FINAL_CONFIG[\"rnn_type\"]\n",
    "FINAL_HIDDEN_SIZE = FINAL_CONFIG[\"hidden_size\"]\n",
    "FINAL_HIDDEN_LAYERS = FINAL_CONFIG[\"num_layers\"]\n",
    "FINAL_BIDIRECTIONAL = FINAL_CONFIG[\"bidirectional\"]\n",
    "FINAL_DROPOUT_RATE = FINAL_CONFIG[\"dropout_rate\"]\n",
    "FINAL_LEARNING_RATE = FINAL_CONFIG[\"lr\"]\n",
    "FINAL_L2_LAMBDA = FINAL_CONFIG[\"l2_lambda\"]\n",
    "FINAL_BATCH_SIZE = FINAL_CONFIG[\"batch_size\"]\n",
    "FINAL_WINDOW_SIZE = FINAL_CONFIG[\"window_size\"]\n",
    "FINAL_STRIDE = FINAL_CONFIG[\"stride\"]\n",
    "N_SPLITS = 5 # Number of folds\n",
    "\n",
    "FINAL_EXPERIMENT_NAME = f\"{FINAL_MODEL_TYPE}_H{FINAL_HIDDEN_SIZE}_L{FINAL_HIDDEN_LAYERS}_B{FINAL_BIDIRECTIONAL}_Optuna_KFold_Ensemble_v4\"\n",
    "submission_filename_base = f\"submission_{FINAL_EXPERIMENT_NAME}_w{FINAL_WINDOW_SIZE}_s{FINAL_STRIDE}.csv\"\n",
    "print(f\"Submission name will be: {submission_filename_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED) \n",
    "print(f\"--- Starting {N_SPLITS}-Fold CV Training ---\")\n",
    "print(f\"Splitting original engineered data: {X_train_full_engineered.shape}\")\n",
    "print(f\"Using Class Weights: {class_weights_tensor.cpu().numpy()}\")\n",
    "\n",
    "fold_val_f1_list = []\n",
    "\n",
    "# --- MODIFICATION: Define feature indices based on our 37 features ---\n",
    "joint_indices = list(range(31))\n",
    "pain_indices = list(range(31, 35))\n",
    "time_index = [35]\n",
    "pirate_index = [36]\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_full_engineered, y_train_full)):\n",
    "    fold_name = f\"kfold_fold_{fold+1}\"\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} --- ({fold_name}) ---\")\n",
    "    \n",
    "    X_train_fold_full = X_train_full_engineered[train_idx]\n",
    "    y_train_fold_full = y_train_full[train_idx]\n",
    "    X_val_fold_full = X_train_full_engineered[val_idx]\n",
    "    y_val_fold_full = y_train_full[val_idx]\n",
    "\n",
    "    # --- MODIFICATION: Scale INSIDE the fold (SELECTIVELY) ---\n",
    "    preprocessor_fold = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('joint_scaler', StandardScaler(), joint_indices),\n",
    "            ('pain_scaler', MinMaxScaler(), pain_indices),\n",
    "            ('time_scaler', MinMaxScaler(), time_index),\n",
    "            ('pirate_passthrough', 'passthrough', pirate_index)\n",
    "        ],\n",
    "        remainder='drop' \n",
    "    )\n",
    "    \n",
    "    # 1. Reshape to 2D for fitting/transforming\n",
    "    ns, ts, f = X_train_fold_full.shape\n",
    "    X_train_2d = X_train_fold_full.reshape(ns * ts, f)\n",
    "    ns_val, ts_val, f_val = X_val_fold_full.shape\n",
    "    X_val_2d = X_val_fold_full.reshape(ns_val * ts_val, f_val)\n",
    "\n",
    "    # 2. Fit preprocessor ONLY on 2D-reshaped training data\n",
    "    preprocessor_fold.fit(X_train_2d)\n",
    "\n",
    "    # 3. Transform 2D data for train and val\n",
    "    X_train_scaled_2d = preprocessor_fold.transform(X_train_2d)\n",
    "    X_val_scaled_2d = preprocessor_fold.transform(X_val_2d)\n",
    "\n",
    "    # 4. Reshape back to 3D\n",
    "    X_train_fold_scaled = X_train_scaled_2d.reshape(ns, ts, -1)\n",
    "    X_val_fold_scaled = X_val_scaled_2d.reshape(ns_val, ts_val, -1)\n",
    "    # --- End of Scaling Modification ---\n",
    "\n",
    "    # --- Create Sliding Windows (POST-SPLIT) ---\n",
    "    X_train_w, y_train_w, _ = create_sliding_windows(\n",
    "        X_train_fold_scaled, y_train_fold_full, \n",
    "        window_size=FINAL_WINDOW_SIZE, stride=FINAL_STRIDE\n",
    "    )\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(\n",
    "        X_val_fold_scaled, y_val_fold_full, \n",
    "        window_size=FINAL_WINDOW_SIZE, stride=FINAL_STRIDE\n",
    "    )\n",
    "    print(f\"  Fold Train Windows: {X_train_w.shape}, Fold Val Windows: {X_val_w.shape}\")\n",
    "\n",
    "    # --- Create Tensors, datasets and dataloaders --\n",
    "    X_train_fold = torch.from_numpy(X_train_w).float()\n",
    "    y_train_fold = torch.from_numpy(y_train_w).long()\n",
    "    X_val_fold = torch.from_numpy(X_val_w).float()\n",
    "    y_val_fold = torch.from_numpy(y_val_w).long()\n",
    "    train_ds_fold = TensorDataset(X_train_fold, y_train_fold)\n",
    "    val_ds_fold = TensorDataset(X_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader_fold = make_loader(train_ds_fold, batch_size=FINAL_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    val_loader_fold = make_loader(val_ds_fold, batch_size=FINAL_BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "    \n",
    "    # --- Create a fresh model (using FINAL_CONFIG) ---\n",
    "    model_fold = RecurrentClassifier(\n",
    "        input_size=N_FEATURES_NEW, # This is 37\n",
    "        hidden_size=FINAL_HIDDEN_SIZE, num_layers=FINAL_HIDDEN_LAYERS,\n",
    "        num_classes=N_CLASSES, dropout_rate=FINAL_DROPOUT_RATE,\n",
    "        bidirectional=FINAL_BIDIRECTIONAL, rnn_type=FINAL_MODEL_TYPE\n",
    "    ).to(device)\n",
    "    \n",
    "    if torch.__version__[0] >= \"2\": model_fold = torch.compile(model_fold)\n",
    "    optimizer_fold = torch.optim.AdamW(model_fold.parameters(), lr=FINAL_LEARNING_RATE, weight_decay=FINAL_L2_LAMBDA)\n",
    "    scaler_fold_amp = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion_fold = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    # --- Train this fold with early stopping ---\n",
    "    model_fold, _, _ = fit(\n",
    "        model=model_fold, train_loader=train_loader_fold,\n",
    "        val_loader=val_loader_fold, epochs=300,\n",
    "        criterion=criterion_fold, optimizer=optimizer_fold,\n",
    "        scaler=scaler_fold_amp, device=device,\n",
    "        writer=None, verbose=25,\n",
    "        experiment_name=fold_name, patience=30\n",
    "    )\n",
    "    \n",
    "    val_loss, val_f1 = validate_one_epoch(model_fold, val_loader_fold, criterion_fold, device)\n",
    "    fold_val_f1_list.append(val_f1)\n",
    "    print(f\"Fold {fold+1} Best Model Val F1: {val_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n--- üèÜ K-Fold Training Complete ---\")\n",
    "print(f\"Fold F1 scores: {[round(f, 4) for f in fold_val_f1_list]}\")\n",
    "print(f\"Average F1 across folds: {np.mean(fold_val_f1_list):.4f}\")\n",
    "\n",
    "# Clean up\n",
    "del X_train_fold, y_train_fold, X_val_fold, y_val_fold\n",
    "del X_train_w, y_train_w, X_val_w, y_val_w\n",
    "del X_train_fold_full, y_train_fold_full, X_val_fold_full, y_val_fold_full\n",
    "del X_train_2d, X_train_scaled_2d, X_val_2d, X_val_scaled_2d\n",
    "del X_train_fold_scaled, X_val_fold_scaled, preprocessor_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9d380",
   "metadata": {},
   "source": [
    "## üì¨ 7. Phase 3: Ensemble Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31871e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Preparing full dataset for FINAL PREPROCESSOR ---\")\n",
    "\n",
    "# --- 1. Prepare Final Preprocessor (Fit on ALL training data) ---\n",
    "\n",
    "# --- MODIFICATION: Define feature indices based on our 37 features ---\n",
    "joint_indices = list(range(31))\n",
    "pain_indices = list(range(31, 35))\n",
    "time_index = [35]\n",
    "pirate_index = [36]\n",
    "\n",
    "preprocessor_final = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('joint_scaler', StandardScaler(), joint_indices),\n",
    "        ('pain_scaler', MinMaxScaler(), pain_indices),\n",
    "        ('time_scaler', MinMaxScaler(), time_index),\n",
    "        ('pirate_passthrough', 'passthrough', pirate_index)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# 2. Fit preprocessor ONLY on 2D-reshaped ALL training data\n",
    "ns, ts, f = X_train_full_engineered.shape\n",
    "X_train_full_2d = X_train_full_engineered.reshape(ns * ts, f)\n",
    "preprocessor_final.fit(X_train_full_2d)\n",
    "print(f\"Fitted FINAL preprocessor on all training data shape: {X_train_full_2d.shape}\")\n",
    "\n",
    "# --- 2. Prepare, Scale (Selectively), and Window the TEST data ---\n",
    "print(\"\\n--- Preparing Test Set (Selective Scaling) ---\")\n",
    "\n",
    "# 1. Reshape test data to 2D\n",
    "ns_test, ts_test, f_test = X_test_full_engineered.shape\n",
    "X_test_2d = X_test_full_engineered.reshape(ns_test * ts_test, f_test)\n",
    "\n",
    "# 2. Transform 2D test data\n",
    "X_test_scaled_2d = preprocessor_final.transform(X_test_2d)\n",
    "\n",
    "# 3. Reshape back to 3D\n",
    "X_test_final_scaled = X_test_scaled_2d.reshape(ns_test, ts_test, -1)\n",
    "print(f\"Created final scaled test set (shape: {X_test_final_scaled.shape})\")\n",
    "\n",
    "# --- 3. Apply Sliding Windows ---\n",
    "print(\"--- Applying sliding windows to final test set ---\")\n",
    "X_test_final_windowed, test_window_indices = create_sliding_windows(\n",
    "    X_test_final_scaled, y=None, \n",
    "    window_size=FINAL_WINDOW_SIZE, stride=FINAL_STRIDE\n",
    ")\n",
    "print(f\"Test windowed shape: {X_test_final_windowed.shape}\")\n",
    "\n",
    "# --- 4. Create Final TestLoader ---\n",
    "final_test_features = torch.from_numpy(X_test_final_windowed).float()\n",
    "final_test_ds = TensorDataset(final_test_features)\n",
    "test_loader = make_loader(final_test_ds, batch_size=FINAL_BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "print(\"Final TestLoader created.\")\n",
    "\n",
    "# --- 5. Get Predictions from all K-Fold Models ---\n",
    "all_fold_probabilities = []\n",
    "print(f\"\\n--- Generating predictions from {N_SPLITS} fold models ---\")\n",
    "\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_name = f\"kfold_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    print(f\"Loading model {fold+1}/{N_SPLITS} from {model_path}...\")\n",
    "\n",
    "    # Create a fresh model shell\n",
    "    model_fold = RecurrentClassifier(\n",
    "        input_size=N_FEATURES_NEW, # This is 37\n",
    "        hidden_size=FINAL_HIDDEN_SIZE, num_layers=FINAL_HIDDEN_LAYERS,\n",
    "        num_classes=N_CLASSES, dropout_rate=FINAL_DROPOUT_RATE,\n",
    "        bidirectional=FINAL_BIDIRECTIONAL, rnn_type=FINAL_MODEL_TYPE\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load the saved weights (with compile-fix)\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    # Remove the '_orig_mod.' prefix if model was compiled\n",
    "    new_state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "    model_fold.load_state_dict(new_state_dict)\n",
    "    model_fold.eval()\n",
    "\n",
    "    # Get Softmax probabilities\n",
    "    fold_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in test_loader: \n",
    "            inputs = inputs.to(device)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model_fold(inputs)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                fold_predictions.append(probs.cpu().numpy())\n",
    "    all_fold_probabilities.append(np.concatenate(fold_predictions))\n",
    "\n",
    "# --- 6. Average the Probabilities ---\n",
    "print(f\"\\n--- Averaging {len(all_fold_probabilities)} sets of probabilities... ---\")\n",
    "mean_probabilities = np.mean(all_fold_probabilities, axis=0)\n",
    "print(f\"Mean probability matrix shape: {mean_probabilities.shape}\")\n",
    "\n",
    "# --- 7. Aggregate Mean Probabilities (MEAN) ---\n",
    "print(\"Aggregating window probabilities to sample predictions (using MEAN)...\")\n",
    "prob_cols = [f\"prob_{i}\" for i in range(N_CLASSES)]\n",
    "df_probs = pd.DataFrame(mean_probabilities, columns=prob_cols)\n",
    "df_probs['original_index'] = test_window_indices \n",
    "agg_probs = df_probs.groupby('original_index')[prob_cols].mean().values\n",
    "print(f\"Aggregated to {len(agg_probs)} final probability vectors.\")\n",
    "\n",
    "# --- 8. Get Final Predictions and Save ---\n",
    "final_predictions_numeric = np.argmax(agg_probs, axis=1)\n",
    "predicted_labels = le.inverse_transform(final_predictions_numeric)\n",
    "\n",
    "print(\"Loading sample submission file for correct formatting...\")\n",
    "test_sample_indices = sorted(X_test_long_df['sample_index'].unique())\n",
    "\n",
    "if len(predicted_labels) != len(test_sample_indices):\n",
    "    print(f\"ERROR: Prediction count mismatch!\")\n",
    "else:\n",
    "    print(\"Prediction count matches. Creating submission.\")\n",
    "    final_submission_df = pd.DataFrame({\n",
    "        'sample_index': test_sample_indices,\n",
    "        'label': predicted_labels \n",
    "    })\n",
    "    final_submission_df['sample_index'] = final_submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "\n",
    "    submission_filepath = os.path.join(\"submissions\", submission_filename_base)\n",
    "    final_submission_df.to_csv(submission_filepath, index=False)\n",
    "\n",
    "    print(f\"\\nSuccessfully saved to {submission_filepath}!\")\n",
    "    print(\"This file is correctly formatted for Kaggle:\")\n",
    "    print(final_submission_df.head())\n",
    "\n",
    "# Clean up\n",
    "del all_fold_probabilities, final_test_features, final_test_ds, test_loader\n",
    "del X_test_full_engineered, X_test_final_scaled, X_test_final_windowed\n",
    "del X_train_full_2d, preprocessor_final\n",
    "del X_test_2d, X_test_scaled_2d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
