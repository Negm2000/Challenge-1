{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è (v13: Final Version)**\n",
    "\n",
    "This notebook integrates the findings from our detailed data analysis to build a robust training and evaluation pipeline. It corrects the preprocessing strategy and implements a proper hold-out test set for unbiased evaluation.\n",
    "\n",
    "**Final Strategy:**\n",
    "1.  **Hold-Out Test Set:** The data is immediately split into a training set (80%) and a hold-out test set (20%). All HPO and K-Fold training is performed *only* on the training set. The test set is used just once for final, unbiased evaluation.\n",
    "2.  **StandardScaler:** Based on our analysis that extreme spikes ('rare jewels') are the primary signal for pain, we have replaced `RobustScaler` and `PowerTransformer` with `StandardScaler`. This preserves the relative magnitude of these critical events.\n",
    "3.  **One-Cycle LR & Compiled Model:** We retain the use of `OneCycleLR` for fast convergence and `torch.compile()` for significant speed improvements.\n",
    "4.  **Focal Loss:** We continue to use Focal Loss to handle the severe class imbalance identified in the analysis.\n",
    "5.  **Final Evaluation:** After K-Fold training, the ensemble of models is evaluated on the unseen test set to generate a final, unbiased Classification Report and Confusion Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Setup & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 1234\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# --- Setup Directories & Device ---\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rc('font', size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 2. Data Loading, Feature Engineering & Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 1. Loading Data ---\")\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_KAGGLE_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "\n",
    "features_long_df = pd.read_csv(X_TRAIN_PATH)\n",
    "labels_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "X_test_kaggle_long_df = pd.read_csv(X_TEST_KAGGLE_PATH)\n",
    "\n",
    "print(\"--- 2. Reshaping and Feature Engineering ---\")\n",
    "N_TIMESTEPS = 160\n",
    "JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(31) if f\"joint_{i:02d}\" != 'joint_30']\n",
    "PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "TIME_FEATURE = ['time']\n",
    "FEATURES = JOINT_FEATURES + PAIN_FEATURES + TIME_FEATURE\n",
    "LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "def reshape_data(df, features_list, n_timesteps):\n",
    "    df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "    data_2d = df_pivot.values\n",
    "    n_samples = data_2d.shape[0]\n",
    "    data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "    return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "X_full = reshape_data(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())], FEATURES, N_TIMESTEPS)\n",
    "X_test_kaggle = reshape_data(X_test_kaggle_long_df, FEATURES, N_TIMESTEPS)\n",
    "\n",
    "y_full_df = labels_df.sort_values(by='sample_index')\n",
    "le = LabelEncoder().fit(list(LABEL_MAPPING.keys()))\n",
    "y_full = le.transform(y_full_df['label'])\n",
    "\n",
    "def engineer_pirate_feature(X_3d, long_df):\n",
    "    static_cols = ['sample_index', 'n_legs', 'n_hands', 'n_eyes']\n",
    "    static_df = long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    pirate_filter = (static_df['n_legs'] == 'one+peg_leg') | (static_df['n_hands'] == 'one+hook_hand') | (static_df['n_eyes'] == 'one+eye_patch')\n",
    "    pirate_indices = static_df[pirate_filter].index\n",
    "    sample_indices_ordered = sorted(long_df['sample_index'].unique())\n",
    "    is_pirate_map = np.array([1 if idx in pirate_indices else 0 for idx in sample_indices_ordered])\n",
    "    pirate_feature_broadcast = np.tile(is_pirate_map.reshape(-1, 1, 1), (1, X_3d.shape[1], 1))\n",
    "    return np.concatenate([X_3d, pirate_feature_broadcast], axis=2)\n",
    "\n",
    "X_full_engineered = engineer_pirate_feature(X_full, features_long_df)\n",
    "X_test_kaggle_engineered = engineer_pirate_feature(X_test_kaggle, X_test_kaggle_long_df)\n",
    "print(f\"Engineered training data shape: {X_full_engineered.shape}\")\n",
    "\n",
    "print(\"\\n--- 3. Creating Stratified Train-Test Split ---\")\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "train_indices, test_indices = next(splitter.split(X_full_engineered, y_full))\n",
    "\n",
    "X_train, y_train = X_full_engineered[train_indices], y_full[train_indices]\n",
    "X_test, y_test = X_full_engineered[test_indices], y_full[test_indices]\n",
    "print(f\"Train set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "print(\"\\n--- 4. Calculating Alpha Weights for Focal Loss (from training set only) ---\")\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights_tensor = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n",
    "alpha_tensor = (class_weights_tensor / class_weights_tensor.sum()).to(device)\n",
    "print(f\"Train class counts (0, 1, 2): {class_counts}\")\n",
    "print(f\"Calculated alpha weights: {alpha_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 3. Helper Functions & Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Implements Focal Loss for cost-sensitive learning.\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets].to(focal_loss.device)\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def create_sliding_windows(X_3d, y=None, window_size=10, stride=2):\n",
    "    new_X, new_y, window_indices = [], [], []\n",
    "    n_samples, n_timesteps, _ = X_3d.shape\n",
    "    for i in range(n_samples):\n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            new_X.append(X_3d[i, idx:idx+window_size, :])\n",
    "            window_indices.append(i)\n",
    "            if y is not None: new_y.append(y[i])\n",
    "            idx += stride\n",
    "    if y is not None:\n",
    "        return np.array(new_X), np.array(new_y), np.array(window_indices)\n",
    "    return np.array(new_X), np.array(window_indices)\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    return DataLoader(ds, batch_size=int(batch_size), shuffle=shuffle, drop_last=drop_last, \n",
    "                      num_workers=2, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 4. Model & Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, rnn_outputs):\n",
    "        energy = torch.tanh(self.attn(rnn_outputs))\n",
    "        attn_scores = self.v(energy).squeeze(2)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(1), rnn_outputs).squeeze(1)\n",
    "        return context_vector\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes,\n",
    "                 conv_out_channels, conv_kernel_size, bidirectional,\n",
    "                 dropout_rate, feature_dropout_rate, rnn_type='GRU'):\n",
    "        super().__init__()\n",
    "        self.rnn_type, self.num_layers, self.hidden_size, self.bidirectional = \\\n",
    "            rnn_type, num_layers, hidden_size, bidirectional\n",
    "        \n",
    "        rnn_hidden_dim = hidden_size * 2 if bidirectional else hidden_size\n",
    "\n",
    "        self.pain_embed_dim, self.pirate_embed_dim = 4, 4\n",
    "        self.pain_embeddings = nn.ModuleList([nn.Embedding(3, self.pain_embed_dim) for _ in range(4)])\n",
    "        self.pirate_embedding = nn.Embedding(2, self.pirate_embed_dim)\n",
    "        \n",
    "        # === MODIFICATION ===\n",
    "        # The number of continuous features is 31 (30 joints + 1 time)\n",
    "        num_continuous_features = 31\n",
    "        total_embedding_dim = (4 * self.pain_embed_dim) + self.pirate_embed_dim\n",
    "        conv_input_size = num_continuous_features + total_embedding_dim\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=conv_input_size, out_channels=conv_out_channels,\n",
    "                                kernel_size=conv_kernel_size, padding='same')\n",
    "        self.conv_activation = nn.ReLU()\n",
    "        self.feature_dropout = nn.Dropout(feature_dropout_rate)\n",
    "\n",
    "        if rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=conv_out_channels, hidden_size=hidden_size,\n",
    "                num_layers=num_layers, batch_first=True, bidirectional=bidirectional,\n",
    "                dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=conv_out_channels, hidden_size=hidden_size,\n",
    "                num_layers=num_layers, batch_first=True, bidirectional=bidirectional,\n",
    "                dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        self.attention = Attention(rnn_hidden_dim)\n",
    "        self.classifier = nn.Linear(rnn_hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # === MODIFICATION === \n",
    "        # Corrected slicing for 31 continuous and 5 categorical features\n",
    "        x_continuous = x[:, :, :31]\n",
    "        x_categorical = x[:, :, 31:].long()\n",
    "        \n",
    "        embedded_cats = [self.pain_embeddings[i](x_categorical[:, :, i]) for i in range(4)] \\\n",
    "                      + [self.pirate_embedding(x_categorical[:, :, 4])]\n",
    "        x_combined = torch.cat([x_continuous] + embedded_cats, dim=2)\n",
    "        x_permuted = x_combined.permute(0, 2, 1)\n",
    "        x_conv = self.conv_activation(self.conv1d(x_permuted))\n",
    "        x_conv_permuted = x_conv.permute(0, 2, 1)\n",
    "        x_dropped = self.feature_dropout(x_conv_permuted)\n",
    "        rnn_outputs, _ = self.rnn(x_dropped)\n",
    "        context_vector = self.attention(rnn_outputs)\n",
    "        return self.classifier(context_vector)\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset.tensors[0]), f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset.tensors[0]), f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scheduler, scaler, device, patience, experiment_name):\n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "    best_f1 = -1; patience_counter = 0\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, scaler, scheduler, device)\n",
    "        val_loss, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        if epoch % 5 == 0: print(f\"Epoch {epoch:3d}/{epochs} | Val F1: {val_f1:.4f} | Best Val F1: {best_f1:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1, patience_counter = val_f1, 0\n",
    "            torch.save(model._orig_mod.state_dict() if hasattr(model, '_orig_mod') else model.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience: print(f\"Early stopping at epoch {epoch}. Best F1: {best_f1:.4f}\"); break\n",
    "    print(f\"--- Finished Training --- Best F1: {best_f1:.4f}\")\n",
    "    uncompiled_model = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "    uncompiled_model.load_state_dict(torch.load(model_path))\n",
    "    return uncompiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ 5. Phase 1: Hyperparameter Search\n",
    "\n",
    "We will run HPO on a validation split taken from our new, smaller training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(config, X_train, y_train, alpha_tensor):\n",
    "    # Create a validation split from the training data for this HPO run\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "    train_idx, val_idx = next(sss.split(X_train, y_train))\n",
    "    X_train_hpo, y_train_hpo = X_train[train_idx], y_train[train_idx]\n",
    "    X_val_hpo, y_val_hpo = X_train[val_idx], y_train[val_idx]\n",
    "\n",
    "    # === MODIFICATION ===\n",
    "    # Re-order and scale data using StandardScaler, based on our analysis\n",
    "    continuous_indices_orig = list(range(30)) + [34] # 30 joints + 1 time\n",
    "    categorical_indices_orig = list(range(30, 34)) + [35] # 4 pain surveys + 1 is_pirate\n",
    "    \n",
    "    X_train_hpo_reordered = np.concatenate([X_train_hpo[:, :, continuous_indices_orig], X_train_hpo[:, :, categorical_indices_orig]], axis=2)\n",
    "    X_val_hpo_reordered = np.concatenate([X_val_hpo[:, :, continuous_indices_orig], X_val_hpo[:, :, categorical_indices_orig]], axis=2)\n",
    "\n",
    "    continuous_indices_reordered = list(range(31))\n",
    "    preprocessor = ColumnTransformer([('scaler', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "    \n",
    "    ns, ts, f = X_train_hpo_reordered.shape\n",
    "    X_train_hpo_scaled = preprocessor.fit_transform(X_train_hpo_reordered.reshape(ns*ts, f)).reshape(ns, ts, f)\n",
    "    ns_val, ts_val, f_val = X_val_hpo_reordered.shape\n",
    "    X_val_hpo_scaled = preprocessor.transform(X_val_hpo_reordered.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "    # === END MODIFICATION ===\n",
    "\n",
    "    X_train_w, y_train_w, _ = create_sliding_windows(X_train_hpo_scaled, y_train_hpo, config['window_size'], config['stride'])\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(X_val_hpo_scaled, y_val_hpo, config['window_size'], config['stride'])\n",
    "    \n",
    "    train_loader = make_loader(TensorDataset(torch.from_numpy(X_train_w).float(), torch.from_numpy(y_train_w).long()), config[\"batch_size\"], True, True)\n",
    "    val_loader = make_loader(TensorDataset(torch.from_numpy(X_val_w).float(), torch.from_numpy(y_val_w).long()), config[\"batch_size\"], False, False)\n",
    "\n",
    "    model_config = {k: v for k, v in config.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "    model = RecurrentClassifier(**model_config, num_classes=N_CLASSES).to(device)\n",
    "    model = torch.compile(model, backend=\"eager\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "    EPOCHS_HPO = 100\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config[\"lr\"], epochs=EPOCHS_HPO, steps_per_epoch=len(train_loader))\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = FocalLoss(alpha=alpha_tensor, gamma=config['focal_loss_gamma'])\n",
    "\n",
    "    best_val_f1 = -1.0; patience_counter = 0; hpo_patience = 25\n",
    "    \n",
    "    for epoch in range(1, EPOCHS_HPO + 1):\n",
    "        train_one_epoch(model, train_loader, criterion, optimizer, scaler, scheduler, device)\n",
    "        _, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        tune.report({\"val_f1\": val_f1})\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1; patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= hpo_patience: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"rnn_type\": tune.choice(['GRU', 'LSTM']),\n",
    "    \"focal_loss_gamma\": tune.uniform(0.5, 3.0),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"batch_size\": tune.choice([64, 128]),\n",
    "    \"hidden_size\": tune.choice([256, 384, 512]),\n",
    "    \"num_layers\": tune.choice([2, 3]),\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.5),\n",
    "    \"feature_dropout_rate\": tune.uniform(0.1, 0.5),\n",
    "    \"bidirectional\": tune.choice([True, False]),\n",
    "    \"l2_lambda\": tune.loguniform(1e-8, 1e-5),\n",
    "    \"conv_out_channels\": tune.choice([64, 128]),\n",
    "    \"conv_kernel_size\": tune.choice([3, 5]),\n",
    "    \"window_size\": tune.choice([10]), # Fixed based on analysis\n",
    "    \"stride\": tune.choice([2])      # Fixed based on analysis\n",
    "}\n",
    "\n",
    "if ray.is_initialized(): ray.shutdown()\n",
    "ray.init(num_cpus=os.cpu_count(), num_gpus=1, ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "print(\"--- Starting HPO ---\")\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(objective_function, X_train=X_train, y_train=y_train, alpha_tensor=alpha_tensor),\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 0.25},\n",
    "    config=search_space,\n",
    "    num_samples=50, # Number of trials to run\n",
    "    search_alg=OptunaSearch(metric=\"val_f1\", mode=\"max\"),\n",
    "    scheduler=ASHAScheduler(metric=\"val_f1\", mode=\"max\", grace_period=25, reduction_factor=2),\n",
    "    name=\"pirate_pain_final_search\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "FINAL_CONFIG = best_trial.config\n",
    "print(\"\\n--- Best Hyperparameters Found ---\")\n",
    "print(FINAL_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ 6. Phase 2: K-Fold Ensemble Training\n",
    "\n",
    "Now we train our final ensemble of models using the best hyperparameters on our full training set with K-Fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "FINAL_EXPERIMENT_NAME = f\"Final-{FINAL_CONFIG['rnn_type']}_H{FINAL_CONFIG['hidden_size']}_L{FINAL_CONFIG['num_layers']}_v13\"\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "fold_val_f1_list = []\n",
    "\n",
    "# === MODIFICATION === \n",
    "# Reorder full training data once before the loop\n",
    "continuous_indices_orig = list(range(30)) + [34]\n",
    "categorical_indices_orig = list(range(30, 34)) + [35]\n",
    "X_train_reordered = np.concatenate([X_train[:, :, continuous_indices_orig], X_train[:, :, categorical_indices_orig]], axis=2)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_reordered, y_train)):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} --- ({fold_name}) ---\")\n",
    "    \n",
    "    X_train_fold, y_train_fold = X_train_reordered[train_idx], y_train[train_idx]\n",
    "    X_val_fold, y_val_fold = X_train_reordered[val_idx], y_train[val_idx]\n",
    "\n",
    "    continuous_indices_reordered = list(range(31))\n",
    "    preprocessor_fold = ColumnTransformer([('scaler', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "\n",
    "    ns, ts, f = X_train_fold.shape\n",
    "    X_train_scaled = preprocessor_fold.fit_transform(X_train_fold.reshape(ns*ts, f)).reshape(ns, ts, f)\n",
    "    ns_val, ts_val, f_val = X_val_fold.shape\n",
    "    X_val_scaled = preprocessor_fold.transform(X_val_fold.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "    \n",
    "    X_train_w, y_train_w, _ = create_sliding_windows(X_train_scaled, y_train_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(X_val_scaled, y_val_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    \n",
    "    train_loader = make_loader(TensorDataset(torch.from_numpy(X_train_w).float(), torch.from_numpy(y_train_w).long()), FINAL_CONFIG['batch_size'], True, True)\n",
    "    val_loader = make_loader(TensorDataset(torch.from_numpy(X_val_w).float(), torch.from_numpy(y_val_w).long()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "    model_config_kfold = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "    model_fold = RecurrentClassifier(**model_config_kfold, num_classes=N_CLASSES).to(device)\n",
    "    model_fold = torch.compile(model_fold, backend=\"eager\")\n",
    "    \n",
    "    EPOCHS_K_FOLD = 150\n",
    "    optimizer = torch.optim.AdamW(model_fold.parameters(), lr=FINAL_CONFIG['lr'], weight_decay=FINAL_CONFIG['l2_lambda'])\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=FINAL_CONFIG['lr'], epochs=EPOCHS_K_FOLD, steps_per_epoch=len(train_loader))\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = FocalLoss(alpha=alpha_tensor, gamma=FINAL_CONFIG['focal_loss_gamma'])\n",
    "\n",
    "    model_fold_uncompiled = fit(model_fold, train_loader, val_loader, EPOCHS_K_FOLD, criterion, optimizer, scheduler, scaler, device, 50, fold_name)\n",
    "    \n",
    "    _, val_f1 = validate_one_epoch(model_fold_uncompiled, val_loader, criterion, device)\n",
    "    fold_val_f1_list.append(val_f1)\n",
    "    print(f\"Fold {fold+1} Final Val F1: {val_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n--- üèÜ K-Fold Training Complete --- Average CV F1: {np.mean(fold_val_f1_list):.4f} ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 7. Phase 3: Final Evaluation on Hold-Out Test Set\n",
    "\n",
    "This is the most important step. We will now evaluate our trained K-Fold ensemble on the test set that we held out at the very beginning. This provides a final, unbiased measure of our model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting Final Evaluation on the Hold-Out Test Set ---\")\n",
    "\n",
    "# --- 1. Preprocess the Hold-Out Test Data ---\n",
    "# We need a scaler fitted on the ENTIRE training set to transform the test set.\n",
    "X_train_reordered_flat = X_train_reordered.reshape(X_train_reordered.shape[0] * N_TIMESTEPS, X_train_reordered.shape[2])\n",
    "final_preprocessor = ColumnTransformer([('scaler', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "final_preprocessor.fit(X_train_reordered_flat)\n",
    "\n",
    "# Reorder and scale the test data\n",
    "X_test_reordered = np.concatenate([X_test[:, :, continuous_indices_orig], X_test[:, :, categorical_indices_orig]], axis=2)\n",
    "X_test_scaled = final_preprocessor.transform(X_test_reordered.reshape(X_test.shape[0] * N_TIMESTEPS, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# Create sliding windows for the test set\n",
    "X_test_w, y_test_w, test_window_indices = create_sliding_windows(X_test_scaled, y_test, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "test_loader = DataLoader(TensorDataset(torch.from_numpy(X_test_w).float()), batch_size=FINAL_CONFIG['batch_size'], shuffle=False)\n",
    "print(\"Test data preprocessed and ready for evaluation.\")\n",
    "\n",
    "# --- 2. Generate Ensemble Predictions ---\n",
    "all_fold_probabilities_test = []\n",
    "model_config_eval = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    print(f\"Loading model {fold+1}/{N_SPLITS} from {model_path}...\")\n",
    "    \n",
    "    model = RecurrentClassifier(**model_config_eval, num_classes=N_CLASSES).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = torch.compile(model, backend=\"eager\")\n",
    "    model.eval()\n",
    "    \n",
    "    fold_probs = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in test_loader:\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                probs = torch.softmax(model(inputs.to(device)), dim=1)\n",
    "                fold_probs.append(probs.cpu().numpy())\n",
    "    all_fold_probabilities_test.append(np.concatenate(fold_probs))\n",
    "\n",
    "# Average probabilities across folds and aggregate for final predictions\n",
    "mean_probabilities_test = np.mean(all_fold_probabilities_test, axis=0)\n",
    "df_probs_test = pd.DataFrame(mean_probabilities_test)\n",
    "df_probs_test['original_index'] = test_window_indices\n",
    "agg_probs_test = df_probs_test.groupby('original_index').mean().values\n",
    "final_predictions_test = np.argmax(agg_probs_test, axis=1)\n",
    "print(\"\\nEnsemble predictions generated for the test set.\")\n",
    "\n",
    "# --- 3. Display Performance Metrics ---\n",
    "print(\"\\n\" + \"*\"*60)\n",
    "print(\"         FINAL UNBIASED PERFORMANCE ON HOLD-OUT TEST SET\")\n",
    "print(\"*\"*60 + \"\\n\")\n",
    "\n",
    "print(f\"Overall Weighted F1-Score: {f1_score(y_test, final_predictions_test, average='weighted'):.4f}\\n\")\n",
    "\n",
    "class_names = le.classes_\n",
    "report = classification_report(y_test, final_predictions_test, target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_test, final_predictions_test)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Final Confusion Matrix on Unseen Test Data', fontsize=16)\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¨ 8. Phase 4: Generate Kaggle Submission\n",
    "\n",
    "Finally, we use our trained ensemble to make predictions on the official Kaggle test set and generate the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Preparing Kaggle test dataset for submission ---\")\n",
    "submission_filename_base = f\"submission_{FINAL_EXPERIMENT_NAME}.csv\"\n",
    "\n",
    "# Reorder and scale the Kaggle test data using the same preprocessor fitted on our training data\n",
    "X_test_kaggle_reordered = np.concatenate([X_test_kaggle_engineered[:, :, continuous_indices_orig], X_test_kaggle_engineered[:, :, categorical_indices_orig]], axis=2)\n",
    "X_test_kaggle_scaled = final_preprocessor.transform(X_test_kaggle_reordered.reshape(X_test_kaggle_reordered.shape[0] * N_TIMESTEPS, X_test_kaggle_reordered.shape[2])).reshape(X_test_kaggle_reordered.shape)\n",
    "\n",
    "X_test_kaggle_w, test_kaggle_window_indices = create_sliding_windows(X_test_kaggle_scaled, y=None, window_size=FINAL_CONFIG['window_size'], stride=FINAL_CONFIG['stride'])\n",
    "test_kaggle_loader = DataLoader(TensorDataset(torch.from_numpy(X_test_kaggle_w).float()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "all_fold_probabilities_kaggle = []\n",
    "\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    print(f\"Loading model {fold+1}/{N_SPLITS} from {model_path} for Kaggle submission...\")\n",
    "    \n",
    "    model = RecurrentClassifier(**model_config_eval, num_classes=N_CLASSES).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = torch.compile(model, backend=\"eager\")\n",
    "    model.eval()\n",
    "    \n",
    "    fold_preds_kaggle = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in test_kaggle_loader:\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                probs = torch.softmax(model(inputs.to(device)), dim=1)\n",
    "                fold_preds_kaggle.append(probs.cpu().numpy())\n",
    "    all_fold_probabilities_kaggle.append(np.concatenate(fold_preds_kaggle))\n",
    "\n",
    "mean_probabilities_kaggle = np.mean(all_fold_probabilities_kaggle, axis=0)\n",
    "df_probs_kaggle = pd.DataFrame(mean_probabilities_kaggle)\n",
    "df_probs_kaggle['original_index'] = test_kaggle_window_indices\n",
    "agg_probs_kaggle = df_probs_kaggle.groupby('original_index').mean().values\n",
    "final_predictions_kaggle = le.inverse_transform(np.argmax(agg_probs_kaggle, axis=1))\n",
    "\n",
    "submission_df = pd.DataFrame({'sample_index': sorted(X_test_kaggle_long_df['sample_index'].unique()), 'label': final_predictions_kaggle})\n",
    "submission_df['sample_index'] = submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "submission_filepath = os.path.join(\"submissions\", submission_filename_base)\n",
    "submission_df.to_csv(submission_filepath, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully saved submission to {submission_filepath}!\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}