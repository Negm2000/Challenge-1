{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67bb0a3d",
   "metadata": {},
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è (v12: Compiled Model & OneCycleLR)**\n",
    "\n",
    "This notebook targets a major training speedup by using PyTorch's JIT compiler. It also replaces the learning rate scheduler with a more aggressive and often faster-converging policy.\n",
    "\n",
    "**Strategy Update:**\n",
    "1.  **üî• PyTorch 2.0 Model Compilation:** The model is now wrapped with `torch.compile()`. This JIT-compiles the model into optimized, high-performance kernels for a significant training and inference speedup with no change to the model's logic.\n",
    "2.  **One-Cycle LR Scheduler:** The Cosine Annealing scheduler has been replaced with `OneCycleLR`. This policy can lead to faster convergence and better final performance. The training loop has been modified to step the scheduler after each batch, as required.\n",
    "3.  **Focal Loss (Cost-Sensitive Learning):** We retain the use of Focal Loss to dynamically focus training on hard-to-classify examples.\n",
    "4.  **HPO for RNN Type:** The hyperparameter search continues to explore whether `GRU` or `LSTM` is the optimal choice for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03291d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Setup & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ba22088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GPU ---\n",
      "PyTorch version: 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 1234\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Sklearn Imports ---\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Ray[tune] & Optuna Imports ---\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from functools import partial\n",
    "\n",
    "# --- Setup Directories & Device ---\n",
    "logs_dir = \"tensorboard\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8582ef",
   "metadata": {},
   "source": [
    "## üîÑ 2. Data Loading & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44203b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Data ---\n",
      "Loaded X_train_full (shape: (661, 160, 36)) and y_train_full (shape: (661,))\n",
      "Loaded X_test_full (shape: (1324, 160, 36))\n",
      "\n",
      "--- 2. Engineering 'is_pirate' Feature ---\n",
      "Created X_train_full_engineered (shape: (661, 160, 37))\n",
      "Created X_test_full_engineered (shape: (1324, 160, 37))\n",
      "N_FEATURES is now: 37\n",
      "\n",
      "--- 3. Calculating Alpha Weights for Focal Loss ---\n",
      "Class counts (0, 1, 2): [511  94  56]\n",
      "Calculated alpha weights: tensor([0.0643, 0.3493, 0.5864], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Loading Data ---\")\n",
    "\n",
    "# --- Define File Paths and Features ---\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "try:\n",
    "    features_long_df = pd.read_csv(X_TRAIN_PATH)\n",
    "    labels_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "    X_test_long_df = pd.read_csv(X_TEST_PATH)\n",
    "    \n",
    "    N_TIMESTEPS = 160\n",
    "    JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(31)]\n",
    "    PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "    TIME_FEATURE = ['time']\n",
    "    FEATURES = JOINT_FEATURES + PAIN_FEATURES + TIME_FEATURE\n",
    "    LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "    N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "    def reshape_data(df, features_list, n_timesteps):\n",
    "        df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "        data_2d = df_pivot.values\n",
    "        n_samples = data_2d.shape[0]\n",
    "        data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "        return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "    X_train_full = reshape_data(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())], FEATURES, N_TIMESTEPS)\n",
    "    X_test_full = reshape_data(X_test_long_df, FEATURES, N_TIMESTEPS)\n",
    "    y_train_full_df = labels_df.sort_values(by='sample_index')\n",
    "    le = LabelEncoder().fit(list(LABEL_MAPPING.keys()))\n",
    "    y_train_full = le.transform(y_train_full_df['label'])\n",
    "    print(f\"Loaded X_train_full (shape: {X_train_full.shape}) and y_train_full (shape: {y_train_full.shape})\")\n",
    "    print(f\"Loaded X_test_full (shape: {X_test_full.shape})\")\n",
    "\n",
    "    print(\"\\n--- 2. Engineering 'is_pirate' Feature ---\")\n",
    "    static_cols = ['sample_index', 'n_legs', 'n_hands', 'n_eyes']\n",
    "    static_df = features_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    pirate_filter = (static_df['n_legs'] == 'one+peg_leg') | (static_df['n_hands'] == 'one+hook_hand') | (static_df['n_eyes'] == 'one+eye_patch')\n",
    "    pirate_indices = static_df[pirate_filter].index\n",
    "    sample_indices_ordered = sorted(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())]['sample_index'].unique())\n",
    "    is_pirate_map = np.array([1 if idx in pirate_indices else 0 for idx in sample_indices_ordered])\n",
    "    pirate_feature_broadcast = np.tile(is_pirate_map.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    X_train_full_engineered = np.concatenate([X_train_full, pirate_feature_broadcast], axis=2)\n",
    "\n",
    "    static_df_test = X_test_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    pirate_filter_test = (static_df_test['n_legs'] == 'one+peg_leg') | (static_df_test['n_hands'] == 'one+hook_hand') | (static_df_test['n_eyes'] == 'one+eye_patch')\n",
    "    pirate_indices_test = static_df_test[pirate_filter_test].index\n",
    "    sample_indices_test_ordered = sorted(X_test_long_df['sample_index'].unique())\n",
    "    is_pirate_map_test = np.array([1 if idx in pirate_indices_test else 0 for idx in sample_indices_test_ordered])\n",
    "    pirate_feature_broadcast_test = np.tile(is_pirate_map_test.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    X_test_full_engineered = np.concatenate([X_test_full, pirate_feature_broadcast_test], axis=2)\n",
    "    \n",
    "    N_FEATURES_NEW = X_train_full_engineered.shape[2]\n",
    "    print(f\"Created X_train_full_engineered (shape: {X_train_full_engineered.shape})\")\n",
    "    print(f\"Created X_test_full_engineered (shape: {X_test_full_engineered.shape})\")\n",
    "    print(f\"N_FEATURES is now: {N_FEATURES_NEW}\")\n",
    "\n",
    "    print(\"\\n--- 3. Calculating Alpha Weights for Focal Loss ---\")\n",
    "    class_counts_series = labels_df['label'].value_counts()\n",
    "    counts_ordered = class_counts_series.reindex(LABEL_MAPPING.keys()).values\n",
    "    class_weights_tensor = 1.0 / torch.tensor(counts_ordered, dtype=torch.float)\n",
    "    alpha_tensor = (class_weights_tensor / class_weights_tensor.sum()).to(device)\n",
    "    print(f\"Class counts (0, 1, 2): {counts_ordered}\")\n",
    "    print(f\"Calculated alpha weights: {alpha_tensor}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f8bc4",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 3. Helper Functions & Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a7c2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Implements Focal Loss for cost-sensitive learning.\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets].to(focal_loss.device)\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def create_sliding_windows(X_3d, y=None, window_size=100, stride=20):\n",
    "    new_X, new_y, window_indices = [], [], []\n",
    "    n_samples, n_timesteps, _ = X_3d.shape\n",
    "    for i in range(n_samples):\n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            new_X.append(X_3d[i, idx:idx+window_size, :])\n",
    "            window_indices.append(i)\n",
    "            if y is not None: new_y.append(y[i])\n",
    "            idx += stride\n",
    "    if y is not None:\n",
    "        return np.array(new_X), np.array(new_y), np.array(window_indices)\n",
    "    return np.array(new_X), np.array(window_indices)\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    return DataLoader(ds, batch_size=int(batch_size), shuffle=shuffle, drop_last=drop_last, \n",
    "                      num_workers=2, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faed930",
   "metadata": {},
   "source": [
    "## üß† 4. Model & Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd3cadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, rnn_outputs):\n",
    "        energy = torch.tanh(self.attn(rnn_outputs))\n",
    "        attn_scores = self.v(energy).squeeze(2)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(1), rnn_outputs).squeeze(1)\n",
    "        return context_vector\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes,\n",
    "                 conv_out_channels, conv_kernel_size, bidirectional,\n",
    "                 dropout_rate, feature_dropout_rate, rnn_type='GRU'):\n",
    "        super().__init__()\n",
    "        self.rnn_type, self.num_layers, self.hidden_size, self.bidirectional = \\\n",
    "            rnn_type, num_layers, hidden_size, bidirectional\n",
    "        \n",
    "        rnn_hidden_dim = hidden_size * 2 if bidirectional else hidden_size\n",
    "\n",
    "        self.pain_embed_dim, self.pirate_embed_dim = 4, 4\n",
    "        self.pain_embeddings = nn.ModuleList([nn.Embedding(3, self.pain_embed_dim) for _ in range(4)])\n",
    "        self.pirate_embedding = nn.Embedding(2, self.pirate_embed_dim)\n",
    "        \n",
    "        num_continuous_features = 32\n",
    "        total_embedding_dim = (4 * self.pain_embed_dim) + self.pirate_embed_dim\n",
    "        conv_input_size = num_continuous_features + total_embedding_dim\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=conv_input_size, out_channels=conv_out_channels,\n",
    "                                kernel_size=conv_kernel_size, padding='same')\n",
    "        self.conv_activation = nn.ReLU()\n",
    "        self.feature_dropout = nn.Dropout(feature_dropout_rate)\n",
    "\n",
    "        if rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=conv_out_channels, hidden_size=hidden_size,\n",
    "                num_layers=num_layers, batch_first=True, bidirectional=bidirectional,\n",
    "                dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=conv_out_channels, hidden_size=hidden_size,\n",
    "                num_layers=num_layers, batch_first=True, bidirectional=bidirectional,\n",
    "                dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        self.attention = Attention(rnn_hidden_dim)\n",
    "        self.classifier = nn.Linear(rnn_hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_continuous = x[:, :, :32]\n",
    "        x_categorical = x[:, :, 32:].long()\n",
    "        embedded_cats = [self.pain_embeddings[i](x_categorical[:, :, i]) for i in range(4)] \\\n",
    "                      + [self.pirate_embedding(x_categorical[:, :, 4])]\n",
    "        x_combined = torch.cat([x_continuous] + embedded_cats, dim=2)\n",
    "        x_permuted = x_combined.permute(0, 2, 1)\n",
    "        x_conv = self.conv_activation(self.conv1d(x_permuted))\n",
    "        x_conv_permuted = x_conv.permute(0, 2, 1)\n",
    "        x_dropped = self.feature_dropout(x_conv_permuted)\n",
    "        rnn_outputs, _ = self.rnn(x_dropped)\n",
    "        context_vector = self.attention(rnn_outputs)\n",
    "        return self.classifier(context_vector)\n",
    "\n",
    "# --- MODIFIED: Scheduler is now passed in and stepped per-batch ---\n",
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step() # <-- OneCycleLR is stepped each batch\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset.tensors[0]), f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset.tensors[0]), f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def objective_function(config, X_train_w, y_train_w, X_val_w, y_val_w, alpha_tensor):\n",
    "    EPOCHS = 150\n",
    "    train_loader = make_loader(TensorDataset(X_train_w, y_train_w), config[\"batch_size\"], True, True)\n",
    "    val_loader = make_loader(TensorDataset(X_val_w, y_val_w), config[\"batch_size\"], False, False)\n",
    "\n",
    "    model_config = {k: v for k, v in config.items() if k not in ['lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "    model = RecurrentClassifier(**model_config, num_classes=N_CLASSES).to(device)\n",
    "    model = torch.compile(model, backend=\"eager\") # <-- SPEEDUP\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "    # --- MODIFIED: Use OneCycleLR ---\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=config[\"lr\"], epochs=EPOCHS, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = FocalLoss(alpha=alpha_tensor, gamma=config['focal_loss_gamma'])\n",
    "\n",
    "    best_val_f1 = -1.0; patience_counter = 0; hpo_patience = 30\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, _ = train_one_epoch(model, train_loader, criterion, optimizer, scaler, scheduler, device)\n",
    "        _, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        tune.report({\"val_f1\": val_f1, \"train_loss\": train_loss})\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1; patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= hpo_patience: break\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scheduler, scaler, device, patience, experiment_name):\n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "    best_f1 = -1; patience_counter = 0\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # --- MODIFIED: Pass scheduler to training function ---\n",
    "        train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, scaler, scheduler, device)\n",
    "        val_loss, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        if epoch % 5 == 0: print(f\"Epoch {epoch:3d}/{epochs} | Val F1: {val_f1:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1, patience_counter = val_f1, 0\n",
    "            # When using torch.compile, it's better to save the state_dict of the original model\n",
    "            torch.save(model._orig_mod.state_dict() if hasattr(model, '_orig_mod') else model.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience: print(f\"Early stopping at epoch {epoch}. Best F1: {best_f1:.4f}\"); break\n",
    "    print(f\"--- Finished Training --- Best F1: {best_f1:.4f}\")\n",
    "    # Load the best weights back into the uncompiled model for consistency\n",
    "    uncompiled_model = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "    uncompiled_model.load_state_dict(torch.load(model_path))\n",
    "    return uncompiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f911c",
   "metadata": {},
   "source": [
    "## üß™ 5. Phase 1: Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d09a835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Splitting data for HPO ---\n",
      "--- Pre-scaling data for HPO efficiency ---\n",
      "--- Creating fixed sliding windows for HPO ---\n",
      "Created training windows of shape: torch.Size([40128, 10, 37])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3079"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define fixed windowing parameters\n",
    "WINDOW_SIZE = 10\n",
    "STRIDE = 2\n",
    "\n",
    "# Reorder columns to group continuous and categorical for easy scaling\n",
    "continuous_indices_orig = list(range(31)) + [35]\n",
    "categorical_indices_orig = list(range(31, 35)) + [36]\n",
    "X_train_full_reordered = np.concatenate([\n",
    "    X_train_full_engineered[:, :, continuous_indices_orig],\n",
    "    X_train_full_engineered[:, :, categorical_indices_orig]\n",
    "], axis=2)\n",
    "\n",
    "print(\"--- Splitting data for HPO ---\")\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "for train_idx, val_idx in sss.split(X_train_full_reordered, y_train_full):\n",
    "    X_train_split, y_train_split = X_train_full_reordered[train_idx], y_train_full[train_idx]\n",
    "    X_val_split, y_val_split = X_train_full_reordered[val_idx], y_train_full[val_idx]\n",
    "\n",
    "print(\"--- Pre-scaling data for HPO efficiency ---\")\n",
    "continuous_indices_reordered = list(range(32))\n",
    "preprocessor_hpo = ColumnTransformer([('scaler', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "ns, ts, f = X_train_split.shape\n",
    "X_train_split_scaled = preprocessor_hpo.fit_transform(X_train_split.reshape(ns*ts, f)).reshape(ns, ts, f)\n",
    "ns_val, ts_val, f_val = X_val_split.shape\n",
    "X_val_split_scaled = preprocessor_hpo.transform(X_val_split.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "\n",
    "print(\"--- Creating fixed sliding windows for HPO ---\")\n",
    "X_train_w, y_train_w, _ = create_sliding_windows(X_train_split_scaled, y_train_split, WINDOW_SIZE, STRIDE)\n",
    "X_val_w, y_val_w, _ = create_sliding_windows(X_val_split_scaled, y_val_split, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "# Convert to tensors once before HPO\n",
    "X_train_w_torch = torch.from_numpy(X_train_w).float()\n",
    "y_train_w_torch = torch.from_numpy(y_train_w).long()\n",
    "X_val_w_torch = torch.from_numpy(X_val_w).float()\n",
    "y_val_w_torch = torch.from_numpy(y_val_w).long()\n",
    "\n",
    "print(f\"Created training windows of shape: {X_train_w_torch.shape}\")\n",
    "\n",
    "# Clean up memory\n",
    "del X_train_split_scaled, X_val_split_scaled, X_train_w, y_train_w, X_val_w, y_val_w\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b42797c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-15 02:20:09</td></tr>\n",
       "<tr><td>Running for: </td><td>02:26:33.22        </td></tr>\n",
       "<tr><td>Memory:      </td><td>8.1/13.9 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=46<br>Bracket: Iter 100.000: 0.9471453193137753 | Iter 50.000: 0.9420575725231499 | Iter 25.000: 0.9346220136839073<br>Logical resource usage: 4.0/16 CPUs, 0.25/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th>bidirectional  </th><th style=\"text-align: right;\">  conv_kernel_size</th><th style=\"text-align: right;\">  conv_out_channels</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  feature_dropout_rate</th><th style=\"text-align: right;\">  focal_loss_gamma</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  l2_lambda</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th>rnn_type  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_f1</th><th style=\"text-align: right;\">  train_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_function_85c26f72</td><td>TERMINATED</td><td>127.0.0.1:4552 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.241413 </td><td style=\"text-align: right;\">            0.459152  </td><td style=\"text-align: right;\">          2.64648 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">5.52371e-08</td><td style=\"text-align: right;\">0.00379156 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         419.528</td><td style=\"text-align: right;\">0.943439</td><td style=\"text-align: right;\"> 0.0012912  </td></tr>\n",
       "<tr><td>objective_function_23454d6d</td><td>TERMINATED</td><td>127.0.0.1:31872</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.178398 </td><td style=\"text-align: right;\">            0.294126  </td><td style=\"text-align: right;\">          1.08428 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.10329e-06</td><td style=\"text-align: right;\">0.00333789 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         954.786</td><td style=\"text-align: right;\">0.95918 </td><td style=\"text-align: right;\"> 0.000282531</td></tr>\n",
       "<tr><td>objective_function_88088720</td><td>TERMINATED</td><td>127.0.0.1:33004</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.0234289</td><td style=\"text-align: right;\">            0.344037  </td><td style=\"text-align: right;\">          2.96123 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">9.87872e-06</td><td style=\"text-align: right;\">0.00435542 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         412.655</td><td style=\"text-align: right;\">0.926253</td><td style=\"text-align: right;\"> 0.00115108 </td></tr>\n",
       "<tr><td>objective_function_8254d48f</td><td>TERMINATED</td><td>127.0.0.1:39328</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.21625  </td><td style=\"text-align: right;\">            0.113576  </td><td style=\"text-align: right;\">          1.84998 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">3.63083e-08</td><td style=\"text-align: right;\">0.000201716</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         225.503</td><td style=\"text-align: right;\">0.926467</td><td style=\"text-align: right;\"> 0.00251195 </td></tr>\n",
       "<tr><td>objective_function_4e1f8e64</td><td>TERMINATED</td><td>127.0.0.1:39876</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.311842 </td><td style=\"text-align: right;\">            0.135101  </td><td style=\"text-align: right;\">          0.892908</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.53467e-06</td><td style=\"text-align: right;\">0.000265395</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         666.041</td><td style=\"text-align: right;\">0.941062</td><td style=\"text-align: right;\"> 0.00038739 </td></tr>\n",
       "<tr><td>objective_function_99d35a68</td><td>TERMINATED</td><td>127.0.0.1:29092</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.350125 </td><td style=\"text-align: right;\">            0.379008  </td><td style=\"text-align: right;\">          1.00362 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">6.12386e-06</td><td style=\"text-align: right;\">0.00378399 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1076.3  </td><td style=\"text-align: right;\">0.921233</td><td style=\"text-align: right;\"> 0.00157458 </td></tr>\n",
       "<tr><td>objective_function_a3b03e65</td><td>TERMINATED</td><td>127.0.0.1:35332</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.485691 </td><td style=\"text-align: right;\">            0.268053  </td><td style=\"text-align: right;\">          2.13652 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.46005e-07</td><td style=\"text-align: right;\">0.00143721 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         558.698</td><td style=\"text-align: right;\">0.942058</td><td style=\"text-align: right;\"> 0.000435691</td></tr>\n",
       "<tr><td>objective_function_0a15aa5d</td><td>TERMINATED</td><td>127.0.0.1:38484</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.27205  </td><td style=\"text-align: right;\">            0.101095  </td><td style=\"text-align: right;\">          2.70443 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">5.93239e-07</td><td style=\"text-align: right;\">0.00264417 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         363.143</td><td style=\"text-align: right;\">0.925924</td><td style=\"text-align: right;\"> 0.00113705 </td></tr>\n",
       "<tr><td>objective_function_fea616eb</td><td>TERMINATED</td><td>127.0.0.1:22792</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.490298 </td><td style=\"text-align: right;\">            0.32942   </td><td style=\"text-align: right;\">          0.883004</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">9.16785e-08</td><td style=\"text-align: right;\">0.000162147</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         543.796</td><td style=\"text-align: right;\">0.929806</td><td style=\"text-align: right;\"> 0.00463117 </td></tr>\n",
       "<tr><td>objective_function_44943e0e</td><td>TERMINATED</td><td>127.0.0.1:27304</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.277226 </td><td style=\"text-align: right;\">            0.132794  </td><td style=\"text-align: right;\">          2.92657 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.97896e-07</td><td style=\"text-align: right;\">0.000177471</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">        2193.4  </td><td style=\"text-align: right;\">0.932401</td><td style=\"text-align: right;\"> 0.00042111 </td></tr>\n",
       "<tr><td>objective_function_0570b646</td><td>TERMINATED</td><td>127.0.0.1:23412</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.1186   </td><td style=\"text-align: right;\">            0.324752  </td><td style=\"text-align: right;\">          2.21181 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.78581e-08</td><td style=\"text-align: right;\">0.00636612 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">        1178.9  </td><td style=\"text-align: right;\">0.904669</td><td style=\"text-align: right;\"> 0.00727569 </td></tr>\n",
       "<tr><td>objective_function_4de6bc13</td><td>TERMINATED</td><td>127.0.0.1:37484</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.108485 </td><td style=\"text-align: right;\">            0.390369  </td><td style=\"text-align: right;\">          0.816701</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">2.44319e-07</td><td style=\"text-align: right;\">0.00254505 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         988.693</td><td style=\"text-align: right;\">0.935051</td><td style=\"text-align: right;\"> 0.0017343  </td></tr>\n",
       "<tr><td>objective_function_af42f74f</td><td>TERMINATED</td><td>127.0.0.1:32552</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.298757 </td><td style=\"text-align: right;\">            0.199085  </td><td style=\"text-align: right;\">          1.5288  </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.18614e-06</td><td style=\"text-align: right;\">0.000123852</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         390.303</td><td style=\"text-align: right;\">0.929792</td><td style=\"text-align: right;\"> 0.00408108 </td></tr>\n",
       "<tr><td>objective_function_e3b721e5</td><td>TERMINATED</td><td>127.0.0.1:9548 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.249139 </td><td style=\"text-align: right;\">            0.23956   </td><td style=\"text-align: right;\">          2.72777 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">2.71587e-08</td><td style=\"text-align: right;\">0.00377728 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         363.222</td><td style=\"text-align: right;\">0.955543</td><td style=\"text-align: right;\"> 0.000721363</td></tr>\n",
       "<tr><td>objective_function_6180d393</td><td>TERMINATED</td><td>127.0.0.1:35780</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.109976 </td><td style=\"text-align: right;\">            0.0203926 </td><td style=\"text-align: right;\">          0.531266</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.86514e-07</td><td style=\"text-align: right;\">0.00996173 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         821.717</td><td style=\"text-align: right;\">0.912853</td><td style=\"text-align: right;\"> 0.00788454 </td></tr>\n",
       "<tr><td>objective_function_0e878cb4</td><td>TERMINATED</td><td>127.0.0.1:26472</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.12796  </td><td style=\"text-align: right;\">            0.00342908</td><td style=\"text-align: right;\">          1.41245 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.49161e-08</td><td style=\"text-align: right;\">0.0084991  </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         479.508</td><td style=\"text-align: right;\">0.93604 </td><td style=\"text-align: right;\"> 0.00134851 </td></tr>\n",
       "<tr><td>objective_function_eb749402</td><td>TERMINATED</td><td>127.0.0.1:19136</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.163602 </td><td style=\"text-align: right;\">            0.0298699 </td><td style=\"text-align: right;\">          1.43134 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.3077e-08 </td><td style=\"text-align: right;\">0.00964844 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         290.908</td><td style=\"text-align: right;\">0.932032</td><td style=\"text-align: right;\"> 0.00215035 </td></tr>\n",
       "<tr><td>objective_function_31bc8126</td><td>TERMINATED</td><td>127.0.0.1:38552</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.192291 </td><td style=\"text-align: right;\">            0.0143013 </td><td style=\"text-align: right;\">          1.39679 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.36749e-08</td><td style=\"text-align: right;\">0.000715422</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         284.192</td><td style=\"text-align: right;\">0.917019</td><td style=\"text-align: right;\"> 0.00184741 </td></tr>\n",
       "<tr><td>objective_function_c98f87cc</td><td>TERMINATED</td><td>127.0.0.1:26944</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.391531 </td><td style=\"text-align: right;\">            0.206444  </td><td style=\"text-align: right;\">          1.24306 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.02328e-08</td><td style=\"text-align: right;\">0.000680966</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         595.09 </td><td style=\"text-align: right;\">0.94109 </td><td style=\"text-align: right;\"> 0.000431771</td></tr>\n",
       "<tr><td>objective_function_8be69bf9</td><td>TERMINATED</td><td>127.0.0.1:2316 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.380345 </td><td style=\"text-align: right;\">            0.233444  </td><td style=\"text-align: right;\">          0.597817</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.33936e-06</td><td style=\"text-align: right;\">0.000754406</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         295.044</td><td style=\"text-align: right;\">0.922555</td><td style=\"text-align: right;\"> 0.00310813 </td></tr>\n",
       "<tr><td>objective_function_d87bb988</td><td>TERMINATED</td><td>127.0.0.1:9656 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.38452  </td><td style=\"text-align: right;\">            0.219765  </td><td style=\"text-align: right;\">          0.619547</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">2.32887e-06</td><td style=\"text-align: right;\">0.000966529</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         853.514</td><td style=\"text-align: right;\">0.940818</td><td style=\"text-align: right;\"> 1.10865e-06</td></tr>\n",
       "<tr><td>objective_function_22aee9f1</td><td>TERMINATED</td><td>127.0.0.1:35332</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.408337 </td><td style=\"text-align: right;\">            0.231547  </td><td style=\"text-align: right;\">          2.24771 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">2.0483e-06 </td><td style=\"text-align: right;\">0.000879153</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         718.29 </td><td style=\"text-align: right;\">0.950209</td><td style=\"text-align: right;\"> 5.30348e-05</td></tr>\n",
       "<tr><td>objective_function_13d2113f</td><td>TERMINATED</td><td>127.0.0.1:36648</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.373025 </td><td style=\"text-align: right;\">            0.241468  </td><td style=\"text-align: right;\">          2.1184  </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">2.47263e-06</td><td style=\"text-align: right;\">0.00109126 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         243.435</td><td style=\"text-align: right;\">0.929232</td><td style=\"text-align: right;\"> 0.00145393 </td></tr>\n",
       "<tr><td>objective_function_b661b0d8</td><td>TERMINATED</td><td>127.0.0.1:11460</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.0366769</td><td style=\"text-align: right;\">            0.258092  </td><td style=\"text-align: right;\">          2.29679 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">6.31381e-07</td><td style=\"text-align: right;\">0.00179354 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         429.166</td><td style=\"text-align: right;\">0.941868</td><td style=\"text-align: right;\"> 0.000366655</td></tr>\n",
       "<tr><td>objective_function_4416b048</td><td>TERMINATED</td><td>127.0.0.1:32908</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.0512911</td><td style=\"text-align: right;\">            0.279089  </td><td style=\"text-align: right;\">          1.87369 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">5.12214e-07</td><td style=\"text-align: right;\">0.00150935 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         430.005</td><td style=\"text-align: right;\">0.937924</td><td style=\"text-align: right;\"> 0.00031721 </td></tr>\n",
       "<tr><td>objective_function_b8a24569</td><td>TERMINATED</td><td>127.0.0.1:39876</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.232725 </td><td style=\"text-align: right;\">            0.495915  </td><td style=\"text-align: right;\">          2.48938 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">5.55951e-08</td><td style=\"text-align: right;\">0.00180311 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         239.853</td><td style=\"text-align: right;\">0.931513</td><td style=\"text-align: right;\"> 0.001467   </td></tr>\n",
       "<tr><td>objective_function_5bfb53eb</td><td>TERMINATED</td><td>127.0.0.1:25572</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.430458 </td><td style=\"text-align: right;\">            0.289519  </td><td style=\"text-align: right;\">          2.44274 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">6.78828e-07</td><td style=\"text-align: right;\">0.000432527</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         908.804</td><td style=\"text-align: right;\">0.944012</td><td style=\"text-align: right;\"> 8.83067e-06</td></tr>\n",
       "<tr><td>objective_function_7d6aef79</td><td>TERMINATED</td><td>127.0.0.1:26172</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.444856 </td><td style=\"text-align: right;\">            0.286212  </td><td style=\"text-align: right;\">          2.49994 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">5.93875e-07</td><td style=\"text-align: right;\">0.00188983 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         649.794</td><td style=\"text-align: right;\">0.953454</td><td style=\"text-align: right;\"> 0.00020213 </td></tr>\n",
       "<tr><td>objective_function_40b8d061</td><td>TERMINATED</td><td>127.0.0.1:38988</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.447546 </td><td style=\"text-align: right;\">            0.291913  </td><td style=\"text-align: right;\">          2.48752 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">6.79319e-07</td><td style=\"text-align: right;\">0.000336882</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         492.054</td><td style=\"text-align: right;\">0.934206</td><td style=\"text-align: right;\"> 0.000266298</td></tr>\n",
       "<tr><td>objective_function_3333b7ad</td><td>TERMINATED</td><td>127.0.0.1:40256</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.424287 </td><td style=\"text-align: right;\">            0.171325  </td><td style=\"text-align: right;\">          2.47399 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.03378e-06</td><td style=\"text-align: right;\">0.000509386</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         497.211</td><td style=\"text-align: right;\">0.941295</td><td style=\"text-align: right;\"> 0.000602168</td></tr>\n",
       "<tr><td>objective_function_631bd332</td><td>TERMINATED</td><td>127.0.0.1:23056</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.437803 </td><td style=\"text-align: right;\">            0.295948  </td><td style=\"text-align: right;\">          2.54512 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.23224e-06</td><td style=\"text-align: right;\">0.000465341</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         233.29 </td><td style=\"text-align: right;\">0.933964</td><td style=\"text-align: right;\"> 0.00123983 </td></tr>\n",
       "<tr><td>objective_function_7b06a91a</td><td>TERMINATED</td><td>127.0.0.1:39136</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.185652 </td><td style=\"text-align: right;\">            0.19037   </td><td style=\"text-align: right;\">          1.98204 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.16088e-06</td><td style=\"text-align: right;\">0.00574177 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         241.982</td><td style=\"text-align: right;\">0.927799</td><td style=\"text-align: right;\"> 0.0012277  </td></tr>\n",
       "<tr><td>objective_function_5b8b3dab</td><td>TERMINATED</td><td>127.0.0.1:38276</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.327981 </td><td style=\"text-align: right;\">            0.403446  </td><td style=\"text-align: right;\">          2.70164 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.12102e-07</td><td style=\"text-align: right;\">0.00517697 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         538.417</td><td style=\"text-align: right;\">0.936341</td><td style=\"text-align: right;\"> 0.00365055 </td></tr>\n",
       "<tr><td>objective_function_a6e3db90</td><td>TERMINATED</td><td>127.0.0.1:7532 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.33518  </td><td style=\"text-align: right;\">            0.388849  </td><td style=\"text-align: right;\">          2.77858 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.13943e-07</td><td style=\"text-align: right;\">0.00473105 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         535.584</td><td style=\"text-align: right;\">0.934123</td><td style=\"text-align: right;\"> 0.00304865 </td></tr>\n",
       "<tr><td>objective_function_89c245a2</td><td>TERMINATED</td><td>127.0.0.1:37456</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.175479 </td><td style=\"text-align: right;\">            0.41818   </td><td style=\"text-align: right;\">          2.74142 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.24574e-07</td><td style=\"text-align: right;\">0.00478752 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         533.114</td><td style=\"text-align: right;\">0.939678</td><td style=\"text-align: right;\"> 0.0017614  </td></tr>\n",
       "<tr><td>objective_function_d426208a</td><td>TERMINATED</td><td>127.0.0.1:38616</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.188301 </td><td style=\"text-align: right;\">            0.40456   </td><td style=\"text-align: right;\">          2.77129 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.78152e-07</td><td style=\"text-align: right;\">0.00274817 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         249.825</td><td style=\"text-align: right;\">0.928909</td><td style=\"text-align: right;\"> 0.000754033</td></tr>\n",
       "<tr><td>objective_function_69d35c4d</td><td>TERMINATED</td><td>127.0.0.1:38440</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.323487 </td><td style=\"text-align: right;\">            0.3802    </td><td style=\"text-align: right;\">          2.77092 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.71874e-07</td><td style=\"text-align: right;\">0.0028974  </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         583.353</td><td style=\"text-align: right;\">0.948484</td><td style=\"text-align: right;\"> 0.000394551</td></tr>\n",
       "<tr><td>objective_function_b383681e</td><td>TERMINATED</td><td>127.0.0.1:1316 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.324493 </td><td style=\"text-align: right;\">            0.355664  </td><td style=\"text-align: right;\">          2.81062 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">5.94419e-06</td><td style=\"text-align: right;\">0.00374869 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         223.975</td><td style=\"text-align: right;\">0.932145</td><td style=\"text-align: right;\"> 0.00134612 </td></tr>\n",
       "<tr><td>objective_function_16acfaef</td><td>TERMINATED</td><td>127.0.0.1:32228</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.257327 </td><td style=\"text-align: right;\">            0.344976  </td><td style=\"text-align: right;\">          2.95173 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">4.88324e-06</td><td style=\"text-align: right;\">0.00342531 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         710.452</td><td style=\"text-align: right;\">0.949171</td><td style=\"text-align: right;\"> 0.000199275</td></tr>\n",
       "<tr><td>objective_function_8cb8f75b</td><td>TERMINATED</td><td>127.0.0.1:35668</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.243424 </td><td style=\"text-align: right;\">            0.347715  </td><td style=\"text-align: right;\">          1.64151 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">5.58122e-06</td><td style=\"text-align: right;\">0.0038597  </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         234.17 </td><td style=\"text-align: right;\">0.931472</td><td style=\"text-align: right;\"> 0.00184045 </td></tr>\n",
       "<tr><td>objective_function_d2304c60</td><td>TERMINATED</td><td>127.0.0.1:11532</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.23612  </td><td style=\"text-align: right;\">            0.35891   </td><td style=\"text-align: right;\">          1.71879 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">6.38196e-06</td><td style=\"text-align: right;\">0.00363179 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         239.585</td><td style=\"text-align: right;\">0.93308 </td><td style=\"text-align: right;\"> 0.00160021 </td></tr>\n",
       "<tr><td>objective_function_f3156aff</td><td>TERMINATED</td><td>127.0.0.1:33068</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.216706 </td><td style=\"text-align: right;\">            0.151588  </td><td style=\"text-align: right;\">          1.75374 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">2.01741e-06</td><td style=\"text-align: right;\">0.00316506 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         918.95 </td><td style=\"text-align: right;\">0.940717</td><td style=\"text-align: right;\"> 0.0001538  </td></tr>\n",
       "<tr><td>objective_function_c00348f3</td><td>TERMINATED</td><td>127.0.0.1:8328 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.271645 </td><td style=\"text-align: right;\">            0.159421  </td><td style=\"text-align: right;\">          2.29654 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">2.00581e-06</td><td style=\"text-align: right;\">0.00221819 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">        1210.68 </td><td style=\"text-align: right;\">0.942643</td><td style=\"text-align: right;\"> 9.90469e-05</td></tr>\n",
       "<tr><td>objective_function_9604ca3a</td><td>TERMINATED</td><td>127.0.0.1:37532</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.468922 </td><td style=\"text-align: right;\">            0.153995  </td><td style=\"text-align: right;\">          2.95733 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">2.05001e-06</td><td style=\"text-align: right;\">0.00252998 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         590.611</td><td style=\"text-align: right;\">0.93289 </td><td style=\"text-align: right;\"> 0.000760449</td></tr>\n",
       "<tr><td>objective_function_0a3a99be</td><td>TERMINATED</td><td>127.0.0.1:39852</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.268558 </td><td style=\"text-align: right;\">            0.166358  </td><td style=\"text-align: right;\">          2.26441 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">3.1226e-06 </td><td style=\"text-align: right;\">0.00214391 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        1064.92 </td><td style=\"text-align: right;\">0.940827</td><td style=\"text-align: right;\"> 0.000406521</td></tr>\n",
       "<tr><td>objective_function_59da1b57</td><td>TERMINATED</td><td>127.0.0.1:31464</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.266239 </td><td style=\"text-align: right;\">            0.321699  </td><td style=\"text-align: right;\">          2.99387 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.96248e-06</td><td style=\"text-align: right;\">0.00215694 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         329.497</td><td style=\"text-align: right;\">0.925026</td><td style=\"text-align: right;\"> 0.00144226 </td></tr>\n",
       "<tr><td>objective_function_e964e725</td><td>TERMINATED</td><td>127.0.0.1:38012</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.146167 </td><td style=\"text-align: right;\">            0.308831  </td><td style=\"text-align: right;\">          2.37752 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">2.89403e-06</td><td style=\"text-align: right;\">0.00209232 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         210.702</td><td style=\"text-align: right;\">0.928462</td><td style=\"text-align: right;\"> 0.000961347</td></tr>\n",
       "<tr><td>objective_function_949148c5</td><td>TERMINATED</td><td>127.0.0.1:25572</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.282256 </td><td style=\"text-align: right;\">            0.318412  </td><td style=\"text-align: right;\">          2.32492 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">4.2018e-06 </td><td style=\"text-align: right;\">0.00197689 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         742.019</td><td style=\"text-align: right;\">0.950278</td><td style=\"text-align: right;\"> 2.33241e-05</td></tr>\n",
       "<tr><td>objective_function_06671e54</td><td>TERMINATED</td><td>127.0.0.1:34060</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.147601 </td><td style=\"text-align: right;\">            0.306044  </td><td style=\"text-align: right;\">          2.58738 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">9.20265e-06</td><td style=\"text-align: right;\">0.00124049 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">        1236.3  </td><td style=\"text-align: right;\">0.947744</td><td style=\"text-align: right;\"> 5.81966e-05</td></tr>\n",
       "<tr><td>objective_function_c85fdf92</td><td>TERMINATED</td><td>127.0.0.1:29728</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.289562 </td><td style=\"text-align: right;\">            0.252784  </td><td style=\"text-align: right;\">          2.59407 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">3.88086e-06</td><td style=\"text-align: right;\">0.00107113 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         765.124</td><td style=\"text-align: right;\">0.932535</td><td style=\"text-align: right;\"> 0.000483996</td></tr>\n",
       "<tr><td>objective_function_e84e34f5</td><td>TERMINATED</td><td>127.0.0.1:2792 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.300923 </td><td style=\"text-align: right;\">            0.262994  </td><td style=\"text-align: right;\">          2.56911 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">9.1843e-06 </td><td style=\"text-align: right;\">0.00677032 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         252.022</td><td style=\"text-align: right;\">0.927493</td><td style=\"text-align: right;\"> 0.00101648 </td></tr>\n",
       "<tr><td>objective_function_92e95df2</td><td>TERMINATED</td><td>127.0.0.1:40880</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.0833308</td><td style=\"text-align: right;\">            0.260669  </td><td style=\"text-align: right;\">          2.59283 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.32426e-08</td><td style=\"text-align: right;\">0.00656218 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         458.544</td><td style=\"text-align: right;\">0.935351</td><td style=\"text-align: right;\"> 0.0026024  </td></tr>\n",
       "<tr><td>objective_function_73231d89</td><td>TERMINATED</td><td>127.0.0.1:38540</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.405962 </td><td style=\"text-align: right;\">            0.102066  </td><td style=\"text-align: right;\">          2.07143 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">8.5408e-07 </td><td style=\"text-align: right;\">0.00118849 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         236.373</td><td style=\"text-align: right;\">0.930839</td><td style=\"text-align: right;\"> 0.00142655 </td></tr>\n",
       "<tr><td>objective_function_552ae3ba</td><td>TERMINATED</td><td>127.0.0.1:36540</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.404806 </td><td style=\"text-align: right;\">            0.229559  </td><td style=\"text-align: right;\">          2.08227 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.42993e-08</td><td style=\"text-align: right;\">0.00115018 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         237.822</td><td style=\"text-align: right;\">0.929411</td><td style=\"text-align: right;\"> 0.00124482 </td></tr>\n",
       "<tr><td>objective_function_e7e3b9ff</td><td>TERMINATED</td><td>127.0.0.1:20564</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.358218 </td><td style=\"text-align: right;\">            0.325876  </td><td style=\"text-align: right;\">          2.85588 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">4.3487e-06 </td><td style=\"text-align: right;\">0.00156177 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         212.18 </td><td style=\"text-align: right;\">0.932337</td><td style=\"text-align: right;\"> 0.000864257</td></tr>\n",
       "<tr><td>objective_function_247899ad</td><td>TERMINATED</td><td>127.0.0.1:32052</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.356917 </td><td style=\"text-align: right;\">            0.330617  </td><td style=\"text-align: right;\">          2.89725 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">4.28273e-06</td><td style=\"text-align: right;\">0.00156733 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         661.642</td><td style=\"text-align: right;\">0.950959</td><td style=\"text-align: right;\"> 4.84094e-05</td></tr>\n",
       "<tr><td>objective_function_e5005bc1</td><td>TERMINATED</td><td>127.0.0.1:30308</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.353412 </td><td style=\"text-align: right;\">            0.332873  </td><td style=\"text-align: right;\">          1.06885 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">4.53806e-06</td><td style=\"text-align: right;\">0.000843362</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         217.24 </td><td style=\"text-align: right;\">0.934443</td><td style=\"text-align: right;\"> 0.0024882  </td></tr>\n",
       "<tr><td>objective_function_3c4462db</td><td>TERMINATED</td><td>127.0.0.1:40380</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.207076 </td><td style=\"text-align: right;\">            0.436686  </td><td style=\"text-align: right;\">          1.27973 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.4645e-06 </td><td style=\"text-align: right;\">0.000871334</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         411.852</td><td style=\"text-align: right;\">0.938118</td><td style=\"text-align: right;\"> 0.00115969 </td></tr>\n",
       "<tr><td>objective_function_7f0a7153</td><td>TERMINATED</td><td>127.0.0.1:39152</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.210721 </td><td style=\"text-align: right;\">            0.30868   </td><td style=\"text-align: right;\">          1.16885 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.68179e-06</td><td style=\"text-align: right;\">0.00141348 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         230.792</td><td style=\"text-align: right;\">0.931365</td><td style=\"text-align: right;\"> 0.00298212 </td></tr>\n",
       "<tr><td>objective_function_538d2d11</td><td>TERMINATED</td><td>127.0.0.1:37068</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.202668 </td><td style=\"text-align: right;\">            0.276418  </td><td style=\"text-align: right;\">          2.87385 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">2.89097e-07</td><td style=\"text-align: right;\">0.00327923 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         441.554</td><td style=\"text-align: right;\">0.954798</td><td style=\"text-align: right;\"> 0.000195025</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m The log monitor on node Negm failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\shutil.py\", line 816, in move\n",
      "    os.rename(src, real_dst)\n",
      "PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\KARIMN~1\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2025-11-14_23-53-24_504603_31812\\\\logs\\\\worker-c28de2b285b56b97dc6d4ade1d817eed27ba6bf613c6feb104cc44bd-01000000-2792.err' -> 'C:\\\\Users\\\\KARIMN~1\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2025-11-14_23-53-24_504603_31812\\\\logs\\\\old\\\\worker-c28de2b285b56b97dc6d4ade1d817eed27ba6bf613c6feb104cc44bd-01000000-2792.err'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\site-packages\\ray\\_private\\log_monitor.py\", line 610, in <module>\n",
      "    log_monitor.run()\n",
      "  File \"e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\site-packages\\ray\\_private\\log_monitor.py\", line 485, in run\n",
      "    self.open_closed_files()\n",
      "  File \"e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\site-packages\\ray\\_private\\log_monitor.py\", line 301, in open_closed_files\n",
      "    self._close_all_files()\n",
      "  File \"e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\site-packages\\ray\\_private\\log_monitor.py\", line 222, in _close_all_files\n",
      "    raise e\n",
      "  File \"e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\site-packages\\ray\\_private\\log_monitor.py\", line 215, in _close_all_files\n",
      "    shutil.move(file_info.filename, target)\n",
      "  File \"e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\shutil.py\", line 837, in move\n",
      "    os.unlink(src)\n",
      "PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\KARIMN~1\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2025-11-14_23-53-24_504603_31812\\\\logs\\\\worker-c28de2b285b56b97dc6d4ade1d817eed27ba6bf613c6feb104cc44bd-01000000-2792.err'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 02:20:09,121\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/Karim Negm/ray_results/pirate_pain_focalloss_search_v12' in 0.0876s.\n",
      "2025-11-15 02:20:09,160\tINFO tune.py:1041 -- Total run time: 8793.30 seconds (8793.12 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# --- MODIFIED: Search space with rnn_type and focal_loss_gamma ---\n",
    "search_space = {\n",
    "    \"rnn_type\": tune.choice(['GRU', 'LSTM']),\n",
    "    \"focal_loss_gamma\": tune.uniform(0.5, 3.0),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2), # This will be the max_lr for OneCycleLR\n",
    "    \"batch_size\": tune.choice([64, 128]),\n",
    "    \"hidden_size\": tune.choice([256, 384, 512]), \n",
    "    \"num_layers\": tune.choice([2, 3]),\n",
    "    \"dropout_rate\": tune.uniform(0, 0.5), \n",
    "    \"feature_dropout_rate\": tune.uniform(0, 0.5),\n",
    "    \"bidirectional\": tune.choice([True, False]), \n",
    "    \"l2_lambda\": tune.loguniform(1e-8, 1e-5),\n",
    "    \"conv_out_channels\": tune.choice([128]), \n",
    "    \"conv_kernel_size\": tune.choice([5])\n",
    "}\n",
    "\n",
    "def short_trial_name(trial): return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "if ray.is_initialized(): ray.shutdown()\n",
    "ray.init(num_cpus=os.cpu_count(), num_gpus=1, ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "print(\"--- Starting HPO with Focal Loss and RNN-Type search ---\")\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(objective_function, \n",
    "                         X_train_w=X_train_w_torch, y_train_w=y_train_w_torch,\n",
    "                         X_val_w=X_val_w_torch, y_val_w=y_val_w_torch,\n",
    "                         alpha_tensor=alpha_tensor),\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 0.25},\n",
    "    config=search_space, \n",
    "    num_samples=60, \n",
    "    search_alg=OptunaSearch(metric=\"val_f1\", mode=\"max\"),\n",
    "    scheduler=ASHAScheduler(metric=\"val_f1\", mode=\"max\", grace_period=25, reduction_factor=2),\n",
    "    name=\"pirate_pain_focalloss_search_v12\", \n",
    "    verbose=1,\n",
    "    trial_dirname_creator=short_trial_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0bbf60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Partial Search Results ---\n",
      "Loading analysis from: C:\\Users\\Karim Negm/ray_results/pirate_pain_focalloss_search_v12\n",
      "Getting best *completed* trial from partial analysis...\n",
      "Best validation F1 score from completed trials: 0.9510\n",
      "Best hyperparameters found:\n",
      "{'rnn_type': 'GRU', 'focal_loss_gamma': 2.897247566896053, 'lr': 0.001567334435113753, 'batch_size': 128, 'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.35691736164090826, 'feature_dropout_rate': 0.3306172575332418, 'bidirectional': False, 'l2_lambda': 4.28273386221474e-06, 'conv_out_channels': 128, 'conv_kernel_size': 5}\n",
      "\n",
      "--- Ready to proceed to K-Fold Training ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Loading Partial Search Results ---\")\n",
    "\n",
    "# --- 1. Define the path to your experiment results ---\n",
    "# Ray's default is '~/ray_results/<experiment_name>'\n",
    "# The traceback confirms your user folder, so this path should work.\n",
    "experiment_path = os.path.expanduser(\"~/ray_results/pirate_pain_focalloss_search_v12\")\n",
    "\n",
    "# --- 2. Load the analysis from disk ---\n",
    "print(f\"Loading analysis from: {experiment_path}\")\n",
    "try:\n",
    "    analysis = tune.ExperimentAnalysis(experiment_path)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load analysis from {experiment_path}\")\n",
    "    print(f\"Make sure the path is correct. The error was: {e}\")\n",
    "    # Raise the error to stop the notebook if the path is wrong\n",
    "    raise e\n",
    "\n",
    "# --- 3. Get the best trial from the *completed* runs ---\n",
    "print(\"Getting best *completed* trial from partial analysis...\")\n",
    "best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "\n",
    "# --- 4. The rest of your original cell logic ---\n",
    "if best_trial:\n",
    "    FINAL_CONFIG = best_trial.config\n",
    "    # Check if 'val_f1' exists, use a default if not\n",
    "    FINAL_BEST_VAL_F1 = best_trial.last_result.get(\"val_f1\", 0.0) \n",
    "    print(f\"Best validation F1 score from completed trials: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(FINAL_CONFIG)\n",
    "else:\n",
    "    print(\"ERROR: No trials completed successfully. Using a default config.\")\n",
    "    FINAL_CONFIG = {'rnn_type': 'GRU', 'focal_loss_gamma': 2.0, 'lr': 0.001, 'batch_size': 128, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.3, 'feature_dropout_rate': 0.3, 'bidirectional': True, 'l2_lambda': 1e-06, 'conv_out_channels': 128, 'conv_kernel_size': 5}\n",
    "    FINAL_BEST_VAL_F1 = 0.0\n",
    "\n",
    "# Add the fixed windowing params to the final config for the next steps\n",
    "# Make sure WINDOW_SIZE and STRIDE are defined (they should be from Cell 5)\n",
    "FINAL_CONFIG['window_size'] = WINDOW_SIZE\n",
    "FINAL_CONFIG['stride'] = STRIDE\n",
    "\n",
    "# Clean up HPO data to save memory\n",
    "try:\n",
    "    del X_train_w_torch, y_train_w_torch, X_val_w_torch, y_val_w_torch\n",
    "    del X_train_split, y_train_split, X_val_split, y_val_split\n",
    "except NameError:\n",
    "    print(\"Data already cleaned up or not in memory.\")\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n--- Ready to proceed to K-Fold Training ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c0bb8",
   "metadata": {},
   "source": [
    "## üèÜ 6. Phase 2: K-Fold Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13308e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèÜ Final Configuration Set --- \n",
      "Best Val F1 from HPO search: 0.9510\n",
      "{'rnn_type': 'GRU', 'focal_loss_gamma': 2.897247566896053, 'lr': 0.001567334435113753, 'batch_size': 128, 'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.35691736164090826, 'feature_dropout_rate': 0.3306172575332418, 'bidirectional': False, 'l2_lambda': 4.28273386221474e-06, 'conv_out_channels': 128, 'conv_kernel_size': 5, 'window_size': 10, 'stride': 2}\n",
      "Submission name will be: submission_Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"--- üèÜ Final Configuration Set --- \")\n",
    "print(f\"Best Val F1 from HPO search: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "print(FINAL_CONFIG)\n",
    "\n",
    "N_SPLITS = 5\n",
    "FINAL_EXPERIMENT_NAME = f\"Compiled-FocalLoss-{FINAL_CONFIG['rnn_type']}_H{FINAL_CONFIG['hidden_size']}_L{FINAL_CONFIG['num_layers']}_\"\\\n",
    "                      f\"C{FINAL_CONFIG['conv_out_channels']}_K{FINAL_CONFIG['conv_kernel_size']}_v12\"\n",
    "submission_filename_base = f\"submission_{FINAL_EXPERIMENT_NAME}.csv\"\n",
    "print(f\"Submission name will be: {submission_filename_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bda55dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/5 --- (Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12_fold_1) ---\n",
      "--- Starting Training: Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12_fold_1 ---\n",
      "Epoch   5/350 | Val F1: 0.9180 | LR: 0.000071\n",
      "Epoch  10/350 | Val F1: 0.9301 | LR: 0.000096\n",
      "Epoch  15/350 | Val F1: 0.9378 | LR: 0.000137\n",
      "Epoch  20/350 | Val F1: 0.9232 | LR: 0.000193\n",
      "Epoch  25/350 | Val F1: 0.9253 | LR: 0.000264\n",
      "Epoch  30/350 | Val F1: 0.9488 | LR: 0.000346\n",
      "Epoch  35/350 | Val F1: 0.9489 | LR: 0.000439\n",
      "Epoch  40/350 | Val F1: 0.9395 | LR: 0.000540\n",
      "Epoch  45/350 | Val F1: 0.9406 | LR: 0.000648\n",
      "Epoch  50/350 | Val F1: 0.9397 | LR: 0.000759\n",
      "Epoch  55/350 | Val F1: 0.9433 | LR: 0.000871\n",
      "Epoch  60/350 | Val F1: 0.9415 | LR: 0.000982\n",
      "Epoch  65/350 | Val F1: 0.9438 | LR: 0.001090\n",
      "Epoch  70/350 | Val F1: 0.9397 | LR: 0.001191\n",
      "Epoch  75/350 | Val F1: 0.9456 | LR: 0.001284\n",
      "Epoch  80/350 | Val F1: 0.9464 | LR: 0.001367\n",
      "Epoch  85/350 | Val F1: 0.9416 | LR: 0.001437\n",
      "Epoch  90/350 | Val F1: 0.9374 | LR: 0.001493\n",
      "Epoch  95/350 | Val F1: 0.9397 | LR: 0.001534\n",
      "Epoch 100/350 | Val F1: 0.9454 | LR: 0.001559\n",
      "Epoch 105/350 | Val F1: 0.9490 | LR: 0.001567\n",
      "Epoch 110/350 | Val F1: 0.9381 | LR: 0.001566\n",
      "Epoch 115/350 | Val F1: 0.9379 | LR: 0.001561\n",
      "Epoch 120/350 | Val F1: 0.9442 | LR: 0.001553\n",
      "Epoch 125/350 | Val F1: 0.9434 | LR: 0.001542\n",
      "Epoch 130/350 | Val F1: 0.9486 | LR: 0.001527\n",
      "Epoch 135/350 | Val F1: 0.9364 | LR: 0.001510\n",
      "Epoch 140/350 | Val F1: 0.9350 | LR: 0.001490\n",
      "Early stopping at epoch 142. Best F1: 0.9506\n",
      "--- Finished Training --- Best F1: 0.9506\n",
      "Fold 1 Final Val F1: 0.9506\n",
      "\n",
      "--- Fold 2/5 --- (Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12_fold_2) ---\n",
      "--- Starting Training: Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12_fold_2 ---\n",
      "Epoch   5/350 | Val F1: 0.8989 | LR: 0.000071\n",
      "Epoch  10/350 | Val F1: 0.9178 | LR: 0.000096\n",
      "Epoch  15/350 | Val F1: 0.9176 | LR: 0.000137\n",
      "Epoch  20/350 | Val F1: 0.9172 | LR: 0.000193\n",
      "Epoch  25/350 | Val F1: 0.9152 | LR: 0.000264\n",
      "Epoch  30/350 | Val F1: 0.9234 | LR: 0.000346\n",
      "Epoch  35/350 | Val F1: 0.9242 | LR: 0.000439\n",
      "Epoch  40/350 | Val F1: 0.9227 | LR: 0.000540\n",
      "Epoch  45/350 | Val F1: 0.9264 | LR: 0.000648\n",
      "Epoch  50/350 | Val F1: 0.9251 | LR: 0.000759\n",
      "Epoch  55/350 | Val F1: 0.9280 | LR: 0.000871\n",
      "Epoch  60/350 | Val F1: 0.9279 | LR: 0.000982\n",
      "Epoch  65/350 | Val F1: 0.9166 | LR: 0.001090\n",
      "Epoch  70/350 | Val F1: 0.9310 | LR: 0.001191\n",
      "Epoch  75/350 | Val F1: 0.9282 | LR: 0.001284\n",
      "Epoch  80/350 | Val F1: 0.9033 | LR: 0.001367\n",
      "Epoch  85/350 | Val F1: 0.9258 | LR: 0.001437\n",
      "Epoch  90/350 | Val F1: 0.9177 | LR: 0.001493\n",
      "Epoch  95/350 | Val F1: 0.9214 | LR: 0.001534\n",
      "Epoch 100/350 | Val F1: 0.9179 | LR: 0.001559\n",
      "Epoch 105/350 | Val F1: 0.9233 | LR: 0.001567\n",
      "Epoch 110/350 | Val F1: 0.9256 | LR: 0.001566\n",
      "Epoch 115/350 | Val F1: 0.9260 | LR: 0.001561\n",
      "Epoch 120/350 | Val F1: 0.9220 | LR: 0.001553\n",
      "Epoch 125/350 | Val F1: 0.9197 | LR: 0.001542\n",
      "Epoch 130/350 | Val F1: 0.9212 | LR: 0.001527\n",
      "Epoch 135/350 | Val F1: 0.9299 | LR: 0.001510\n",
      "Epoch 140/350 | Val F1: 0.9168 | LR: 0.001490\n",
      "Epoch 145/350 | Val F1: 0.9222 | LR: 0.001466\n",
      "Epoch 150/350 | Val F1: 0.9288 | LR: 0.001440\n",
      "Epoch 155/350 | Val F1: 0.9282 | LR: 0.001412\n",
      "Early stopping at epoch 159. Best F1: 0.9336\n",
      "--- Finished Training --- Best F1: 0.9336\n",
      "Fold 2 Final Val F1: 0.9336\n",
      "\n",
      "--- Fold 3/5 --- (Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12_fold_3) ---\n",
      "--- Starting Training: Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12_fold_3 ---\n",
      "Epoch   5/350 | Val F1: 0.8959 | LR: 0.000071\n",
      "Epoch  10/350 | Val F1: 0.8957 | LR: 0.000096\n",
      "Epoch  15/350 | Val F1: 0.9062 | LR: 0.000137\n",
      "Epoch  20/350 | Val F1: 0.9154 | LR: 0.000193\n",
      "Epoch  25/350 | Val F1: 0.9122 | LR: 0.000264\n",
      "Epoch  30/350 | Val F1: 0.9199 | LR: 0.000346\n",
      "Epoch  35/350 | Val F1: 0.9134 | LR: 0.000439\n",
      "Epoch  40/350 | Val F1: 0.9174 | LR: 0.000540\n",
      "Epoch  45/350 | Val F1: 0.9261 | LR: 0.000648\n",
      "Epoch  50/350 | Val F1: 0.9167 | LR: 0.000759\n",
      "Epoch  55/350 | Val F1: 0.9276 | LR: 0.000871\n",
      "Epoch  60/350 | Val F1: 0.9182 | LR: 0.000982\n",
      "Epoch  65/350 | Val F1: 0.9203 | LR: 0.001090\n",
      "Epoch  70/350 | Val F1: 0.9278 | LR: 0.001191\n",
      "Epoch  75/350 | Val F1: 0.9292 | LR: 0.001284\n",
      "Epoch  80/350 | Val F1: 0.9218 | LR: 0.001367\n",
      "Epoch  85/350 | Val F1: 0.9292 | LR: 0.001437\n",
      "Epoch  90/350 | Val F1: 0.9238 | LR: 0.001493\n",
      "Epoch  95/350 | Val F1: 0.9241 | LR: 0.001534\n",
      "Epoch 100/350 | Val F1: 0.9272 | LR: 0.001559\n",
      "Epoch 105/350 | Val F1: 0.9268 | LR: 0.001567\n",
      "Epoch 110/350 | Val F1: 0.9314 | LR: 0.001566\n",
      "Epoch 115/350 | Val F1: 0.9308 | LR: 0.001561\n",
      "Epoch 120/350 | Val F1: 0.9310 | LR: 0.001553\n",
      "Epoch 125/350 | Val F1: 0.9252 | LR: 0.001542\n",
      "Epoch 130/350 | Val F1: 0.9235 | LR: 0.001527\n",
      "Epoch 135/350 | Val F1: 0.9241 | LR: 0.001510\n",
      "Epoch 140/350 | Val F1: 0.9167 | LR: 0.001490\n",
      "Epoch 145/350 | Val F1: 0.9295 | LR: 0.001466\n",
      "Epoch 150/350 | Val F1: 0.9218 | LR: 0.001440\n",
      "Epoch 155/350 | Val F1: 0.9271 | LR: 0.001412\n",
      "Epoch 160/350 | Val F1: 0.9250 | LR: 0.001380\n",
      "Epoch 165/350 | Val F1: 0.9176 | LR: 0.001347\n",
      "Epoch 170/350 | Val F1: 0.9243 | LR: 0.001311\n",
      "Epoch 175/350 | Val F1: 0.9317 | LR: 0.001272\n",
      "Epoch 180/350 | Val F1: 0.9247 | LR: 0.001232\n",
      "Epoch 185/350 | Val F1: 0.9286 | LR: 0.001190\n",
      "Epoch 190/350 | Val F1: 0.9279 | LR: 0.001146\n",
      "Epoch 195/350 | Val F1: 0.9296 | LR: 0.001101\n",
      "Epoch 200/350 | Val F1: 0.9246 | LR: 0.001054\n",
      "Epoch 205/350 | Val F1: 0.9246 | LR: 0.001007\n",
      "Epoch 210/350 | Val F1: 0.9286 | LR: 0.000958\n",
      "Epoch 215/350 | Val F1: 0.9289 | LR: 0.000909\n",
      "Epoch 220/350 | Val F1: 0.9261 | LR: 0.000859\n",
      "Epoch 225/350 | Val F1: 0.9275 | LR: 0.000809\n",
      "Epoch 230/350 | Val F1: 0.9240 | LR: 0.000759\n",
      "Epoch 235/350 | Val F1: 0.9220 | LR: 0.000708\n",
      "Epoch 240/350 | Val F1: 0.9194 | LR: 0.000659\n",
      "Early stopping at epoch 244. Best F1: 0.9346\n",
      "--- Finished Training --- Best F1: 0.9346\n",
      "Fold 3 Final Val F1: 0.9346\n",
      "\n",
      "--- Fold 4/5 --- (Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12_fold_4) ---\n",
      "--- Starting Training: Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12_fold_4 ---\n",
      "Epoch   5/350 | Val F1: 0.9070 | LR: 0.000071\n",
      "Epoch  10/350 | Val F1: 0.9196 | LR: 0.000096\n",
      "Epoch  15/350 | Val F1: 0.9378 | LR: 0.000137\n",
      "Epoch  20/350 | Val F1: 0.9278 | LR: 0.000193\n",
      "Epoch  25/350 | Val F1: 0.9201 | LR: 0.000264\n",
      "Epoch  30/350 | Val F1: 0.9180 | LR: 0.000346\n",
      "Epoch  35/350 | Val F1: 0.9217 | LR: 0.000439\n",
      "Epoch  40/350 | Val F1: 0.9181 | LR: 0.000540\n",
      "Epoch  45/350 | Val F1: 0.9311 | LR: 0.000648\n",
      "Epoch  50/350 | Val F1: 0.9318 | LR: 0.000759\n",
      "Epoch  55/350 | Val F1: 0.9266 | LR: 0.000871\n",
      "Epoch  60/350 | Val F1: 0.9303 | LR: 0.000982\n",
      "Epoch  65/350 | Val F1: 0.9283 | LR: 0.001090\n",
      "Epoch  70/350 | Val F1: 0.9326 | LR: 0.001191\n",
      "Epoch  75/350 | Val F1: 0.9225 | LR: 0.001284\n",
      "Epoch  80/350 | Val F1: 0.9410 | LR: 0.001367\n",
      "Epoch  85/350 | Val F1: 0.9194 | LR: 0.001437\n",
      "Epoch  90/350 | Val F1: 0.9301 | LR: 0.001493\n",
      "Epoch  95/350 | Val F1: 0.9345 | LR: 0.001534\n",
      "Epoch 100/350 | Val F1: 0.9276 | LR: 0.001559\n",
      "Epoch 105/350 | Val F1: 0.9303 | LR: 0.001567\n",
      "Epoch 110/350 | Val F1: 0.9409 | LR: 0.001566\n",
      "Epoch 115/350 | Val F1: 0.9319 | LR: 0.001561\n",
      "Epoch 120/350 | Val F1: 0.9271 | LR: 0.001553\n",
      "Epoch 125/350 | Val F1: 0.9344 | LR: 0.001542\n",
      "Epoch 130/350 | Val F1: 0.9302 | LR: 0.001527\n",
      "Epoch 135/350 | Val F1: 0.9310 | LR: 0.001510\n",
      "Epoch 140/350 | Val F1: 0.9283 | LR: 0.001490\n",
      "Epoch 145/350 | Val F1: 0.9351 | LR: 0.001466\n",
      "Epoch 150/350 | Val F1: 0.9200 | LR: 0.001440\n",
      "Epoch 155/350 | Val F1: 0.9236 | LR: 0.001412\n",
      "Epoch 160/350 | Val F1: 0.9312 | LR: 0.001380\n",
      "Epoch 165/350 | Val F1: 0.9242 | LR: 0.001347\n",
      "Epoch 170/350 | Val F1: 0.9275 | LR: 0.001311\n",
      "Epoch 175/350 | Val F1: 0.9308 | LR: 0.001272\n",
      "Epoch 180/350 | Val F1: 0.9283 | LR: 0.001232\n",
      "Early stopping at epoch 180. Best F1: 0.9410\n",
      "--- Finished Training --- Best F1: 0.9410\n",
      "Fold 4 Final Val F1: 0.9410\n",
      "\n",
      "--- Fold 5/5 --- (Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12_fold_5) ---\n",
      "--- Starting Training: Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12_fold_5 ---\n",
      "Epoch   5/350 | Val F1: 0.8709 | LR: 0.000071\n",
      "Epoch  10/350 | Val F1: 0.8930 | LR: 0.000096\n",
      "Epoch  15/350 | Val F1: 0.8903 | LR: 0.000137\n",
      "Epoch  20/350 | Val F1: 0.8952 | LR: 0.000193\n",
      "Epoch  25/350 | Val F1: 0.8889 | LR: 0.000264\n",
      "Epoch  30/350 | Val F1: 0.9086 | LR: 0.000346\n",
      "Epoch  35/350 | Val F1: 0.9022 | LR: 0.000439\n",
      "Epoch  40/350 | Val F1: 0.9007 | LR: 0.000540\n",
      "Epoch  45/350 | Val F1: 0.9072 | LR: 0.000648\n",
      "Epoch  50/350 | Val F1: 0.9010 | LR: 0.000759\n",
      "Epoch  55/350 | Val F1: 0.8961 | LR: 0.000871\n",
      "Epoch  60/350 | Val F1: 0.9055 | LR: 0.000982\n",
      "Epoch  65/350 | Val F1: 0.9029 | LR: 0.001090\n",
      "Epoch  70/350 | Val F1: 0.8997 | LR: 0.001191\n",
      "Epoch  75/350 | Val F1: 0.9074 | LR: 0.001284\n",
      "Epoch  80/350 | Val F1: 0.8914 | LR: 0.001367\n",
      "Epoch  85/350 | Val F1: 0.8948 | LR: 0.001437\n",
      "Epoch  90/350 | Val F1: 0.9029 | LR: 0.001493\n",
      "Epoch  95/350 | Val F1: 0.9095 | LR: 0.001534\n",
      "Epoch 100/350 | Val F1: 0.9020 | LR: 0.001559\n",
      "Epoch 105/350 | Val F1: 0.8994 | LR: 0.001567\n",
      "Epoch 110/350 | Val F1: 0.9034 | LR: 0.001566\n",
      "Epoch 115/350 | Val F1: 0.8980 | LR: 0.001561\n",
      "Epoch 120/350 | Val F1: 0.8996 | LR: 0.001553\n",
      "Epoch 125/350 | Val F1: 0.8893 | LR: 0.001542\n",
      "Epoch 130/350 | Val F1: 0.8862 | LR: 0.001527\n",
      "Epoch 135/350 | Val F1: 0.9013 | LR: 0.001510\n",
      "Epoch 140/350 | Val F1: 0.8956 | LR: 0.001490\n",
      "Epoch 145/350 | Val F1: 0.8958 | LR: 0.001466\n",
      "Epoch 150/350 | Val F1: 0.8941 | LR: 0.001440\n",
      "Epoch 155/350 | Val F1: 0.8909 | LR: 0.001412\n",
      "Epoch 160/350 | Val F1: 0.8931 | LR: 0.001380\n",
      "Epoch 165/350 | Val F1: 0.8992 | LR: 0.001347\n",
      "Epoch 170/350 | Val F1: 0.8990 | LR: 0.001311\n",
      "Epoch 175/350 | Val F1: 0.8956 | LR: 0.001272\n",
      "Epoch 180/350 | Val F1: 0.8988 | LR: 0.001232\n",
      "Epoch 185/350 | Val F1: 0.9013 | LR: 0.001190\n",
      "Early stopping at epoch 186. Best F1: 0.9130\n",
      "--- Finished Training --- Best F1: 0.9130\n",
      "Fold 5 Final Val F1: 0.9130\n",
      "\n",
      "--- üèÜ K-Fold Training Complete --- Average F1: 0.9346\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "fold_val_f1_list = []\n",
    "continuous_indices_reordered = list(range(32))\n",
    "EPOCHS = 350\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_full_reordered, y_train_full)):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} --- ({fold_name}) ---\")\n",
    "    \n",
    "    X_train_fold, y_train_fold = X_train_full_reordered[train_idx], y_train_full[train_idx]\n",
    "    X_val_fold, y_val_fold = X_train_full_reordered[val_idx], y_train_full[val_idx]\n",
    "\n",
    "    preprocessor_fold = ColumnTransformer([('s', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "    ns, ts, f = X_train_fold.shape\n",
    "    X_train_scaled = preprocessor_fold.fit_transform(X_train_fold.reshape(ns*ts, f)).reshape(ns, ts, f)\n",
    "    ns_val, ts_val, f_val = X_val_fold.shape\n",
    "    X_val_scaled = preprocessor_fold.transform(X_val_fold.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "    \n",
    "    X_train_w, y_train_w, _ = create_sliding_windows(X_train_scaled, y_train_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(X_val_scaled, y_val_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    \n",
    "    train_loader = make_loader(TensorDataset(torch.from_numpy(X_train_w).float(), torch.from_numpy(y_train_w).long()), FINAL_CONFIG['batch_size'], True, True)\n",
    "    val_loader = make_loader(TensorDataset(torch.from_numpy(X_val_w).float(), torch.from_numpy(y_val_w).long()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "    model_config_kfold = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "    model_fold = RecurrentClassifier(**model_config_kfold, num_classes=N_CLASSES).to(device)\n",
    "    model_fold = torch.compile(model_fold, backend=\"eager\") # <-- SPEEDUP\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model_fold.parameters(), lr=FINAL_CONFIG['lr'], weight_decay=FINAL_CONFIG['l2_lambda'])\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=FINAL_CONFIG['lr'], epochs=EPOCHS, steps_per_epoch=len(train_loader))\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = FocalLoss(alpha=alpha_tensor, gamma=FINAL_CONFIG['focal_loss_gamma'])\n",
    "\n",
    "    # Note: `fit` returns the uncompiled model, which is what we want for saving/inference\n",
    "    model_fold_uncompiled = fit(model_fold, train_loader, val_loader, EPOCHS, criterion, optimizer, scheduler, scaler, device, 100, fold_name)\n",
    "    \n",
    "    # Re-validate with the uncompiled model to get final score\n",
    "    _, val_f1 = validate_one_epoch(model_fold_uncompiled, val_loader, criterion, device)\n",
    "    fold_val_f1_list.append(val_f1)\n",
    "    print(f\"Fold {fold+1} Final Val F1: {val_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n--- üèÜ K-Fold Training Complete --- Average F1: {np.mean(fold_val_f1_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9d380",
   "metadata": {},
   "source": [
    "## üì¨ 7. Phase 3: Ensemble Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31871e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing test dataset for submission ---\n",
      "Loading model 2/5 from models/Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12_fold_2_best_model.pt...\n",
      "Loading model 3/5 from models/Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12_fold_3_best_model.pt...\n",
      "Loading model 4/5 from models/Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12_fold_4_best_model.pt...\n",
      "\n",
      "Successfully saved to submissions\\submission_Compiled-FocalLoss-GRU_H512_L2_C128_K5_v12.csv!\n",
      "  sample_index    label\n",
      "0          000  no_pain\n",
      "1          001  no_pain\n",
      "2          002  no_pain\n",
      "3          003  no_pain\n",
      "4          004  no_pain\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Preparing test dataset for submission ---\")\n",
    "continuous_indices_orig = list(range(31)) + [35]\n",
    "categorical_indices_orig = list(range(31, 35)) + [36]\n",
    "X_test_full_reordered = np.concatenate([\n",
    "    X_test_full_engineered[:, :, continuous_indices_orig],\n",
    "    X_test_full_engineered[:, :, categorical_indices_orig]], axis=2)\n",
    "\n",
    "continuous_indices_reordered = list(range(32))\n",
    "preprocessor_final = ColumnTransformer([('scaler', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "\n",
    "ns, ts, f = X_train_full_reordered.shape\n",
    "preprocessor_final.fit(X_train_full_reordered.reshape(ns * ts, f))\n",
    "\n",
    "ns_test, ts_test, f_test = X_test_full_reordered.shape\n",
    "X_test_scaled = preprocessor_final.transform(X_test_full_reordered.reshape(ns_test * ts_test, f_test)).reshape(ns_test, ts_test, f_test)\n",
    "X_test_w, test_window_indices = create_sliding_windows(X_test_scaled, y=None, window_size=FINAL_CONFIG['window_size'], stride=FINAL_CONFIG['stride'])\n",
    "test_loader = make_loader(TensorDataset(torch.from_numpy(X_test_w).float()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "model_config_final = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "all_fold_probabilities = []\n",
    "\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    print(f\"Loading model {fold+1}/{N_SPLITS} from {model_path}...\")\n",
    "    # Load state_dict into the uncompiled model structure\n",
    "    model_fold = RecurrentClassifier(**model_config_final, num_classes=N_CLASSES).to(device)\n",
    "    model_fold.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model_fold = torch.compile(model_fold, backend=\"eager\") # <-- Compile for faster inference\n",
    "    model_fold.eval()\n",
    "    \n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in test_loader:\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                probs = torch.softmax(model_fold(inputs.to(device)), dim=1)\n",
    "                fold_preds.append(probs.cpu().numpy())\n",
    "    all_fold_probabilities.append(np.concatenate(fold_preds))\n",
    "\n",
    "mean_probabilities = np.mean(all_fold_probabilities, axis=0)\n",
    "df_probs = pd.DataFrame(mean_probabilities, columns=[f\"prob_{i}\" for i in range(N_CLASSES)])\n",
    "df_probs['original_index'] = test_window_indices\n",
    "agg_probs = df_probs.groupby('original_index')[[f\"prob_{i}\" for i in range(N_CLASSES)]].mean().values\n",
    "final_predictions = le.inverse_transform(np.argmax(agg_probs, axis=1))\n",
    "\n",
    "submission_df = pd.DataFrame({'sample_index': sorted(X_test_long_df['sample_index'].unique()), 'label': final_predictions})\n",
    "submission_df['sample_index'] = submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "submission_filepath = os.path.join(\"submissions\", submission_filename_base)\n",
    "submission_df.to_csv(submission_filepath, index=False)\n",
    "print(f\"\\nSuccessfully saved to {submission_filepath}!\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
