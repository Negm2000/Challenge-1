{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67bb0a3d",
   "metadata": {},
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è (v8: Attention + Label Smoothing)**\n",
    "\n",
    "This notebook introduces an **Attention Mechanism** and **Label Smoothing** to our Conv1d-GRU architecture, aiming to push past the 0.96 plateau and claim the top spot.\n",
    "\n",
    "**Strategy Update:**\n",
    "1.  **Label Smoothing (Professor's Hint):** We replace the standard `CrossEntropyLoss`. By softening the target labels (e.g., `[0.05, 0.9, 0.05]` instead of `[0, 1, 0]`), we create a more robust regularizer that prevents the model from becoming overconfident, improving generalization.\n",
    "2.  **Attention Mechanism:** Instead of relying only on the GRU's final hidden state, we add an Attention layer. This allows the model to intelligently focus on the most relevant time steps in the sequence when making a prediction, creating a more powerful and context-aware representation.\n",
    "3.  **Hyperparameter Search:** The new `label_smoothing_factor` is added to the Ray Tune & Optuna search space to find the optimal smoothing value.\n",
    "4.  **K-Fold Ensemble:** The best configuration, now including Attention and Label Smoothing, is trained on 5 folds for a robust final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03291d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Setup & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba22088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GPU ---\n",
      "PyTorch version: 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Sklearn Imports ---\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Ray[tune] & Optuna Imports ---\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from functools import partial\n",
    "\n",
    "# --- Setup Directories & Device ---\n",
    "logs_dir = \"tensorboard\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8582ef",
   "metadata": {},
   "source": [
    "## üîÑ 2. Data Loading & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44203b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Data ---\n",
      "Loaded X_train_full (shape: (661, 160, 36)) and y_train_full (shape: (661,))\n",
      "Loaded X_test_full (shape: (1324, 160, 36))\n",
      "\n",
      "--- 2. Engineering 'is_pirate' Feature ---\n",
      "Created X_train_full_engineered (shape: (661, 160, 37))\n",
      "Created X_test_full_engineered (shape: (1324, 160, 37))\n",
      "N_FEATURES is now: 37\n",
      "\n",
      "--- 3. Calculating Class Weights ---\n",
      "Class counts (0, 1, 2): [511  94  56]\n",
      "Calculated class weights: tensor([0.0643, 0.3493, 0.5864], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Loading Data ---\")\n",
    "\n",
    "# --- Define File Paths and Features ---\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "try:\n",
    "    features_long_df = pd.read_csv(X_TRAIN_PATH)\n",
    "    labels_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "    X_test_long_df = pd.read_csv(X_TEST_PATH)\n",
    "    \n",
    "    N_TIMESTEPS = 160\n",
    "    JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(31)]\n",
    "    PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "    TIME_FEATURE = ['time']\n",
    "    FEATURES = JOINT_FEATURES + PAIN_FEATURES + TIME_FEATURE\n",
    "    LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "    N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "    def reshape_data(df, features_list, n_timesteps):\n",
    "        df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "        data_2d = df_pivot.values\n",
    "        n_samples = data_2d.shape[0]\n",
    "        data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "        return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "    X_train_full = reshape_data(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())], FEATURES, N_TIMESTEPS)\n",
    "    X_test_full = reshape_data(X_test_long_df, FEATURES, N_TIMESTEPS)\n",
    "    y_train_full_df = labels_df.sort_values(by='sample_index')\n",
    "    le = LabelEncoder().fit(list(LABEL_MAPPING.keys()))\n",
    "    y_train_full = le.transform(y_train_full_df['label'])\n",
    "    print(f\"Loaded X_train_full (shape: {X_train_full.shape}) and y_train_full (shape: {y_train_full.shape})\")\n",
    "    print(f\"Loaded X_test_full (shape: {X_test_full.shape})\")\n",
    "\n",
    "    print(\"\\n--- 2. Engineering 'is_pirate' Feature ---\")\n",
    "    static_cols = ['sample_index', 'n_legs', 'n_hands', 'n_eyes']\n",
    "    static_df = features_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    pirate_filter = (static_df['n_legs'] == 'one+peg_leg') | (static_df['n_hands'] == 'one+hook_hand') | (static_df['n_eyes'] == 'one+eye_patch')\n",
    "    pirate_indices = static_df[pirate_filter].index\n",
    "    sample_indices_ordered = sorted(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())]['sample_index'].unique())\n",
    "    is_pirate_map = np.array([1 if idx in pirate_indices else 0 for idx in sample_indices_ordered])\n",
    "    pirate_feature_broadcast = np.tile(is_pirate_map.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    X_train_full_engineered = np.concatenate([X_train_full, pirate_feature_broadcast], axis=2)\n",
    "\n",
    "    static_df_test = X_test_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    pirate_filter_test = (static_df_test['n_legs'] == 'one+peg_leg') | (static_df_test['n_hands'] == 'one+hook_hand') | (static_df_test['n_eyes'] == 'one+eye_patch')\n",
    "    pirate_indices_test = static_df_test[pirate_filter_test].index\n",
    "    sample_indices_test_ordered = sorted(X_test_long_df['sample_index'].unique())\n",
    "    is_pirate_map_test = np.array([1 if idx in pirate_indices_test else 0 for idx in sample_indices_test_ordered])\n",
    "    pirate_feature_broadcast_test = np.tile(is_pirate_map_test.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    X_test_full_engineered = np.concatenate([X_test_full, pirate_feature_broadcast_test], axis=2)\n",
    "    \n",
    "    N_FEATURES_NEW = X_train_full_engineered.shape[2]\n",
    "    print(f\"Created X_train_full_engineered (shape: {X_train_full_engineered.shape})\")\n",
    "    print(f\"Created X_test_full_engineered (shape: {X_test_full_engineered.shape})\")\n",
    "    print(f\"N_FEATURES is now: {N_FEATURES_NEW}\")\n",
    "\n",
    "    print(\"\\n--- 3. Calculating Class Weights ---\")\n",
    "    class_counts_series = labels_df['label'].value_counts()\n",
    "    counts_ordered = class_counts_series.reindex(LABEL_MAPPING.keys()).values\n",
    "    class_weights_tensor = 1.0 / torch.tensor(counts_ordered, dtype=torch.float)\n",
    "    class_weights_tensor = (class_weights_tensor / class_weights_tensor.sum()).to(device)\n",
    "    print(f\"Class counts (0, 1, 2): {counts_ordered}\")\n",
    "    print(f\"Calculated class weights: {class_weights_tensor}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f8bc4",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a7c2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(X_3d, y=None, window_size=100, stride=20):\n",
    "    new_X, new_y, window_indices = [], [], []\n",
    "    n_samples, n_timesteps, _ = X_3d.shape\n",
    "    for i in range(n_samples):\n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            new_X.append(X_3d[i, idx:idx+window_size, :])\n",
    "            window_indices.append(i)\n",
    "            if y is not None: new_y.append(y[i])\n",
    "            idx += stride\n",
    "    if y is not None:\n",
    "        return np.array(new_X), np.array(new_y), np.array(window_indices)\n",
    "    return np.array(new_X), np.array(window_indices)\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    return DataLoader(ds, batch_size=int(batch_size), shuffle=shuffle, drop_last=drop_last, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faed930",
   "metadata": {},
   "source": [
    "## üß† 4. Model & Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3cadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, rnn_outputs):\n",
    "        # rnn_outputs: (batch_size, seq_len, hidden_size)\n",
    "        energy = torch.tanh(self.attn(rnn_outputs))\n",
    "        # energy: (batch_size, seq_len, hidden_size)\n",
    "        attn_scores = self.v(energy).squeeze(2)\n",
    "        # attn_scores: (batch_size, seq_len)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        # attn_weights: (batch_size, seq_len)\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(1), rnn_outputs).squeeze(1)\n",
    "        # context_vector: (batch_size, hidden_size)\n",
    "        return context_vector\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes,\n",
    "                 conv_out_channels, conv_kernel_size, bidirectional,\n",
    "                 dropout_rate, feature_dropout_rate, rnn_type='GRU'):\n",
    "        super().__init__()\n",
    "        self.rnn_type, self.num_layers, self.hidden_size, self.bidirectional = \\\n",
    "            rnn_type, num_layers, hidden_size, bidirectional\n",
    "        \n",
    "        rnn_hidden_dim = hidden_size * 2 if bidirectional else hidden_size\n",
    "\n",
    "        # --- Feature Engineering Layers ---\n",
    "        self.pain_embed_dim, self.pirate_embed_dim = 4, 4\n",
    "        self.pain_embeddings = nn.ModuleList([nn.Embedding(3, self.pain_embed_dim) for _ in range(4)])\n",
    "        self.pirate_embedding = nn.Embedding(2, self.pirate_embed_dim)\n",
    "        \n",
    "        num_continuous_features = 32 # 31 joints + 1 time\n",
    "        total_embedding_dim = (4 * self.pain_embed_dim) + self.pirate_embed_dim\n",
    "        conv_input_size = num_continuous_features + total_embedding_dim\n",
    "\n",
    "        # --- Conv1d Layer for Feature Extraction ---\n",
    "        self.conv1d = nn.Conv1d(in_channels=conv_input_size, out_channels=conv_out_channels,\n",
    "                                kernel_size=conv_kernel_size, padding='same')\n",
    "        self.conv_activation = nn.ReLU()\n",
    "        self.feature_dropout = nn.Dropout(feature_dropout_rate)\n",
    "\n",
    "        # --- RNN Layer for Sequence Modeling ---\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=conv_out_channels, hidden_size=hidden_size,\n",
    "            num_layers=num_layers, batch_first=True, bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        # --- Attention Layer ---\n",
    "        self.attention = Attention(rnn_hidden_dim)\n",
    "        \n",
    "        # --- Classifier Head ---\n",
    "        self.classifier = nn.Linear(rnn_hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_continuous = x[:, :, :32]\n",
    "        x_categorical = x[:, :, 32:].long()\n",
    "        embedded_cats = [self.pain_embeddings[i](x_categorical[:, :, i]) for i in range(4)] \\\n",
    "                      + [self.pirate_embedding(x_categorical[:, :, 4])]\n",
    "        x_combined = torch.cat([x_continuous] + embedded_cats, dim=2)\n",
    "        x_permuted = x_combined.permute(0, 2, 1)\n",
    "        x_conv = self.conv_activation(self.conv1d(x_permuted))\n",
    "        x_conv_permuted = x_conv.permute(0, 2, 1)\n",
    "        x_dropped = self.feature_dropout(x_conv_permuted)\n",
    "        \n",
    "        # Get RNN outputs for all time steps\n",
    "        rnn_outputs, _ = self.rnn(x_dropped)\n",
    "        \n",
    "        # Apply attention\n",
    "        context_vector = self.attention(rnn_outputs)\n",
    "        \n",
    "        return self.classifier(context_vector)\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset.tensors[0]), f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset.tensors[0]), f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def objective_function(config, X_train, y_train, X_val, y_val, class_weights):\n",
    "    continuous_indices = list(range(32))\n",
    "    preprocessor = ColumnTransformer([('scaler', StandardScaler(), continuous_indices)], remainder='passthrough')\n",
    "    ns, ts, f = X_train.shape\n",
    "    X_train_final = preprocessor.fit_transform(X_train.reshape(ns * ts, f)).reshape(ns, ts, -1)\n",
    "    ns_val, ts_val, f_val = X_val.shape\n",
    "    X_val_final = preprocessor.transform(X_val.reshape(ns_val * ts_val, f_val)).reshape(ns_val, ts_val, -1)\n",
    "\n",
    "    X_train_w, y_train_w, _ = create_sliding_windows(X_train_final, y_train, config[\"window_size\"], config[\"stride\"])\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(X_val_final, y_val, config[\"window_size\"], config[\"stride\"])\n",
    "    train_loader = make_loader(TensorDataset(torch.from_numpy(X_train_w).float(), torch.from_numpy(y_train_w).long()), config[\"batch_size\"], True, True)\n",
    "    val_loader = make_loader(TensorDataset(torch.from_numpy(X_val_w).float(), torch.from_numpy(y_val_w).long()), config[\"batch_size\"], False, False)\n",
    "\n",
    "    model_config = {\n",
    "        'hidden_size': config['hidden_size'], 'num_layers': config['num_layers'],\n",
    "        'conv_out_channels': config['conv_out_channels'], 'conv_kernel_size': config['conv_kernel_size'],\n",
    "        'bidirectional': config['bidirectional'], 'dropout_rate': config['dropout_rate'],\n",
    "        'feature_dropout_rate': config['feature_dropout_rate']\n",
    "    }\n",
    "    model = RecurrentClassifier(**model_config, num_classes=N_CLASSES, rnn_type='GRU').to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=150)\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=config['label_smoothing_factor'])\n",
    "\n",
    "    best_val_f1 = -1.0\n",
    "    patience_counter = 0\n",
    "    hpo_patience = 30  \n",
    "    \n",
    "    for epoch in range(1, 151):\n",
    "        train_loss, _ = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "        _, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        tune.report({\"val_f1\": val_f1, \"train_loss\": train_loss})\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= hpo_patience:\n",
    "                # print(f\"Trial early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scheduler, scaler, device, patience, experiment_name):\n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "    best_f1 = -1\n",
    "    patience_counter = 0\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "        val_loss, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch % 25 == 0: print(f\"Epoch {epoch:3d}/{epochs} | Val F1: {val_f1:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1, patience_counter = val_f1, 0\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best F1: {best_f1:.4f}\")\n",
    "                break\n",
    "    print(f\"--- Finished Training --- Best F1: {best_f1:.4f}\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f911c",
   "metadata": {},
   "source": [
    "## üß™ 5. Phase 1: Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09a835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-ordered X_train_full shape: (661, 160, 37)\n",
      "\n",
      "--- Splitting re-ordered, unscaled data for HPO ---\n",
      "  X_train_split: (528, 160, 37)\n",
      "  X_val_split:   (133, 160, 37)\n"
     ]
    }
   ],
   "source": [
    "# Re-order columns: 32 continuous (joints+time), then 5 categorical (pain+pirate)\n",
    "continuous_indices_orig = list(range(31)) + [35]\n",
    "categorical_indices_orig = list(range(31, 35)) + [36]\n",
    "X_train_full_reordered = np.concatenate([\n",
    "    X_train_full_engineered[:, :, continuous_indices_orig],\n",
    "    X_train_full_engineered[:, :, categorical_indices_orig]\n",
    "], axis=2)\n",
    "print(f\"Re-ordered X_train_full shape: {X_train_full_reordered.shape}\")\n",
    "\n",
    "# --- Split the re-ordered data ---\n",
    "print(\"\\n--- Splitting re-ordered, unscaled data for HPO ---\")\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "for train_idx, val_idx in sss.split(X_train_full_reordered, y_train_full):\n",
    "    X_train_split, y_train_split = X_train_full_reordered[train_idx], y_train_full[train_idx]\n",
    "    X_val_split, y_val_split = X_train_full_reordered[val_idx], y_train_full[val_idx]\n",
    "print(f\"  X_train_split: {X_train_split.shape}\\n  X_val_split:   {X_val_split.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42797c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-13 18:26:12</td></tr>\n",
       "<tr><td>Running for: </td><td>00:09:40.79        </td></tr>\n",
       "<tr><td>Memory:      </td><td>11.7/13.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=1<br>Bracket: Iter 100.000: None | Iter 50.000: 0.9195509482132849 | Iter 25.000: 0.9138273585914646<br>Logical resource usage: 16.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th>bidirectional  </th><th style=\"text-align: right;\">  conv_kernel_size</th><th style=\"text-align: right;\">  conv_out_channels</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  feature_dropout_rate</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  l2_lambda</th><th style=\"text-align: right;\">          label_smoothing_fact\n",
       "or</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th style=\"text-align: right;\">  stride</th><th style=\"text-align: right;\">  window_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_f1</th><th style=\"text-align: right;\">  train_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_function_b9612f58</td><td>RUNNING   </td><td>127.0.0.1:25548</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.140847</td><td style=\"text-align: right;\">              0.211852</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">4.52488e-06</td><td style=\"text-align: right;\">0.0781708</td><td style=\"text-align: right;\">0.000125347</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         569.674</td><td style=\"text-align: right;\">0.920172</td><td style=\"text-align: right;\">    0.12321 </td></tr>\n",
       "<tr><td>objective_function_ce20d0ae</td><td>RUNNING   </td><td>127.0.0.1:25268</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.244749</td><td style=\"text-align: right;\">              0.390168</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">9.47297e-06</td><td style=\"text-align: right;\">0.161678 </td><td style=\"text-align: right;\">0.00116191 </td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         558.66 </td><td style=\"text-align: right;\">0.917453</td><td style=\"text-align: right;\">    0.209764</td></tr>\n",
       "<tr><td>objective_function_67e29287</td><td>RUNNING   </td><td>127.0.0.1:10796</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.164577</td><td style=\"text-align: right;\">              0.360334</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.00338e-07</td><td style=\"text-align: right;\">0.101839 </td><td style=\"text-align: right;\">0.000363743</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         547.644</td><td style=\"text-align: right;\">0.917912</td><td style=\"text-align: right;\">    0.148789</td></tr>\n",
       "<tr><td>objective_function_8a9aaef6</td><td>RUNNING   </td><td>127.0.0.1:11432</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.412202</td><td style=\"text-align: right;\">              0.409335</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">6.64615e-07</td><td style=\"text-align: right;\">0.121012 </td><td style=\"text-align: right;\">0.00122482 </td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         109.097</td><td style=\"text-align: right;\">0.913194</td><td style=\"text-align: right;\">    0.182363</td></tr>\n",
       "<tr><td>objective_function_458fe60d</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.347721</td><td style=\"text-align: right;\">              0.230126</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.05131e-05</td><td style=\"text-align: right;\">0.076914 </td><td style=\"text-align: right;\">0.000886189</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>objective_function_fa99570f</td><td>TERMINATED</td><td>127.0.0.1:24820</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 7</td><td style=\"text-align: right;\">                 64</td><td style=\"text-align: right;\">      0.3914  </td><td style=\"text-align: right;\">              0.409907</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.1968e-07 </td><td style=\"text-align: right;\">0.183843 </td><td style=\"text-align: right;\">0.00376771 </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         396.587</td><td style=\"text-align: right;\">0.906816</td><td style=\"text-align: right;\">    0.23989 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"window_size\": tune.choice([10, 20, 40]), \"stride\": tune.choice([2, 5]),\n",
    "    \"lr\": tune.loguniform(1e-4, 5e-3),\n",
    "    \"batch_size\": tune.choice([64, 128]),\n",
    "    \"hidden_size\": tune.choice([256, 384]),\n",
    "    \"num_layers\": tune.choice([2, 3]),\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.5),\n",
    "    \"feature_dropout_rate\": tune.uniform(0.2, 0.5),\n",
    "    \"bidirectional\": tune.choice([True, False]),\n",
    "    \"l2_lambda\": tune.loguniform(1e-7, 1e-4),\n",
    "    \"conv_out_channels\": tune.choice([64, 128]),\n",
    "    \"conv_kernel_size\": tune.choice([3, 5, 7]),\n",
    "    # --- NEW: Add label smoothing to the search space ---\n",
    "    \"label_smoothing_factor\": tune.uniform(0.05, 0.2)\n",
    "}\n",
    "\n",
    "def short_trial_name(trial): return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "if ray.is_initialized(): ray.shutdown()\n",
    "ray.init(num_cpus=16, num_gpus=1, ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "print(\"Starting hyperparameter search...\")\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(objective_function, X_train=X_train_split, y_train=y_train_split, X_val=X_val_split, y_val=y_val_split, class_weights=class_weights_tensor),\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 0.25}, config=search_space, num_samples=50, # Increased samples for new param\n",
    "    search_alg=OptunaSearch(metric=\"val_f1\", mode=\"max\"),\n",
    "    scheduler=ASHAScheduler(metric=\"val_f1\", mode=\"max\", grace_period=25, reduction_factor=2),\n",
    "    name=\"pirate_pain_attention_ls_search_v8\", verbose=1,\n",
    "    trial_dirname_creator=short_trial_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Search Complete ---\\n\")\n",
    "print(\"Getting best trial from analysis...\")\n",
    "best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "if best_trial:\n",
    "    FINAL_CONFIG = best_trial.config\n",
    "    FINAL_BEST_VAL_F1 = best_trial.last_result[\"val_f1\"]\n",
    "    print(f\"Best validation F1 score: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(FINAL_CONFIG)\n",
    "else:\n",
    "    print(\"ERROR: No trials completed successfully. Using a default config.\")\n",
    "    # Fallback config if HPO fails\n",
    "    FINAL_CONFIG = {'window_size': 10, 'stride': 2, 'lr': 0.001, 'batch_size': 128, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.3, 'feature_dropout_rate': 0.3, 'bidirectional': True, 'l2_lambda': 1e-06, 'conv_out_channels': 128, 'conv_kernel_size': 5, 'label_smoothing_factor': 0.1}\n",
    "    FINAL_BEST_VAL_F1 = 0.0\n",
    "\n",
    "del X_train_split, y_train_split, X_val_split, y_val_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c0bb8",
   "metadata": {},
   "source": [
    "## üèÜ 6. Phase 2: K-Fold Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13308e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- üèÜ Final Configuration Set --- \")\n",
    "print(f\"Best Val F1 from HPO search: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "print(FINAL_CONFIG)\n",
    "\n",
    "N_SPLITS = 5\n",
    "FINAL_EXPERIMENT_NAME = f\"Attention-GRU_H{FINAL_CONFIG['hidden_size']}_L{FINAL_CONFIG['num_layers']}_\" \\\n",
    "                      f\"C{FINAL_CONFIG['conv_out_channels']}_K{FINAL_CONFIG['conv_kernel_size']}_v8\"\n",
    "submission_filename_base = f\"submission_{FINAL_EXPERIMENT_NAME}.csv\"\n",
    "print(f\"Submission name will be: {submission_filename_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "fold_val_f1_list = []\n",
    "continuous_indices_reordered = list(range(32))\n",
    "EPOCHS = 350\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_full_reordered, y_train_full)):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} --- ({fold_name}) ---\")\n",
    "    \n",
    "    X_train_fold, y_train_fold = X_train_full_reordered[train_idx], y_train_full[train_idx]\n",
    "    X_val_fold, y_val_fold = X_train_full_reordered[val_idx], y_train_full[val_idx]\n",
    "\n",
    "    preprocessor_fold = ColumnTransformer([('s', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "    ns, ts, f = X_train_fold.shape\n",
    "    X_train_scaled = preprocessor_fold.fit_transform(X_train_fold.reshape(ns*ts, f)).reshape(ns, ts, f)\n",
    "    ns_val, ts_val, f_val = X_val_fold.shape\n",
    "    X_val_scaled = preprocessor_fold.transform(X_val_fold.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "    \n",
    "    X_train_w, y_train_w, _ = create_sliding_windows(X_train_scaled, y_train_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(X_val_scaled, y_val_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    train_loader = make_loader(TensorDataset(torch.from_numpy(X_train_w).float(), torch.from_numpy(y_train_w).long()), FINAL_CONFIG['batch_size'], True, True)\n",
    "    val_loader = make_loader(TensorDataset(torch.from_numpy(X_val_w).float(), torch.from_numpy(y_val_w).long()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "    model_config_kfold = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'label_smoothing_factor']}\n",
    "    model_fold = RecurrentClassifier(**model_config_kfold, num_classes=N_CLASSES).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model_fold.parameters(), lr=FINAL_CONFIG['lr'], weight_decay=FINAL_CONFIG['l2_lambda'])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=FINAL_CONFIG['label_smoothing_factor'])\n",
    "\n",
    "    model_fold = fit(model_fold, train_loader, val_loader, EPOCHS, criterion, optimizer, scheduler, scaler, device, 50, fold_name)\n",
    "    _, val_f1 = validate_one_epoch(model_fold, val_loader, criterion, device)\n",
    "    fold_val_f1_list.append(val_f1)\n",
    "    print(f\"Fold {fold+1} Final Val F1: {val_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n--- üèÜ K-Fold Training Complete --- Average F1: {np.mean(fold_val_f1_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9d380",
   "metadata": {},
   "source": [
    "## üì¨ 7. Phase 3: Ensemble Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31871e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Preparing test dataset for submission ---\")\n",
    "continuous_indices_orig = list(range(31)) + [35]\n",
    "categorical_indices_orig = list(range(31, 35)) + [36]\n",
    "X_test_full_reordered = np.concatenate([\n",
    "    X_test_full_engineered[:, :, continuous_indices_orig],\n",
    "    X_test_full_engineered[:, :, categorical_indices_orig]], axis=2)\n",
    "\n",
    "continuous_indices_reordered = list(range(32))\n",
    "preprocessor_final = ColumnTransformer([('scaler', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "preprocessor_final.fit(X_train_full_reordered.reshape(-1, N_FEATURES_NEW))\n",
    "\n",
    "ns_test, ts_test, f_test = X_test_full_reordered.shape\n",
    "X_test_scaled = preprocessor_final.transform(X_test_full_reordered.reshape(ns_test * ts_test, f_test)).reshape(ns_test, ts_test, f_test)\n",
    "X_test_w, test_window_indices = create_sliding_windows(X_test_scaled, y=None, window_size=FINAL_CONFIG['window_size'], stride=FINAL_CONFIG['stride'])\n",
    "test_loader = make_loader(TensorDataset(torch.from_numpy(X_test_w).float()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "model_config_final = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'label_smoothing_factor']}\n",
    "all_fold_probabilities = []\n",
    "\n",
    "# Note: I am not skipping any folds here for the final prediction to maximize ensemble power.\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    print(f\"Loading model {fold+1}/{N_SPLITS} from {model_path}...\")\n",
    "    model_fold = RecurrentClassifier(**model_config_final, num_classes=N_CLASSES).to(device)\n",
    "    model_fold.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model_fold.eval()\n",
    "    \n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in test_loader:\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                probs = torch.softmax(model_fold(inputs.to(device)), dim=1)\n",
    "                fold_preds.append(probs.cpu().numpy())\n",
    "    all_fold_probabilities.append(np.concatenate(fold_preds))\n",
    "\n",
    "mean_probabilities = np.mean(all_fold_probabilities, axis=0)\n",
    "df_probs = pd.DataFrame(mean_probabilities, columns=[f\"prob_{i}\" for i in range(N_CLASSES)])\n",
    "df_probs['original_index'] = test_window_indices\n",
    "agg_probs = df_probs.groupby('original_index')[[f\"prob_{i}\" for i in range(N_CLASSES)]].mean().values\n",
    "final_predictions = le.inverse_transform(np.argmax(agg_probs, axis=1))\n",
    "\n",
    "submission_df = pd.DataFrame({'sample_index': sorted(X_test_long_df['sample_index'].unique()), 'label': final_predictions})\n",
    "submission_df['sample_index'] = submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "submission_filepath = os.path.join(\"submissions\", submission_filename_base)\n",
    "submission_df.to_csv(submission_filepath, index=False)\n",
    "print(f\"\\nSuccessfully saved to {submission_filepath}!\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
