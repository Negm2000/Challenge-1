{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67bb0a3d",
   "metadata": {},
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è (v12: Compiled Model & OneCycleLR)**\n",
    "\n",
    "This notebook targets a major training speedup by using PyTorch's JIT compiler. It also replaces the learning rate scheduler with a more aggressive and often faster-converging policy.\n",
    "\n",
    "**Strategy Update:**\n",
    "1.  **üî• PyTorch 2.0 Model Compilation:** The model is now wrapped with `torch.compile()`. This JIT-compiles the model into optimized, high-performance kernels for a significant training and inference speedup with no change to the model's logic.\n",
    "2.  **One-Cycle LR Scheduler:** The Cosine Annealing scheduler has been replaced with `OneCycleLR`. This policy can lead to faster convergence and better final performance. The training loop has been modified to step the scheduler after each batch, as required.\n",
    "3.  **Focal Loss (Cost-Sensitive Learning):** We retain the use of Focal Loss to dynamically focus training on hard-to-classify examples.\n",
    "4.  **HPO for RNN Type:** The hyperparameter search continues to explore whether `GRU` or `LSTM` is the optimal choice for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03291d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Setup & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ba22088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GPU ---\n",
      "PyTorch version: 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 1234\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Sklearn Imports ---\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Ray[tune] & Optuna Imports ---\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from functools import partial\n",
    "\n",
    "# --- Setup Directories & Device ---\n",
    "logs_dir = \"tensorboard\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8582ef",
   "metadata": {},
   "source": [
    "## üîÑ 2. Data Loading & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44203b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Data ---\n",
      "REMOVED 'joint_30' from feature list.\n",
      "Loaded X_train_full (shape: (661, 160, 35)) and y_train_full (shape: (661,))\n",
      "Loaded X_test_full (shape: (1324, 160, 35))\n",
      "\n",
      "--- 2. Engineering 'is_pirate' Feature ---\n",
      "Created X_train_full_engineered (shape: (661, 160, 36))\n",
      "Created X_test_full_engineered (shape: (1324, 160, 36))\n",
      "N_FEATURES is now: 36\n",
      "\n",
      "--- 3. Calculating Alpha Weights for Focal Loss ---\n",
      "Class counts (0, 1, 2): [511  94  56]\n",
      "Calculated alpha weights: tensor([0.0643, 0.3493, 0.5864], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Loading Data ---\")\n",
    "\n",
    "# --- Define File Paths and Features ---\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "try:\n",
    "    features_long_df = pd.read_csv(X_TRAIN_PATH)\n",
    "    labels_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "    X_test_long_df = pd.read_csv(X_TEST_PATH)\n",
    "    \n",
    "    N_TIMESTEPS = 160\n",
    "    JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(31)]\n",
    "    PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "    TIME_FEATURE = ['time']\n",
    "    FEATURES = JOINT_FEATURES + PAIN_FEATURES + TIME_FEATURE\n",
    "    \n",
    "    # === NEW CLEANING STEP: Remove Zero-Variance Column ===\n",
    "    if 'joint_30' in FEATURES:\n",
    "        FEATURES.remove('joint_30')\n",
    "        print(\"REMOVED 'joint_30' from feature list.\")\n",
    "    # ========================================================\n",
    "    \n",
    "    LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "    N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "    def reshape_data(df, features_list, n_timesteps):\n",
    "        df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "        data_2d = df_pivot.values\n",
    "        n_samples = data_2d.shape[0]\n",
    "        data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "        return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "    X_train_full = reshape_data(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())], FEATURES, N_TIMESTEPS)\n",
    "    X_test_full = reshape_data(X_test_long_df, FEATURES, N_TIMESTEPS)\n",
    "    y_train_full_df = labels_df.sort_values(by='sample_index')\n",
    "    le = LabelEncoder().fit(list(LABEL_MAPPING.keys()))\n",
    "    y_train_full = le.transform(y_train_full_df['label'])\n",
    "    print(f\"Loaded X_train_full (shape: {X_train_full.shape}) and y_train_full (shape: {y_train_full.shape})\")\n",
    "    print(f\"Loaded X_test_full (shape: {X_test_full.shape})\")\n",
    "\n",
    "    print(\"\\n--- 2. Engineering 'is_pirate' Feature ---\")\n",
    "    static_cols = ['sample_index', 'n_legs', 'n_hands', 'n_eyes']\n",
    "    static_df = features_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    pirate_filter = (static_df['n_legs'] == 'one+peg_leg') | (static_df['n_hands'] == 'one+hook_hand') | (static_df['n_eyes'] == 'one+eye_patch')\n",
    "    pirate_indices = static_df[pirate_filter].index\n",
    "    sample_indices_ordered = sorted(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())]['sample_index'].unique())\n",
    "    is_pirate_map = np.array([1 if idx in pirate_indices else 0 for idx in sample_indices_ordered])\n",
    "    pirate_feature_broadcast = np.tile(is_pirate_map.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    X_train_full_engineered = np.concatenate([X_train_full, pirate_feature_broadcast], axis=2)\n",
    "\n",
    "    static_df_test = X_test_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    pirate_filter_test = (static_df_test['n_legs'] == 'one+peg_leg') | (static_df_test['n_hands'] == 'one+hook_hand') | (static_df_test['n_eyes'] == 'one+eye_patch')\n",
    "    pirate_indices_test = static_df_test[pirate_filter_test].index\n",
    "    sample_indices_test_ordered = sorted(X_test_long_df['sample_index'].unique())\n",
    "    is_pirate_map_test = np.array([1 if idx in pirate_indices_test else 0 for idx in sample_indices_test_ordered])\n",
    "    pirate_feature_broadcast_test = np.tile(is_pirate_map_test.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    X_test_full_engineered = np.concatenate([X_test_full, pirate_feature_broadcast_test], axis=2)\n",
    "    \n",
    "    N_FEATURES_NEW = X_train_full_engineered.shape[2]\n",
    "    print(f\"Created X_train_full_engineered (shape: {X_train_full_engineered.shape})\")\n",
    "    print(f\"Created X_test_full_engineered (shape: {X_test_full_engineered.shape})\")\n",
    "    print(f\"N_FEATURES is now: {N_FEATURES_NEW}\")\n",
    "\n",
    "    print(\"\\n--- 3. Calculating Alpha Weights for Focal Loss ---\")\n",
    "    class_counts_series = labels_df['label'].value_counts()\n",
    "    counts_ordered = class_counts_series.reindex(LABEL_MAPPING.keys()).values\n",
    "    class_weights_tensor = 1.0 / torch.tensor(counts_ordered, dtype=torch.float)\n",
    "    alpha_tensor = (class_weights_tensor / class_weights_tensor.sum()).to(device)\n",
    "    print(f\"Class counts (0, 1, 2): {counts_ordered}\")\n",
    "    print(f\"Calculated alpha weights: {alpha_tensor}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f8bc4",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 3. Helper Functions & Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a7c2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Implements Focal Loss for cost-sensitive learning.\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets].to(focal_loss.device)\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def create_sliding_windows(X_3d, y=None, window_size=100, stride=20):\n",
    "    new_X, new_y, window_indices = [], [], []\n",
    "    n_samples, n_timesteps, _ = X_3d.shape\n",
    "    for i in range(n_samples):\n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            new_X.append(X_3d[i, idx:idx+window_size, :])\n",
    "            window_indices.append(i)\n",
    "            if y is not None: new_y.append(y[i])\n",
    "            idx += stride\n",
    "    if y is not None:\n",
    "        return np.array(new_X), np.array(new_y), np.array(window_indices)\n",
    "    return np.array(new_X), np.array(window_indices)\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    return DataLoader(ds, batch_size=int(batch_size), shuffle=shuffle, drop_last=drop_last, \n",
    "                      num_workers=2, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faed930",
   "metadata": {},
   "source": [
    "## üß† 4. Model & Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3cadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, rnn_outputs):\n",
    "        energy = torch.tanh(self.attn(rnn_outputs))\n",
    "        attn_scores = self.v(energy).squeeze(2)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(1), rnn_outputs).squeeze(1)\n",
    "        return context_vector\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes,\n",
    "                 conv_out_channels, conv_kernel_size, bidirectional,\n",
    "                 dropout_rate, feature_dropout_rate, rnn_type='GRU'):\n",
    "        super().__init__()\n",
    "        self.rnn_type, self.num_layers, self.hidden_size, self.bidirectional = \\\n",
    "            rnn_type, num_layers, hidden_size, bidirectional\n",
    "        \n",
    "        rnn_hidden_dim = hidden_size * 2 if bidirectional else hidden_size\n",
    "\n",
    "        self.pain_embed_dim, self.pirate_embed_dim = 4, 4\n",
    "        self.pain_embeddings = nn.ModuleList([nn.Embedding(3, self.pain_embed_dim) for _ in range(4)])\n",
    "        self.pirate_embedding = nn.Embedding(2, self.pirate_embed_dim)\n",
    "        \n",
    "        # --- MODIFIED: Changed from 32 to 31 (due to joint_30 removal) ---\n",
    "        num_continuous_features = 31 \n",
    "        total_embedding_dim = (4 * self.pain_embed_dim) + self.pirate_embed_dim\n",
    "        conv_input_size = num_continuous_features + total_embedding_dim\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=conv_input_size, out_channels=conv_out_channels,\n",
    "                                kernel_size=conv_kernel_size, padding='same')\n",
    "        self.conv_activation = nn.ReLU()\n",
    "        self.feature_dropout = nn.Dropout(feature_dropout_rate)\n",
    "\n",
    "        if rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=conv_out_channels, hidden_size=hidden_size,\n",
    "                num_layers=num_layers, batch_first=True, bidirectional=bidirectional,\n",
    "                dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=conv_out_channels, hidden_size=hidden_size,\n",
    "                num_layers=num_layers, batch_first=True, bidirectional=bidirectional,\n",
    "                dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        self.attention = Attention(rnn_hidden_dim)\n",
    "        self.classifier = nn.Linear(rnn_hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- MODIFIED: Changed slice from :32 to :31 ---\n",
    "        x_continuous = x[:, :, :31]\n",
    "        x_categorical = x[:, :, 31:].long()\n",
    "        \n",
    "        embedded_cats = [self.pain_embeddings[i](x_categorical[:, :, i]) for i in range(4)] \\\n",
    "                      + [self.pirate_embedding(x_categorical[:, :, 4])]\n",
    "        x_combined = torch.cat([x_continuous] + embedded_cats, dim=2)\n",
    "        x_permuted = x_combined.permute(0, 2, 1)\n",
    "        x_conv = self.conv_activation(self.conv1d(x_permuted))\n",
    "        x_conv_permuted = x_conv.permute(0, 2, 1)\n",
    "        x_dropped = self.feature_dropout(x_conv_permuted)\n",
    "        rnn_outputs, _ = self.rnn(x_dropped)\n",
    "        context_vector = self.attention(rnn_outputs)\n",
    "        return self.classifier(context_vector)\n",
    "\n",
    "# --- MODIFIED: Scheduler is now passed in and stepped per-batch ---\n",
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step() # <-- OneCycleLR is stepped each batch\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset.tensors[0]), f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset.tensors[0]), f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def objective_function(config, X_train_w, y_train_w, X_val_w, y_val_w, alpha_tensor):\n",
    "    EPOCHS = 150\n",
    "    train_loader = make_loader(TensorDataset(X_train_w, y_train_w), config[\"batch_size\"], True, True)\n",
    "    val_loader = make_loader(TensorDataset(X_val_w, y_val_w), config[\"batch_size\"], False, False)\n",
    "\n",
    "    model_config = {k: v for k, v in config.items() if k not in ['lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "    model = RecurrentClassifier(**model_config, num_classes=N_CLASSES).to(device)\n",
    "    model = torch.compile(model, backend=\"eager\") # <-- SPEEDUP\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "    # --- MODIFIED: Use OneCycleLR ---\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=config[\"lr\"], epochs=EPOCHS, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = FocalLoss(alpha=alpha_tensor, gamma=config['focal_loss_gamma'])\n",
    "\n",
    "    best_val_f1 = -1.0; patience_counter = 0; hpo_patience = 30\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, _ = train_one_epoch(model, train_loader, criterion, optimizer, scaler, scheduler, device)\n",
    "        _, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        tune.report({\"val_f1\": val_f1, \"train_loss\": train_loss})\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1; patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= hpo_patience: break\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scheduler, scaler, device, patience, experiment_name):\n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "    best_f1 = -1; patience_counter = 0\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # --- MODIFIED: Pass scheduler to training function ---\n",
    "        train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, scaler, scheduler, device)\n",
    "        val_loss, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        if epoch % 5 == 0: print(f\"Epoch {epoch:3d}/{epochs} | Best Val F1: {best_f1:.4f} | Val F1: {val_f1:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1, patience_counter = val_f1, 0\n",
    "            # When using torch.compile, it's better to save the state_dict of the original model\n",
    "            torch.save(model._orig_mod.state_dict() if hasattr(model, '_orig_mod') else model.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience: print(f\"Early stopping at epoch {epoch}. Best F1: {best_f1:.4f}\"); break\n",
    "    print(f\"--- Finished Training --- Best F1: {best_f1:.4f}\")\n",
    "    # Load the best weights back into the uncompiled model for consistency\n",
    "    uncompiled_model = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "    uncompiled_model.load_state_dict(torch.load(model_path))\n",
    "    return uncompiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f911c",
   "metadata": {},
   "source": [
    "## üß™ 5. Phase 1: Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d09a835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordered features. New shape: (661, 160, 36)\n",
      "Created new PowerTransforming + RobustScaling preprocessor.\n",
      "--- Splitting data for HPO ---\n",
      "--- Pre-scaling data for HPO efficiency ---\n",
      "--- Creating fixed sliding windows for HPO ---\n",
      "Created training windows of shape: torch.Size([40128, 10, 36])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define fixed windowing parameters\n",
    "WINDOW_SIZE = 10\n",
    "STRIDE = 2\n",
    "\n",
    "# --- MODIFICATION START ---\n",
    "\n",
    "# 1. DEFINE CORRECT FEATURE INDICES (POST-joint_30 REMOVAL)\n",
    "# Original features in X_train_full_engineered (36 total):\n",
    "# -> 0-29: joint_0-29 (30 features)\n",
    "# -> 30-33: pain_survey_1-4 (4 features)\n",
    "# -> 34: time (1 feature)\n",
    "# -> 35: is_pirate (1 feature)\n",
    "\n",
    "continuous_indices_orig = list(range(30)) + [34] # 30 joints + 1 time\n",
    "categorical_indices_orig = list(range(30, 34)) + [35] # 4 pain surveys + 1 is_pirate\n",
    "\n",
    "# Reorder columns to group continuous (31) and categorical (5) for easy scaling\n",
    "X_train_full_reordered = np.concatenate([\n",
    "    X_train_full_engineered[:, :, continuous_indices_orig],\n",
    "    X_train_full_engineered[:, :, categorical_indices_orig]\n",
    "], axis=2)\n",
    "print(f\"Reordered features. New shape: {X_train_full_reordered.shape}\")\n",
    "\n",
    "# 2. DEFINE PREPROCESSING PIPELINE\n",
    "# In our reordered array (X_train_full_reordered):\n",
    "# -> Indices 0-29 are joint_0-29\n",
    "# -> Index 30 is 'time'\n",
    "# -> Indices 31-35 are categorical (handled by remainder='passthrough')\n",
    "\n",
    "# Define index lists for the ColumnTransformer\n",
    "SKEWED_CONTINUOUS_INDICES = list(range(13, 26)) # joint_13 to joint_25\n",
    "OTHER_CONTINUOUS_INDICES = list(range(13)) + list(range(26, 30)) + [30] # joint_0-12, joint_26-29, and 'time'\n",
    "\n",
    "# Pipeline 1: For highly skewed data\n",
    "# Apply Yeo-Johnson transformation, then scale using StandardScaler\n",
    "skewed_pipeline = Pipeline([\n",
    "    ('power', PowerTransformer(method='yeo-johnson')),\n",
    "    ('scaler', StandardScaler()) \n",
    "])\n",
    "\n",
    "# Pipeline 2: For other continuous data\n",
    "# Just use StandardScaler (safer than StandardScaler)\n",
    "other_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 3. CREATE THE COLUMN TRANSFORMER\n",
    "# This will apply the correct pipeline to each group and leave\n",
    "# the categorical features (indices 31-35) untouched.\n",
    "preprocessor_hpo = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('skewed', skewed_pipeline, SKEWED_CONTINUOUS_INDICES),\n",
    "        ('other', other_pipeline, OTHER_CONTINUOUS_INDICES)\n",
    "    ],\n",
    "    remainder='passthrough' # Leaves categorical (indices 31-35) untouched\n",
    ")\n",
    "print(\"Created new PowerTransforming + RobustScaling preprocessor.\")\n",
    "\n",
    "# --- MODIFICATION END ---\n",
    "\n",
    "print(\"--- Splitting data for HPO ---\")\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "for train_idx, val_idx in sss.split(X_train_full_reordered, y_train_full):\n",
    "    X_train_split, y_train_split = X_train_full_reordered[train_idx], y_train_full[train_idx]\n",
    "    X_val_split, y_val_split = X_train_full_reordered[val_idx], y_train_full[val_idx]\n",
    "\n",
    "print(\"--- Pre-scaling data for HPO efficiency ---\")\n",
    "# This section now uses our powerful new preprocessor_hpo\n",
    "ns, ts, f = X_train_split.shape\n",
    "X_train_split_scaled = preprocessor_hpo.fit_transform(X_train_split.reshape(ns*ts, f)).reshape(ns, ts, f)\n",
    "ns_val, ts_val, f_val = X_val_split.shape\n",
    "X_val_split_scaled = preprocessor_hpo.transform(X_val_split.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "\n",
    "print(\"--- Creating fixed sliding windows for HPO ---\")\n",
    "X_train_w, y_train_w, _ = create_sliding_windows(X_train_split_scaled, y_train_split, WINDOW_SIZE, STRIDE)\n",
    "X_val_w, y_val_w, _ = create_sliding_windows(X_val_split_scaled, y_val_split, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "# Convert to tensors once before HPO\n",
    "X_train_w_torch = torch.from_numpy(X_train_w).float()\n",
    "y_train_w_torch = torch.from_numpy(y_train_w).long()\n",
    "X_val_w_torch = torch.from_numpy(X_val_w).float()\n",
    "y_val_w_torch = torch.from_numpy(y_val_w).long()\n",
    "\n",
    "print(f\"Created training windows of shape: {X_train_w_torch.shape}\")\n",
    "\n",
    "# Clean up memory\n",
    "del X_train_split_scaled, X_val_split_scaled, X_train_w, y_train_w, X_val_w, y_val_w\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b42797c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-15 21:01:13</td></tr>\n",
       "<tr><td>Running for: </td><td>00:38:10.94        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.3/13.9 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=7<br>Bracket: Iter 100.000: 0.9556451123780682 | Iter 50.000: 0.9471829795140011 | Iter 25.000: 0.9355903103675673<br>Logical resource usage: 4.0/16 CPUs, 0.25/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th>bidirectional  </th><th style=\"text-align: right;\">  conv_kernel_size</th><th style=\"text-align: right;\">  conv_out_channels</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  feature_dropout_rate</th><th style=\"text-align: right;\">  focal_loss_gamma</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  l2_lambda</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th>rnn_type  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_f1</th><th style=\"text-align: right;\">  train_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_function_182014c1</td><td>TERMINATED</td><td>127.0.0.1:47696</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.448313</td><td style=\"text-align: right;\">           0.0329935  </td><td style=\"text-align: right;\">          1.73333 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.7229e-08 </td><td style=\"text-align: right;\">0.000395751</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">        1546.89 </td><td style=\"text-align: right;\">0.940249</td><td style=\"text-align: right;\"> 0.00150993 </td></tr>\n",
       "<tr><td>objective_function_04f94a5f</td><td>TERMINATED</td><td>127.0.0.1:47108</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.432408</td><td style=\"text-align: right;\">           0.007398   </td><td style=\"text-align: right;\">          2.71729 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.29733e-08</td><td style=\"text-align: right;\">0.000457615</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         617.033</td><td style=\"text-align: right;\">0.916688</td><td style=\"text-align: right;\"> 0.00096299 </td></tr>\n",
       "<tr><td>objective_function_6c28069a</td><td>TERMINATED</td><td>127.0.0.1:47152</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.444522</td><td style=\"text-align: right;\">           0.21918    </td><td style=\"text-align: right;\">          0.690762</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">8.30337e-08</td><td style=\"text-align: right;\">0.00038123 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         666.395</td><td style=\"text-align: right;\">0.942149</td><td style=\"text-align: right;\"> 0.000913154</td></tr>\n",
       "<tr><td>objective_function_01d53a7a</td><td>TERMINATED</td><td>127.0.0.1:42144</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.332371</td><td style=\"text-align: right;\">           0.400343   </td><td style=\"text-align: right;\">          1.88229 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.13271e-08</td><td style=\"text-align: right;\">0.00345245 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         323.021</td><td style=\"text-align: right;\">0.918116</td><td style=\"text-align: right;\"> 0.00221385 </td></tr>\n",
       "<tr><td>objective_function_852b6d06</td><td>TERMINATED</td><td>127.0.0.1:29792</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.497002</td><td style=\"text-align: right;\">           0.0720787  </td><td style=\"text-align: right;\">          2.41771 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.87069e-08</td><td style=\"text-align: right;\">0.000417102</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         605.674</td><td style=\"text-align: right;\">0.923981</td><td style=\"text-align: right;\"> 0.00147654 </td></tr>\n",
       "<tr><td>objective_function_117f05f6</td><td>TERMINATED</td><td>127.0.0.1:42568</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.261547</td><td style=\"text-align: right;\">           0.0628552  </td><td style=\"text-align: right;\">          2.80822 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.29973e-07</td><td style=\"text-align: right;\">0.00061898 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        1629.8  </td><td style=\"text-align: right;\">0.955645</td><td style=\"text-align: right;\"> 1.82617e-06</td></tr>\n",
       "<tr><td>objective_function_15388e0c</td><td>TERMINATED</td><td>127.0.0.1:35268</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.489037</td><td style=\"text-align: right;\">           0.29658    </td><td style=\"text-align: right;\">          2.37081 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">5.21076e-06</td><td style=\"text-align: right;\">0.00411469 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         853.726</td><td style=\"text-align: right;\">0.942897</td><td style=\"text-align: right;\"> 0.000346225</td></tr>\n",
       "<tr><td>objective_function_3334736f</td><td>TERMINATED</td><td>127.0.0.1:45928</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.223899</td><td style=\"text-align: right;\">           0.399432   </td><td style=\"text-align: right;\">          1.29826 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">6.81964e-08</td><td style=\"text-align: right;\">0.000661341</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         802.649</td><td style=\"text-align: right;\">0.929455</td><td style=\"text-align: right;\"> 0.00111905 </td></tr>\n",
       "<tr><td>objective_function_a8350bd7</td><td>TERMINATED</td><td>127.0.0.1:32092</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.442899</td><td style=\"text-align: right;\">           0.000439364</td><td style=\"text-align: right;\">          2.40641 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.53866e-07</td><td style=\"text-align: right;\">0.000382274</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         210.438</td><td style=\"text-align: right;\">0.934554</td><td style=\"text-align: right;\"> 0.00153485 </td></tr>\n",
       "<tr><td>objective_function_f923cd06</td><td>TERMINATED</td><td>127.0.0.1:32900</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.483005</td><td style=\"text-align: right;\">           0.409517   </td><td style=\"text-align: right;\">          1.21567 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.66162e-08</td><td style=\"text-align: right;\">0.00668374 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         238.633</td><td style=\"text-align: right;\">0.922421</td><td style=\"text-align: right;\"> 0.00312497 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 21:01:13,535\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/Karim Negm/ray_results/pirate_pain_focalloss_search_v12' in 0.0199s.\n",
      "2025-11-15 21:01:13,553\tINFO tune.py:1041 -- Total run time: 2291.00 seconds (2290.91 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# --- MODIFIED: Search space with rnn_type and focal_loss_gamma ---\n",
    "search_space = {\n",
    "    \"rnn_type\": tune.choice(['GRU', 'LSTM']),\n",
    "    \"focal_loss_gamma\": tune.uniform(0.5, 3.0),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2), # This will be the max_lr for OneCycleLR\n",
    "    \"batch_size\": tune.choice([64, 128]),\n",
    "    \"hidden_size\": tune.choice([256, 384, 512]), \n",
    "    \"num_layers\": tune.choice([2, 3]),\n",
    "    \"dropout_rate\": tune.uniform(0, 0.5), \n",
    "    \"feature_dropout_rate\": tune.uniform(0, 0.5),\n",
    "    \"bidirectional\": tune.choice([True, False]), \n",
    "    \"l2_lambda\": tune.loguniform(1e-8, 1e-5),\n",
    "    \"conv_out_channels\": tune.choice([128]), \n",
    "    \"conv_kernel_size\": tune.choice([5])\n",
    "}\n",
    "\n",
    "def short_trial_name(trial): return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "if ray.is_initialized(): ray.shutdown()\n",
    "ray.init(num_cpus=os.cpu_count(), num_gpus=1, ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "print(\"--- Starting HPO with Focal Loss and RNN-Type search ---\")\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(objective_function, \n",
    "                         X_train_w=X_train_w_torch, y_train_w=y_train_w_torch,\n",
    "                         X_val_w=X_val_w_torch, y_val_w=y_val_w_torch,\n",
    "                         alpha_tensor=alpha_tensor),\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 0.25},\n",
    "    config=search_space, \n",
    "    num_samples=10, \n",
    "    search_alg=OptunaSearch(metric=\"val_f1\", mode=\"max\"),\n",
    "    scheduler=ASHAScheduler(metric=\"val_f1\", mode=\"max\", grace_period=25, reduction_factor=2),\n",
    "    name=\"pirate_pain_focalloss_search_v12\", \n",
    "    verbose=1,\n",
    "    trial_dirname_creator=short_trial_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0bbf60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Partial Search Results ---\n",
      "Loading analysis from: C:\\Users\\Karim Negm/ray_results/pirate_pain_focalloss_search_v12\n",
      "Getting best *completed* trial from partial analysis...\n",
      "Best validation F1 score from completed trials: 0.9556\n",
      "Best hyperparameters found:\n",
      "{'rnn_type': 'GRU', 'focal_loss_gamma': 2.8082203178049916, 'lr': 0.0006189800893397386, 'batch_size': 64, 'hidden_size': 256, 'num_layers': 3, 'dropout_rate': 0.26154739277453204, 'feature_dropout_rate': 0.06285520676214107, 'bidirectional': False, 'l2_lambda': 1.299730145545422e-07, 'conv_out_channels': 128, 'conv_kernel_size': 5}\n",
      "\n",
      "--- Ready to proceed to K-Fold Training ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Loading Partial Search Results ---\")\n",
    "\n",
    "# --- 1. Define the path to your experiment results ---\n",
    "# Ray's default is '~/ray_results/<experiment_name>'\n",
    "# The traceback confirms your user folder, so this path should work.\n",
    "experiment_path = os.path.expanduser(\"~/ray_results/pirate_pain_focalloss_search_v12\")\n",
    "\n",
    "# --- 2. Load the analysis from disk ---\n",
    "print(f\"Loading analysis from: {experiment_path}\")\n",
    "try:\n",
    "    analysis = tune.ExperimentAnalysis(experiment_path)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load analysis from {experiment_path}\")\n",
    "    print(f\"Make sure the path is correct. The error was: {e}\")\n",
    "    # Raise the error to stop the notebook if the path is wrong\n",
    "    raise e\n",
    "\n",
    "# --- 3. Get the best trial from the *completed* runs ---\n",
    "print(\"Getting best *completed* trial from partial analysis...\")\n",
    "best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "\n",
    "# --- 4. The rest of your original cell logic ---\n",
    "if best_trial:\n",
    "    FINAL_CONFIG = best_trial.config\n",
    "    # Check if 'val_f1' exists, use a default if not\n",
    "    FINAL_BEST_VAL_F1 = best_trial.last_result.get(\"val_f1\", 0.0) \n",
    "    print(f\"Best validation F1 score from completed trials: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(FINAL_CONFIG)\n",
    "else:\n",
    "    print(\"ERROR: No trials completed successfully. Using a default config.\")\n",
    "    FINAL_CONFIG = {'rnn_type': 'GRU', 'focal_loss_gamma': 2.0, 'lr': 0.001, 'batch_size': 128, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.3, 'feature_dropout_rate': 0.3, 'bidirectional': True, 'l2_lambda': 1e-06, 'conv_out_channels': 128, 'conv_kernel_size': 5}\n",
    "    FINAL_BEST_VAL_F1 = 0.0\n",
    "\n",
    "# Add the fixed windowing params to the final config for the next steps\n",
    "# Make sure WINDOW_SIZE and STRIDE are defined (they should be from Cell 5)\n",
    "FINAL_CONFIG['window_size'] = WINDOW_SIZE\n",
    "FINAL_CONFIG['stride'] = STRIDE\n",
    "\n",
    "# Clean up HPO data to save memory\n",
    "try:\n",
    "    del X_train_w_torch, y_train_w_torch, X_val_w_torch, y_val_w_torch\n",
    "    del X_train_split, y_train_split, X_val_split, y_val_split\n",
    "except NameError:\n",
    "    print(\"Data already cleaned up or not in memory.\")\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n--- Ready to proceed to K-Fold Training ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c0bb8",
   "metadata": {},
   "source": [
    "## üèÜ 6. Phase 2: K-Fold Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13308e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèÜ Final Configuration Set --- \n",
      "Best Val F1 from HPO search: 0.9556\n",
      "{'rnn_type': 'GRU', 'focal_loss_gamma': 2.8082203178049916, 'lr': 0.0006189800893397386, 'batch_size': 64, 'hidden_size': 256, 'num_layers': 3, 'dropout_rate': 0.26154739277453204, 'feature_dropout_rate': 0.06285520676214107, 'bidirectional': False, 'l2_lambda': 1.299730145545422e-07, 'conv_out_channels': 128, 'conv_kernel_size': 5, 'window_size': 10, 'stride': 2}\n",
      "Submission name will be: submission_Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"--- üèÜ Final Configuration Set --- \")\n",
    "print(f\"Best Val F1 from HPO search: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "print(FINAL_CONFIG)\n",
    "\n",
    "N_SPLITS = 5\n",
    "FINAL_EXPERIMENT_NAME = f\"Compiled-FocalLoss-{FINAL_CONFIG['rnn_type']}_H{FINAL_CONFIG['hidden_size']}_L{FINAL_CONFIG['num_layers']}_\"\\\n",
    "                      f\"C{FINAL_CONFIG['conv_out_channels']}_K{FINAL_CONFIG['conv_kernel_size']}_v12\"\n",
    "submission_filename_base = f\"submission_{FINAL_EXPERIMENT_NAME}.csv\"\n",
    "print(f\"Submission name will be: {submission_filename_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bda55dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/5 --- (Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12_fold_1) ---\n",
      "--- Starting Training: Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12_fold_1 ---\n",
      "Epoch   5/150 | Val F1: 0.9048 | LR: 0.000043\n",
      "Epoch  10/150 | Val F1: 0.9296 | LR: 0.000094\n",
      "Epoch  15/150 | Val F1: 0.9218 | LR: 0.000173\n",
      "Epoch  20/150 | Val F1: 0.9303 | LR: 0.000270\n",
      "Epoch  25/150 | Val F1: 0.9384 | LR: 0.000373\n",
      "Epoch  30/150 | Val F1: 0.9366 | LR: 0.000470\n",
      "Epoch  35/150 | Val F1: 0.9336 | LR: 0.000549\n",
      "Epoch  40/150 | Val F1: 0.9432 | LR: 0.000601\n",
      "Epoch  45/150 | Val F1: 0.9391 | LR: 0.000619\n",
      "Epoch  50/150 | Val F1: 0.9377 | LR: 0.000616\n",
      "Epoch  55/150 | Val F1: 0.9374 | LR: 0.000605\n",
      "Epoch  60/150 | Val F1: 0.9351 | LR: 0.000588\n",
      "Epoch  65/150 | Val F1: 0.9393 | LR: 0.000565\n",
      "Epoch  70/150 | Val F1: 0.9405 | LR: 0.000536\n",
      "Epoch  75/150 | Val F1: 0.9397 | LR: 0.000502\n",
      "Epoch  80/150 | Val F1: 0.9400 | LR: 0.000464\n",
      "Epoch  85/150 | Val F1: 0.9394 | LR: 0.000423\n",
      "Epoch  90/150 | Val F1: 0.9462 | LR: 0.000378\n",
      "Epoch  95/150 | Val F1: 0.9466 | LR: 0.000333\n",
      "Epoch 100/150 | Val F1: 0.9452 | LR: 0.000286\n",
      "Epoch 105/150 | Val F1: 0.9413 | LR: 0.000241\n",
      "Epoch 110/150 | Val F1: 0.9483 | LR: 0.000196\n",
      "Epoch 115/150 | Val F1: 0.9491 | LR: 0.000155\n",
      "Epoch 120/150 | Val F1: 0.9502 | LR: 0.000117\n",
      "Epoch 125/150 | Val F1: 0.9491 | LR: 0.000083\n",
      "Epoch 130/150 | Val F1: 0.9500 | LR: 0.000054\n",
      "Epoch 135/150 | Val F1: 0.9502 | LR: 0.000031\n",
      "Epoch 140/150 | Val F1: 0.9501 | LR: 0.000014\n",
      "Epoch 145/150 | Val F1: 0.9500 | LR: 0.000003\n",
      "Epoch 150/150 | Val F1: 0.9501 | LR: 0.000000\n",
      "--- Finished Training --- Best F1: 0.9514\n",
      "Fold 1 Final Val F1: 0.9514\n",
      "\n",
      "--- Fold 2/5 --- (Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12_fold_2) ---\n",
      "--- Starting Training: Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12_fold_2 ---\n",
      "Epoch   5/150 | Val F1: 0.8900 | LR: 0.000043\n",
      "Epoch  10/150 | Val F1: 0.9029 | LR: 0.000094\n",
      "Epoch  15/150 | Val F1: 0.9141 | LR: 0.000173\n",
      "Epoch  20/150 | Val F1: 0.9137 | LR: 0.000270\n",
      "Epoch  25/150 | Val F1: 0.9236 | LR: 0.000373\n",
      "Epoch  30/150 | Val F1: 0.9173 | LR: 0.000470\n",
      "Epoch  35/150 | Val F1: 0.9227 | LR: 0.000549\n",
      "Epoch  40/150 | Val F1: 0.9021 | LR: 0.000601\n",
      "Epoch  45/150 | Val F1: 0.9205 | LR: 0.000619\n",
      "Epoch  50/150 | Val F1: 0.9224 | LR: 0.000616\n",
      "Epoch  55/150 | Val F1: 0.9226 | LR: 0.000605\n",
      "Epoch  60/150 | Val F1: 0.9135 | LR: 0.000588\n",
      "Epoch  65/150 | Val F1: 0.9211 | LR: 0.000565\n",
      "Epoch  70/150 | Val F1: 0.9231 | LR: 0.000536\n",
      "Epoch  75/150 | Val F1: 0.9209 | LR: 0.000502\n",
      "Epoch  80/150 | Val F1: 0.9156 | LR: 0.000464\n",
      "Epoch  85/150 | Val F1: 0.9181 | LR: 0.000423\n",
      "Epoch  90/150 | Val F1: 0.9112 | LR: 0.000378\n",
      "Epoch  95/150 | Val F1: 0.9087 | LR: 0.000333\n",
      "Epoch 100/150 | Val F1: 0.9197 | LR: 0.000286\n",
      "Epoch 105/150 | Val F1: 0.9188 | LR: 0.000241\n",
      "Epoch 110/150 | Val F1: 0.9236 | LR: 0.000196\n",
      "Epoch 115/150 | Val F1: 0.9222 | LR: 0.000155\n",
      "Epoch 120/150 | Val F1: 0.9232 | LR: 0.000117\n",
      "Epoch 125/150 | Val F1: 0.9213 | LR: 0.000083\n",
      "Epoch 130/150 | Val F1: 0.9204 | LR: 0.000054\n",
      "Epoch 135/150 | Val F1: 0.9203 | LR: 0.000031\n",
      "Epoch 140/150 | Val F1: 0.9192 | LR: 0.000014\n",
      "Epoch 145/150 | Val F1: 0.9197 | LR: 0.000003\n",
      "Epoch 150/150 | Val F1: 0.9198 | LR: 0.000000\n",
      "--- Finished Training --- Best F1: 0.9289\n",
      "Fold 2 Final Val F1: 0.9289\n",
      "\n",
      "--- Fold 3/5 --- (Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12_fold_3) ---\n",
      "--- Starting Training: Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12_fold_3 ---\n",
      "Epoch   5/150 | Val F1: 0.8767 | LR: 0.000043\n",
      "Epoch  10/150 | Val F1: 0.8975 | LR: 0.000094\n",
      "Epoch  15/150 | Val F1: 0.9068 | LR: 0.000173\n",
      "Epoch  20/150 | Val F1: 0.9028 | LR: 0.000270\n",
      "Epoch  25/150 | Val F1: 0.9186 | LR: 0.000373\n",
      "Epoch  30/150 | Val F1: 0.9124 | LR: 0.000470\n",
      "Epoch  35/150 | Val F1: 0.9262 | LR: 0.000549\n",
      "Epoch  40/150 | Val F1: 0.9231 | LR: 0.000601\n",
      "Epoch  45/150 | Val F1: 0.9238 | LR: 0.000619\n",
      "Epoch  50/150 | Val F1: 0.9199 | LR: 0.000616\n",
      "Epoch  55/150 | Val F1: 0.9253 | LR: 0.000605\n",
      "Epoch  60/150 | Val F1: 0.9090 | LR: 0.000588\n",
      "Epoch  65/150 | Val F1: 0.9286 | LR: 0.000565\n",
      "Epoch  70/150 | Val F1: 0.9300 | LR: 0.000536\n",
      "Epoch  75/150 | Val F1: 0.9306 | LR: 0.000502\n",
      "Epoch  80/150 | Val F1: 0.9270 | LR: 0.000464\n",
      "Epoch  85/150 | Val F1: 0.9289 | LR: 0.000423\n",
      "Epoch  90/150 | Val F1: 0.9165 | LR: 0.000378\n",
      "Epoch  95/150 | Val F1: 0.9270 | LR: 0.000333\n",
      "Epoch 100/150 | Val F1: 0.9236 | LR: 0.000286\n",
      "Epoch 105/150 | Val F1: 0.9243 | LR: 0.000241\n",
      "Epoch 110/150 | Val F1: 0.9232 | LR: 0.000196\n",
      "Epoch 115/150 | Val F1: 0.9214 | LR: 0.000155\n",
      "Epoch 120/150 | Val F1: 0.9228 | LR: 0.000117\n",
      "Epoch 125/150 | Val F1: 0.9222 | LR: 0.000083\n",
      "Epoch 130/150 | Val F1: 0.9230 | LR: 0.000054\n",
      "Epoch 135/150 | Val F1: 0.9229 | LR: 0.000031\n",
      "Epoch 140/150 | Val F1: 0.9219 | LR: 0.000014\n",
      "Epoch 145/150 | Val F1: 0.9226 | LR: 0.000003\n",
      "Epoch 150/150 | Val F1: 0.9225 | LR: 0.000000\n",
      "--- Finished Training --- Best F1: 0.9329\n",
      "Fold 3 Final Val F1: 0.9329\n",
      "\n",
      "--- Fold 4/5 --- (Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12_fold_4) ---\n",
      "--- Starting Training: Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12_fold_4 ---\n",
      "Epoch   5/150 | Val F1: 0.9001 | LR: 0.000043\n",
      "Epoch  10/150 | Val F1: 0.9136 | LR: 0.000094\n",
      "Epoch  15/150 | Val F1: 0.9133 | LR: 0.000173\n",
      "Epoch  20/150 | Val F1: 0.9179 | LR: 0.000270\n",
      "Epoch  25/150 | Val F1: 0.9335 | LR: 0.000373\n",
      "Epoch  30/150 | Val F1: 0.9232 | LR: 0.000470\n",
      "Epoch  35/150 | Val F1: 0.9139 | LR: 0.000549\n",
      "Epoch  40/150 | Val F1: 0.9127 | LR: 0.000601\n",
      "Epoch  45/150 | Val F1: 0.9299 | LR: 0.000619\n",
      "Epoch  50/150 | Val F1: 0.9290 | LR: 0.000616\n",
      "Epoch  55/150 | Val F1: 0.9246 | LR: 0.000605\n",
      "Epoch  60/150 | Val F1: 0.9178 | LR: 0.000588\n",
      "Epoch  65/150 | Val F1: 0.9299 | LR: 0.000565\n",
      "Epoch  70/150 | Val F1: 0.9283 | LR: 0.000536\n",
      "Epoch  75/150 | Val F1: 0.9268 | LR: 0.000502\n",
      "Epoch  80/150 | Val F1: 0.9285 | LR: 0.000464\n",
      "Epoch  85/150 | Val F1: 0.9147 | LR: 0.000423\n",
      "Epoch  90/150 | Val F1: 0.9244 | LR: 0.000378\n",
      "Epoch  95/150 | Val F1: 0.9339 | LR: 0.000333\n",
      "Epoch 100/150 | Val F1: 0.9243 | LR: 0.000286\n",
      "Epoch 105/150 | Val F1: 0.9228 | LR: 0.000241\n",
      "Epoch 110/150 | Val F1: 0.9267 | LR: 0.000196\n",
      "Epoch 115/150 | Val F1: 0.9233 | LR: 0.000155\n",
      "Epoch 120/150 | Val F1: 0.9274 | LR: 0.000117\n",
      "Epoch 125/150 | Val F1: 0.9282 | LR: 0.000083\n",
      "Epoch 130/150 | Val F1: 0.9277 | LR: 0.000054\n",
      "Epoch 135/150 | Val F1: 0.9267 | LR: 0.000031\n",
      "Epoch 140/150 | Val F1: 0.9266 | LR: 0.000014\n",
      "Epoch 145/150 | Val F1: 0.9266 | LR: 0.000003\n",
      "Epoch 150/150 | Val F1: 0.9266 | LR: 0.000000\n",
      "--- Finished Training --- Best F1: 0.9403\n",
      "Fold 4 Final Val F1: 0.9403\n",
      "\n",
      "--- Fold 5/5 --- (Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12_fold_5) ---\n",
      "--- Starting Training: Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12_fold_5 ---\n",
      "Epoch   5/150 | Val F1: 0.8599 | LR: 0.000043\n",
      "Epoch  10/150 | Val F1: 0.8701 | LR: 0.000094\n",
      "Epoch  15/150 | Val F1: 0.8673 | LR: 0.000173\n",
      "Epoch  20/150 | Val F1: 0.8733 | LR: 0.000270\n",
      "Epoch  25/150 | Val F1: 0.8925 | LR: 0.000373\n",
      "Epoch  30/150 | Val F1: 0.8935 | LR: 0.000470\n",
      "Epoch  35/150 | Val F1: 0.8948 | LR: 0.000549\n",
      "Epoch  40/150 | Val F1: 0.8931 | LR: 0.000601\n",
      "Epoch  45/150 | Val F1: 0.8975 | LR: 0.000619\n",
      "Epoch  50/150 | Val F1: 0.8995 | LR: 0.000616\n",
      "Epoch  55/150 | Val F1: 0.8920 | LR: 0.000605\n",
      "Epoch  60/150 | Val F1: 0.9008 | LR: 0.000588\n",
      "Epoch  65/150 | Val F1: 0.9011 | LR: 0.000565\n",
      "Epoch  70/150 | Val F1: 0.9030 | LR: 0.000536\n",
      "Epoch  75/150 | Val F1: 0.9053 | LR: 0.000502\n",
      "Epoch  80/150 | Val F1: 0.8999 | LR: 0.000464\n",
      "Epoch  85/150 | Val F1: 0.9060 | LR: 0.000423\n",
      "Epoch  90/150 | Val F1: 0.8991 | LR: 0.000378\n",
      "Epoch  95/150 | Val F1: 0.9034 | LR: 0.000333\n",
      "Epoch 100/150 | Val F1: 0.9021 | LR: 0.000286\n",
      "Epoch 105/150 | Val F1: 0.8995 | LR: 0.000241\n",
      "Epoch 110/150 | Val F1: 0.9049 | LR: 0.000196\n",
      "Epoch 115/150 | Val F1: 0.9047 | LR: 0.000155\n",
      "Epoch 120/150 | Val F1: 0.9022 | LR: 0.000117\n",
      "Epoch 125/150 | Val F1: 0.9029 | LR: 0.000083\n",
      "Epoch 130/150 | Val F1: 0.9036 | LR: 0.000054\n",
      "Epoch 135/150 | Val F1: 0.9044 | LR: 0.000031\n",
      "Epoch 140/150 | Val F1: 0.9041 | LR: 0.000014\n",
      "Epoch 145/150 | Val F1: 0.9046 | LR: 0.000003\n",
      "Early stopping at epoch 148. Best F1: 0.9080\n",
      "--- Finished Training --- Best F1: 0.9080\n",
      "Fold 5 Final Val F1: 0.9080\n",
      "\n",
      "--- üèÜ K-Fold Training Complete --- Average F1: 0.9323\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "fold_val_f1_list = []\n",
    "continuous_indices_reordered = list(range(31))\n",
    "EPOCHS = 150\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_full_reordered, y_train_full)):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} --- ({fold_name}) ---\")\n",
    "    \n",
    "    X_train_fold, y_train_fold = X_train_full_reordered[train_idx], y_train_full[train_idx]\n",
    "    X_val_fold, y_val_fold = X_train_full_reordered[val_idx], y_train_full[val_idx]\n",
    "\n",
    "    preprocessor_fold = ColumnTransformer([('s', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "    ns, ts, f = X_train_fold.shape\n",
    "    X_train_scaled = preprocessor_fold.fit_transform(X_train_fold.reshape(ns*ts, f)).reshape(ns, ts, f)\n",
    "    ns_val, ts_val, f_val = X_val_fold.shape\n",
    "    X_val_scaled = preprocessor_fold.transform(X_val_fold.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "    \n",
    "    X_train_w, y_train_w, _ = create_sliding_windows(X_train_scaled, y_train_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(X_val_scaled, y_val_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    \n",
    "    train_loader = make_loader(TensorDataset(torch.from_numpy(X_train_w).float(), torch.from_numpy(y_train_w).long()), FINAL_CONFIG['batch_size'], True, True)\n",
    "    val_loader = make_loader(TensorDataset(torch.from_numpy(X_val_w).float(), torch.from_numpy(y_val_w).long()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "    model_config_kfold = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "    model_fold = RecurrentClassifier(**model_config_kfold, num_classes=N_CLASSES).to(device)\n",
    "    model_fold = torch.compile(model_fold, backend=\"eager\") # <-- SPEEDUP\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model_fold.parameters(), lr=FINAL_CONFIG['lr'], weight_decay=FINAL_CONFIG['l2_lambda'])\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=FINAL_CONFIG['lr'], epochs=EPOCHS, steps_per_epoch=len(train_loader))\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = FocalLoss(alpha=alpha_tensor, gamma=FINAL_CONFIG['focal_loss_gamma'])\n",
    "\n",
    "    # Note: `fit` returns the uncompiled model, which is what we want for saving/inference\n",
    "    model_fold_uncompiled = fit(model_fold, train_loader, val_loader, EPOCHS, criterion, optimizer, scheduler, scaler, device, 100, fold_name)\n",
    "    \n",
    "    # Re-validate with the uncompiled model to get final score\n",
    "    _, val_f1 = validate_one_epoch(model_fold_uncompiled, val_loader, criterion, device)\n",
    "    fold_val_f1_list.append(val_f1)\n",
    "    print(f\"Fold {fold+1} Final Val F1: {val_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n--- üèÜ K-Fold Training Complete --- Average F1: {np.mean(fold_val_f1_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9d380",
   "metadata": {},
   "source": [
    "## üì¨ 7. Phase 3: Ensemble Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b31871e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing test dataset for submission ---\n",
      "Loading model 1/5 from models/Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12_fold_1_best_model.pt...\n",
      "Loading model 3/5 from models/Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12_fold_3_best_model.pt...\n",
      "Loading model 4/5 from models/Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12_fold_4_best_model.pt...\n",
      "\n",
      "Successfully saved to submissions\\submission_Compiled-FocalLoss-GRU_H256_L3_C128_K5_v12.csv!\n",
      "  sample_index    label\n",
      "0          000  no_pain\n",
      "1          001  no_pain\n",
      "2          002  no_pain\n",
      "3          003  no_pain\n",
      "4          004  no_pain\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Preparing test dataset for submission ---\")\n",
    "continuous_indices_orig = list(range(30)) + [34]\n",
    "categorical_indices_orig = list(range(30, 34)) + [35]\n",
    "X_test_full_reordered = np.concatenate([\n",
    "    X_test_full_engineered[:, :, continuous_indices_orig],\n",
    "    X_test_full_engineered[:, :, categorical_indices_orig]], axis=2)\n",
    "\n",
    "continuous_indices_reordered = list(range(31))\n",
    "preprocessor_final = ColumnTransformer([('scaler', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "\n",
    "ns, ts, f = X_train_full_reordered.shape\n",
    "preprocessor_final.fit(X_train_full_reordered.reshape(ns * ts, f))\n",
    "\n",
    "ns_test, ts_test, f_test = X_test_full_reordered.shape\n",
    "X_test_scaled = preprocessor_final.transform(X_test_full_reordered.reshape(ns_test * ts_test, f_test)).reshape(ns_test, ts_test, f_test)\n",
    "X_test_w, test_window_indices = create_sliding_windows(X_test_scaled, y=None, window_size=FINAL_CONFIG['window_size'], stride=FINAL_CONFIG['stride'])\n",
    "test_loader = make_loader(TensorDataset(torch.from_numpy(X_test_w).float()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "model_config_final = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "all_fold_probabilities = []\n",
    "\n",
    "for fold in range(N_SPLITS-1):\n",
    "    if fold == 1: continue  \n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    print(f\"Loading model {fold+1}/{N_SPLITS} from {model_path}...\")\n",
    "    # Load state_dict into the uncompiled model structure\n",
    "    model_fold = RecurrentClassifier(**model_config_final, num_classes=N_CLASSES).to(device)\n",
    "    model_fold.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model_fold = torch.compile(model_fold, backend=\"eager\") # <-- Compile for faster inference\n",
    "    model_fold.eval()\n",
    "    \n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in test_loader:\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                probs = torch.softmax(model_fold(inputs.to(device)), dim=1)\n",
    "                fold_preds.append(probs.cpu().numpy())\n",
    "    all_fold_probabilities.append(np.concatenate(fold_preds))\n",
    "\n",
    "mean_probabilities = np.mean(all_fold_probabilities, axis=0)\n",
    "df_probs = pd.DataFrame(mean_probabilities, columns=[f\"prob_{i}\" for i in range(N_CLASSES)])\n",
    "df_probs['original_index'] = test_window_indices\n",
    "agg_probs = df_probs.groupby('original_index')[[f\"prob_{i}\" for i in range(N_CLASSES)]].mean().values\n",
    "final_predictions = le.inverse_transform(np.argmax(agg_probs, axis=1))\n",
    "\n",
    "submission_df = pd.DataFrame({'sample_index': sorted(X_test_long_df['sample_index'].unique()), 'label': final_predictions})\n",
    "submission_df['sample_index'] = submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "submission_filepath = os.path.join(\"submissions\", submission_filename_base)\n",
    "submission_df.to_csv(submission_filepath, index=False)\n",
    "print(f\"\\nSuccessfully saved to {submission_filepath}!\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
