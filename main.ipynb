{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67bb0a3d",
   "metadata": {},
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è (v13: Robust Evaluation)**\n",
    "\n",
    "This notebook refines the training and evaluation strategy to combat high variance between folds and produce a more reliable performance metric. The core model remains the same, but the validation framework is significantly upgraded.\n",
    "\n",
    "**Strategy Update:**\n",
    "1.  **Hold-Out Test Set:** A true 20% hold-out test set is created at the beginning. This data is **never seen** during HPO or training and is used only for a final, unbiased evaluation of the ensemble's generalization performance.\n",
    "2.  **üî• Repeated Stratified K-Fold:** To combat the \"terrible folds\" problem, `StratifiedKFold` is replaced with `RepeatedStratifiedKFold`. We now use 5 splits with 3 repeats, training **15 models in total**. This smooths out variance caused by unlucky data splits, especially important given the small number of 'pirate' samples.\n",
    "3.  **Final Evaluation Metrics:** A new section has been added to evaluate the full 15-model ensemble on the hold-out test set, generating a final classification report and confusion matrix to precisely measure performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03291d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Setup & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ba22088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GPU ---\n",
      "PyTorch version: 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 1234\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Sklearn Imports ---\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# --- Ray[tune] & Optuna Imports ---\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from functools import partial\n",
    "\n",
    "# --- Setup Directories & Device ---\n",
    "logs_dir = \"tensorboard\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8582ef",
   "metadata": {},
   "source": [
    "## üîÑ 2. Data Loading, Splitting & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "44203b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Data ---\n",
      "REMOVED 'joint_30' from feature list.\n",
      "Loaded X_train_full (shape: (661, 160, 35)) and y_train_full (shape: (661,))\n",
      "Loaded X_test_full (shape: (1324, 160, 35))\n",
      "\n",
      "--- 2. Engineering 'is_pirate' Feature ---\n",
      "Created X_train_full_engineered (shape: (661, 160, 36))\n",
      "Created X_test_full_engineered (shape: (1324, 160, 36))\n",
      "N_FEATURES is now: 36\n",
      "\n",
      "--- 3. Calculating Alpha Weights for Focal Loss ---\n",
      "Class counts (0, 1, 2): [511  94  56]\n",
      "Calculated alpha weights: tensor([0.0643, 0.3493, 0.5864], device='cuda:0')\n",
      "\n",
      "--- 4. Creating High-Quality Hold-Out Test Set via Grouped Stratification ---\n",
      "Data split successfully.\n",
      "  Training set size:      594 (More data for training!)\n",
      "  Hold-out test set size: 67\n",
      "\n",
      "--- Verifying Stratification ---\n",
      "Original Distribution:\n",
      " label_0    0.084720\n",
      "label_1    0.140696\n",
      "label_2    0.765507\n",
      "pirate     0.009077\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test Set Distribution:\n",
      " label_0    0.089552\n",
      "label_1    0.134328\n",
      "label_2    0.761194\n",
      "pirate     0.014925\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Loading Data ---\")\n",
    "\n",
    "# --- Define File Paths and Features ---\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "\n",
    "try:\n",
    "    features_long_df = pd.read_csv(X_TRAIN_PATH)\n",
    "    labels_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "    X_test_long_df = pd.read_csv(X_TEST_PATH)\n",
    "    \n",
    "    N_TIMESTEPS = 160\n",
    "    JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(31)]\n",
    "    PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "    TIME_FEATURE = ['time']\n",
    "    FEATURES = JOINT_FEATURES + PAIN_FEATURES + TIME_FEATURE\n",
    "    if 'joint_30' in FEATURES:\n",
    "        FEATURES.remove('joint_30')\n",
    "        print(\"REMOVED 'joint_30' from feature list.\")\n",
    "        \n",
    "    LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "    N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "    def reshape_data(df, features_list, n_timesteps):\n",
    "        df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "        data_2d = df_pivot.values\n",
    "        n_samples = data_2d.shape[0]\n",
    "        data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "        return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "    X_train_full_raw = reshape_data(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())], FEATURES, N_TIMESTEPS)\n",
    "    X_submission_raw = reshape_data(X_test_long_df, FEATURES, N_TIMESTEPS)\n",
    "    y_train_full_df = labels_df.sort_values(by='sample_index')\n",
    "    le = LabelEncoder().fit(list(LABEL_MAPPING.keys()))\n",
    "    y_train_full_raw = le.transform(y_train_full_df['label'])\n",
    "    print(f\"Loaded X_train_full (shape: {X_train_full_raw.shape}) and y_train_full (shape: {y_train_full_raw.shape})\")\n",
    "    print(f\"Loaded X_test_full (shape: {X_submission_raw.shape})\")\n",
    "\n",
    "    print(\"\\n--- 2. Engineering 'is_pirate' Feature ---\")\n",
    "    static_cols = ['sample_index', 'n_legs', 'n_hands', 'n_eyes']\n",
    "    static_df = features_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    pirate_filter = (static_df['n_legs'] == 'one+peg_leg') | (static_df['n_hands'] == 'one+hook_hand') | (static_df['n_eyes'] == 'one+eye_patch')\n",
    "    pirate_indices = static_df[pirate_filter].index\n",
    "    sample_indices_ordered = sorted(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())]['sample_index'].unique())\n",
    "    is_pirate_map = np.array([1 if idx in pirate_indices else 0 for idx in sample_indices_ordered])\n",
    "    pirate_feature_broadcast = np.tile(is_pirate_map.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    X_train_full_engineered = np.concatenate([X_train_full_raw, pirate_feature_broadcast], axis=2)\n",
    "\n",
    "    static_df_test = X_test_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    pirate_filter_test = (static_df_test['n_legs'] == 'one+peg_leg') | (static_df_test['n_hands'] == 'one+hook_hand') | (static_df_test['n_eyes'] == 'one+eye_patch')\n",
    "    pirate_indices_test = static_df_test[pirate_filter_test].index\n",
    "    sample_indices_test_ordered = sorted(X_test_long_df['sample_index'].unique())\n",
    "    is_pirate_map_test = np.array([1 if idx in pirate_indices_test else 0 for idx in sample_indices_test_ordered])\n",
    "    pirate_feature_broadcast_test = np.tile(is_pirate_map_test.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    X_submission_engineered = np.concatenate([X_submission_raw, pirate_feature_broadcast_test], axis=2)\n",
    "    \n",
    "    N_FEATURES_NEW = X_train_full_engineered.shape[2]\n",
    "    print(f\"Created X_train_full_engineered (shape: {X_train_full_engineered.shape})\")\n",
    "    print(f\"Created X_test_full_engineered (shape: {X_submission_engineered.shape})\")\n",
    "    print(f\"N_FEATURES is now: {N_FEATURES_NEW}\")\n",
    "\n",
    "    print(\"\\n--- 3. Calculating Alpha Weights for Focal Loss ---\")\n",
    "    class_counts_series = labels_df['label'].value_counts()\n",
    "    counts_ordered = class_counts_series.reindex(LABEL_MAPPING.keys()).values\n",
    "    class_weights_tensor = 1.0 / torch.tensor(counts_ordered, dtype=torch.float)\n",
    "    alpha_tensor = (class_weights_tensor / class_weights_tensor.sum()).to(device)\n",
    "    print(f\"Class counts (0, 1, 2): {counts_ordered}\")\n",
    "    print(f\"Calculated alpha weights: {alpha_tensor}\")\n",
    "\n",
    "    # --- 4. Creating a HIGH-QUALITY 10% Hold-Out Test Set via Grouped Stratification ---\n",
    "    print(\"\\n--- 4. Creating High-Quality Hold-Out Test Set via Grouped Stratification ---\")\n",
    "\n",
    "    # Create a new, more robust composite column for stratification.\n",
    "    # If a sample is a pirate, its key is just 'pirate'.\n",
    "    # Otherwise, its key is its pain label (0, 1, or 2).\n",
    "    # This ensures the small pirate class is grouped together and can be split correctly.\n",
    "    stratify_col_grouped = [f\"pirate\" if pirate == 1 else f\"label_{label}\" \n",
    "                            for label, pirate in zip(y_train_full_raw, is_pirate_map)]\n",
    "\n",
    "    # Now, create the split using this new, more robust key\n",
    "    sss_test_split = StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=SEED) # Using 10%\n",
    "\n",
    "    # The split is performed on a dummy X, but uses the stratify_col_grouped to generate indices\n",
    "    train_indices, test_indices = next(sss_test_split.split(X_train_full_engineered, stratify_col_grouped))\n",
    "\n",
    "    # This is the main training data for HPO and K-Fold\n",
    "    X_train = X_train_full_engineered[train_indices]\n",
    "    y_train = y_train_full_raw[train_indices]\n",
    "\n",
    "    # This is the final, unseen, and now highly representative test set\n",
    "    X_test = X_train_full_engineered[test_indices]\n",
    "    y_test = y_train_full_raw[test_indices]\n",
    "\n",
    "    print(f\"Data split successfully.\")\n",
    "    print(f\"  Training set size:      {len(X_train)} (More data for training!)\")\n",
    "    print(f\"  Hold-out test set size: {len(X_test)}\")\n",
    "\n",
    "    # Optional: Verify the stratification worked\n",
    "    print(\"\\n--- Verifying Stratification ---\")\n",
    "    original_dist = pd.Series(stratify_col_grouped).value_counts(normalize=True).sort_index()\n",
    "    test_dist = pd.Series([stratify_col_grouped[i] for i in test_indices]).value_counts(normalize=True).sort_index()\n",
    "    print(\"Original Distribution:\\n\", original_dist)\n",
    "    print(\"\\nTest Set Distribution:\\n\", test_dist)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f8bc4",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 3. Helper Functions & Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a7c2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Implements Focal Loss for cost-sensitive learning.\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets].to(focal_loss.device)\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def create_sliding_windows(X_3d, y=None, window_size=100, stride=20):\n",
    "    new_X, new_y, window_indices = [], [], []\n",
    "    n_samples, n_timesteps, _ = X_3d.shape\n",
    "    for i in range(n_samples):\n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            new_X.append(X_3d[i, idx:idx+window_size, :])\n",
    "            window_indices.append(i)\n",
    "            if y is not None: new_y.append(y[i])\n",
    "            idx += stride\n",
    "    if y is not None:\n",
    "        return np.array(new_X), np.array(new_y), np.array(window_indices)\n",
    "    return np.array(new_X), np.array(window_indices)\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    return DataLoader(ds, batch_size=int(batch_size), shuffle=shuffle, drop_last=drop_last, \n",
    "                      num_workers=2, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faed930",
   "metadata": {},
   "source": [
    "## üß† 4. Model & Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd3cadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, rnn_outputs):\n",
    "        energy = torch.tanh(self.attn(rnn_outputs))\n",
    "        attn_scores = self.v(energy).squeeze(2)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(1), rnn_outputs).squeeze(1)\n",
    "        return context_vector\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes,\n",
    "                 conv_out_channels, conv_kernel_size, bidirectional,\n",
    "                 dropout_rate, feature_dropout_rate, rnn_type='GRU'):\n",
    "        super().__init__()\n",
    "        self.rnn_type, self.num_layers, self.hidden_size, self.bidirectional = \\\n",
    "            rnn_type, num_layers, hidden_size, bidirectional\n",
    "        \n",
    "        rnn_hidden_dim = hidden_size * 2 if bidirectional else hidden_size\n",
    "\n",
    "        self.pain_embed_dim, self.pirate_embed_dim = 4, 4\n",
    "        self.pain_embeddings = nn.ModuleList([nn.Embedding(3, self.pain_embed_dim) for _ in range(4)])\n",
    "        self.pirate_embedding = nn.Embedding(2, self.pirate_embed_dim)\n",
    "        \n",
    "        # --- BUG FIX: Correct number of continuous features ---\n",
    "        num_continuous_features = 31 # 30 joints + 1 time\n",
    "        total_embedding_dim = (4 * self.pain_embed_dim) + self.pirate_embed_dim\n",
    "        conv_input_size = num_continuous_features + total_embedding_dim\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=conv_input_size, out_channels=conv_out_channels,\n",
    "                                kernel_size=conv_kernel_size, padding='same')\n",
    "        self.conv_activation = nn.ReLU()\n",
    "        self.feature_dropout = nn.Dropout(feature_dropout_rate)\n",
    "\n",
    "        if rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=conv_out_channels, hidden_size=hidden_size,\n",
    "                num_layers=num_layers, batch_first=True, bidirectional=bidirectional,\n",
    "                dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=conv_out_channels, hidden_size=hidden_size,\n",
    "                num_layers=num_layers, batch_first=True, bidirectional=bidirectional,\n",
    "                dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        self.attention = Attention(rnn_hidden_dim)\n",
    "        self.classifier = nn.Linear(rnn_hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- BUG FIX: Correct slicing for 31 continuous features ---\n",
    "        x_continuous = x[:, :, :31]\n",
    "        x_categorical = x[:, :, 31:].long()\n",
    "        \n",
    "        embedded_cats = [self.pain_embeddings[i](x_categorical[:, :, i]) for i in range(4)] \\\n",
    "                      + [self.pirate_embedding(x_categorical[:, :, 4])]\n",
    "        x_combined = torch.cat([x_continuous] + embedded_cats, dim=2)\n",
    "        x_permuted = x_combined.permute(0, 2, 1)\n",
    "        x_conv = self.conv_activation(self.conv1d(x_permuted))\n",
    "        x_conv_permuted = x_conv.permute(0, 2, 1)\n",
    "        x_dropped = self.feature_dropout(x_conv_permuted)\n",
    "        rnn_outputs, _ = self.rnn(x_dropped)\n",
    "        context_vector = self.attention(rnn_outputs)\n",
    "        return self.classifier(context_vector)\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset.tensors[0]), f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset.tensors[0]), f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def objective_function(config, X_train_w, y_train_w, X_val_w, y_val_w, alpha_tensor):\n",
    "    EPOCHS = 150\n",
    "    train_loader = make_loader(TensorDataset(X_train_w, y_train_w), config[\"batch_size\"], True, True)\n",
    "    val_loader = make_loader(TensorDataset(X_val_w, y_val_w), config[\"batch_size\"], False, False)\n",
    "\n",
    "    model_config = {k: v for k, v in config.items() if k not in ['lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "    model = RecurrentClassifier(**model_config, num_classes=N_CLASSES).to(device)\n",
    "    model = torch.compile(model, backend=\"eager\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=config[\"lr\"], epochs=EPOCHS, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = FocalLoss(alpha=alpha_tensor, gamma=config['focal_loss_gamma'])\n",
    "\n",
    "    best_val_f1 = -1.0; patience_counter = 0; hpo_patience = 30\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, _ = train_one_epoch(model, train_loader, criterion, optimizer, scaler, scheduler, device)\n",
    "        _, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        tune.report({\"val_f1\": val_f1, \"train_loss\": train_loss})\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1; patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= hpo_patience: break\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scheduler, scaler, device, patience, experiment_name):\n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "    best_f1 = -1; patience_counter = 0\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, scaler, scheduler, device)\n",
    "        val_loss, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1, patience_counter = val_f1, 0\n",
    "            torch.save(model._orig_mod.state_dict() if hasattr(model, '_orig_mod') else model.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience: print(f\"Early stopping at epoch {epoch}. Best F1: {best_f1:.4f}\"); break\n",
    "        if epoch % 3 == 0: print(f\"Epoch {epoch:3d}/{epochs} | Best Val F1: {best_f1:.4f} | Current Val F1: {val_f1:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "    print(f\"--- Finished Training --- Best F1: {best_f1:.4f}\")\n",
    "    uncompiled_model = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "    uncompiled_model.load_state_dict(torch.load(model_path))\n",
    "    return uncompiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f911c",
   "metadata": {},
   "source": [
    "## üß™ 5. Phase 1: Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d09a835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing data for HPO ---\n",
      "--- Creating internal HPO split from main training set ---\n",
      "--- Pre-scaling HPO data for efficiency (with dual-scaling) ---\n",
      "--- Pre-creating fixed sliding windows for HPO ---\n",
      "Created training windows of shape: torch.Size([36100, 10, 36])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--- Preparing data for HPO ---\")\n",
    "\n",
    "# Import the new scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "WINDOW_SIZE = 10\n",
    "STRIDE = 2\n",
    "\n",
    "# --- 1. Define feature indices for dual scaling ---\n",
    "# Original indices (before reordering) of the \"spiky\" joints [13-25]\n",
    "spiky_indices_orig = list(range(13, 26)) \n",
    "# Original indices of \"normal\" continuous features (joints 0-12, 26-29, and time=34)\n",
    "normal_indices_orig = list(range(13)) + list(range(26, 30)) + [34] \n",
    "# Original indices of categorical features\n",
    "categorical_indices_orig = list(range(30, 34)) + [35]\n",
    "\n",
    "# --- 2. Define reordered indices ---\n",
    "# This list defines the NEW order of continuous features (normal first, then spiky)\n",
    "continuous_indices_orig_all = normal_indices_orig + spiky_indices_orig\n",
    "\n",
    "# Create a mapping from the original index to the new reordered index (0-30)\n",
    "reordered_mapping = {orig_idx: new_idx for new_idx, orig_idx in enumerate(continuous_indices_orig_all)}\n",
    "\n",
    "# Get the new locations (in the 0-30 range) for each feature group\n",
    "normal_indices_reordered = [reordered_mapping[i] for i in normal_indices_orig]\n",
    "spiky_indices_reordered = [reordered_mapping[i] for i in spiky_indices_orig]\n",
    "# Note: Categorical features will be at 31-35 after concatenation\n",
    "\n",
    "# --- 3. Reorder data based on the new 'all' list ---\n",
    "X_train_reordered = np.concatenate([\n",
    "    X_train[:, :, continuous_indices_orig_all],\n",
    "    X_train[:, :, categorical_indices_orig]\n",
    "], axis=2)\n",
    "\n",
    "print(\"--- Creating internal HPO split from main training set ---\")\n",
    "sss_hpo = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "hpo_train_idx, hpo_val_idx = next(sss_hpo.split(X_train_reordered, y_train))\n",
    "X_train_hpo, y_train_hpo = X_train_reordered[hpo_train_idx], y_train[hpo_train_idx]\n",
    "X_val_hpo, y_val_hpo = X_train_reordered[hpo_val_idx], y_train[hpo_val_idx]\n",
    "\n",
    "print(\"--- Pre-scaling HPO data for efficiency (with dual-scaling) ---\")\n",
    "# --- 4. Define the CORRECTED ColumnTransformer ---\n",
    "preprocessor_hpo = ColumnTransformer(\n",
    "    [\n",
    "        ('normal_scaler', StandardScaler(), normal_indices_reordered),\n",
    "        ('spiky_scaler', MinMaxScaler(), spiky_indices_reordered) \n",
    "    ], \n",
    "    remainder='passthrough' # Passes categorical features (31-35) untouched\n",
    ")\n",
    "\n",
    "ns, ts, f = X_train_hpo.shape\n",
    "X_train_hpo_scaled = preprocessor_hpo.fit_transform(X_train_hpo.reshape(ns*ts, f)).reshape(ns, ts, f)\n",
    "ns_val, ts_val, f_val = X_val_hpo.shape\n",
    "X_val_hpo_scaled = preprocessor_hpo.transform(X_val_hpo.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "\n",
    "print(\"--- Pre-creating fixed sliding windows for HPO ---\")\n",
    "X_train_w, y_train_w, _ = create_sliding_windows(X_train_hpo_scaled, y_train_hpo, WINDOW_SIZE, STRIDE)\n",
    "X_val_w, y_val_w, _ = create_sliding_windows(X_val_hpo_scaled, y_val_hpo, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "X_train_w_torch = torch.from_numpy(X_train_w).float()\n",
    "y_train_w_torch = torch.from_numpy(y_train_w).long()\n",
    "X_val_w_torch = torch.from_numpy(X_val_w).float()\n",
    "y_val_w_torch = torch.from_numpy(y_val_w).long()\n",
    "\n",
    "print(f\"Created training windows of shape: {X_train_w_torch.shape}\")\n",
    "\n",
    "del X_train_hpo_scaled, X_val_hpo_scaled, X_train_w, y_train_w, X_val_w, y_val_w\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b42797c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-16 14:18:00</td></tr>\n",
       "<tr><td>Running for: </td><td>01:14:35.01        </td></tr>\n",
       "<tr><td>Memory:      </td><td>8.7/13.9 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=19<br>Bracket: Iter 100.000: None | Iter 50.000: 0.9223435885081197 | Iter 25.000: 0.9098862373218746<br>Logical resource usage: 4.0/16 CPUs, 0.25/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                          </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_function_e95a3e0e</td><td style=\"text-align: right;\">           1</td><td>C:/Users/KARIMN~1/AppData/Local/Temp/ray/session_2025-11-16_13-03-15_759355_21672/artifacts/2025-11-16_13-03-25/pirate_pain_robust_search_v13/driver_artifacts/objective_function_e95a3e0e/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th>bidirectional  </th><th style=\"text-align: right;\">  conv_kernel_size</th><th style=\"text-align: right;\">  conv_out_channels</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  feature_dropout_rate</th><th style=\"text-align: right;\">  focal_loss_gamma</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  l2_lambda</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th>rnn_type  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_f1</th><th style=\"text-align: right;\">  train_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_function_26d65349</td><td>TERMINATED</td><td>127.0.0.1:3792 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.178085</td><td style=\"text-align: right;\">              0.336455</td><td style=\"text-align: right;\">          1.53799 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">6.56271e-06</td><td style=\"text-align: right;\">0.0036955  </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         374.317</td><td style=\"text-align: right;\">0.919145</td><td style=\"text-align: right;\"> 0.00111193 </td></tr>\n",
       "<tr><td>objective_function_6a84467c</td><td>TERMINATED</td><td>127.0.0.1:10368</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.220297</td><td style=\"text-align: right;\">              0.340759</td><td style=\"text-align: right;\">          0.97236 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">4.05669e-06</td><td style=\"text-align: right;\">0.00196045 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         766.423</td><td style=\"text-align: right;\">0.92968 </td><td style=\"text-align: right;\"> 0.000783908</td></tr>\n",
       "<tr><td>objective_function_79f0fded</td><td>TERMINATED</td><td>127.0.0.1:27472</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.140081</td><td style=\"text-align: right;\">              0.129682</td><td style=\"text-align: right;\">          2.63341 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.78375e-06</td><td style=\"text-align: right;\">0.00157964 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         783.361</td><td style=\"text-align: right;\">0.91055 </td><td style=\"text-align: right;\"> 0.000559984</td></tr>\n",
       "<tr><td>objective_function_dbe141af</td><td>TERMINATED</td><td>127.0.0.1:11400</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.200051</td><td style=\"text-align: right;\">              0.444765</td><td style=\"text-align: right;\">          2.10652 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">4.64873e-07</td><td style=\"text-align: right;\">2.82139e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         207.826</td><td style=\"text-align: right;\">0.858156</td><td style=\"text-align: right;\"> 0.0145945  </td></tr>\n",
       "<tr><td>objective_function_f38ffe02</td><td>TERMINATED</td><td>127.0.0.1:11104</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.365864</td><td style=\"text-align: right;\">              0.108681</td><td style=\"text-align: right;\">          1.76345 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">3.23712e-07</td><td style=\"text-align: right;\">0.000108134</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         339.079</td><td style=\"text-align: right;\">0.913519</td><td style=\"text-align: right;\"> 0.000724817</td></tr>\n",
       "<tr><td>objective_function_d040a477</td><td>TERMINATED</td><td>127.0.0.1:24144</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.241948</td><td style=\"text-align: right;\">              0.297812</td><td style=\"text-align: right;\">          0.601804</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.83339e-05</td><td style=\"text-align: right;\">1.44252e-05</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         341.721</td><td style=\"text-align: right;\">0.817361</td><td style=\"text-align: right;\"> 0.0350143  </td></tr>\n",
       "<tr><td>objective_function_e6d9d3fb</td><td>TERMINATED</td><td>127.0.0.1:14968</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.225167</td><td style=\"text-align: right;\">              0.452049</td><td style=\"text-align: right;\">          2.13407 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">4.16481e-05</td><td style=\"text-align: right;\">3.83134e-05</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         196.259</td><td style=\"text-align: right;\">0.861376</td><td style=\"text-align: right;\"> 0.0126471  </td></tr>\n",
       "<tr><td>objective_function_3ae9d58f</td><td>TERMINATED</td><td>127.0.0.1:452  </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.134181</td><td style=\"text-align: right;\">              0.282792</td><td style=\"text-align: right;\">          2.41432 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.06027e-07</td><td style=\"text-align: right;\">0.00197769 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         196.823</td><td style=\"text-align: right;\">0.901064</td><td style=\"text-align: right;\"> 0.00104231 </td></tr>\n",
       "<tr><td>objective_function_dc68d458</td><td>TERMINATED</td><td>127.0.0.1:13360</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.204783</td><td style=\"text-align: right;\">              0.186527</td><td style=\"text-align: right;\">          2.90572 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">2.48275e-07</td><td style=\"text-align: right;\">3.56523e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         375.13 </td><td style=\"text-align: right;\">0.878786</td><td style=\"text-align: right;\"> 0.00451933 </td></tr>\n",
       "<tr><td>objective_function_816168d2</td><td>TERMINATED</td><td>127.0.0.1:25140</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.254927</td><td style=\"text-align: right;\">              0.157565</td><td style=\"text-align: right;\">          1.37767 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">2.88834e-05</td><td style=\"text-align: right;\">0.000193499</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">        1163.52 </td><td style=\"text-align: right;\">0.926795</td><td style=\"text-align: right;\"> 3.65194e-05</td></tr>\n",
       "<tr><td>objective_function_9ad2a4c2</td><td>TERMINATED</td><td>127.0.0.1:25368</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.229934</td><td style=\"text-align: right;\">              0.336726</td><td style=\"text-align: right;\">          2.46787 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">5.72899e-06</td><td style=\"text-align: right;\">2.43083e-05</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         343.119</td><td style=\"text-align: right;\">0.865578</td><td style=\"text-align: right;\"> 0.0107917  </td></tr>\n",
       "<tr><td>objective_function_d7e6551d</td><td>TERMINATED</td><td>127.0.0.1:11736</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.464643</td><td style=\"text-align: right;\">              0.334121</td><td style=\"text-align: right;\">          2.64724 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.7812e-05 </td><td style=\"text-align: right;\">0.000234358</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">        1286.84 </td><td style=\"text-align: right;\">0.925063</td><td style=\"text-align: right;\"> 5.64692e-05</td></tr>\n",
       "<tr><td>objective_function_022fb351</td><td>TERMINATED</td><td>127.0.0.1:26368</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.392379</td><td style=\"text-align: right;\">              0.144912</td><td style=\"text-align: right;\">          1.5352  </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.98507e-07</td><td style=\"text-align: right;\">0.00315598 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         616.334</td><td style=\"text-align: right;\">0.922344</td><td style=\"text-align: right;\"> 0.00292858 </td></tr>\n",
       "<tr><td>objective_function_9aa836de</td><td>TERMINATED</td><td>127.0.0.1:20116</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.491753</td><td style=\"text-align: right;\">              0.403982</td><td style=\"text-align: right;\">          0.704761</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">2.45321e-06</td><td style=\"text-align: right;\">0.000451641</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         633.297</td><td style=\"text-align: right;\">0.912738</td><td style=\"text-align: right;\"> 0.00101966 </td></tr>\n",
       "<tr><td>objective_function_d7999be1</td><td>TERMINATED</td><td>127.0.0.1:14244</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.49437 </td><td style=\"text-align: right;\">              0.389147</td><td style=\"text-align: right;\">          0.796495</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.63393e-06</td><td style=\"text-align: right;\">0.000522256</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         769.512</td><td style=\"text-align: right;\">0.928958</td><td style=\"text-align: right;\"> 0.000523288</td></tr>\n",
       "<tr><td>objective_function_d3cd655f</td><td>TERMINATED</td><td>127.0.0.1:27164</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.466223</td><td style=\"text-align: right;\">              0.213979</td><td style=\"text-align: right;\">          0.997005</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.23101e-06</td><td style=\"text-align: right;\">0.000565689</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         610.717</td><td style=\"text-align: right;\">0.91736 </td><td style=\"text-align: right;\"> 0.00124515 </td></tr>\n",
       "<tr><td>objective_function_8d1d8428</td><td>TERMINATED</td><td>127.0.0.1:7044 </td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.357705</td><td style=\"text-align: right;\">              0.219518</td><td style=\"text-align: right;\">          1.19087 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.04862e-06</td><td style=\"text-align: right;\">0.000621663</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         305.483</td><td style=\"text-align: right;\">0.904221</td><td style=\"text-align: right;\"> 0.00261894 </td></tr>\n",
       "<tr><td>objective_function_6c4f6f19</td><td>TERMINATED</td><td>127.0.0.1:13916</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.29862 </td><td style=\"text-align: right;\">              0.203874</td><td style=\"text-align: right;\">          1.10839 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.50379e-05</td><td style=\"text-align: right;\">0.000562075</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         304.467</td><td style=\"text-align: right;\">0.902984</td><td style=\"text-align: right;\"> 0.00195297 </td></tr>\n",
       "<tr><td>objective_function_b3c89075</td><td>TERMINATED</td><td>127.0.0.1:15340</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.301444</td><td style=\"text-align: right;\">              0.230557</td><td style=\"text-align: right;\">          1.06206 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.53681e-05</td><td style=\"text-align: right;\">0.000561155</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         569.041</td><td style=\"text-align: right;\">0.921849</td><td style=\"text-align: right;\"> 0.00150254 </td></tr>\n",
       "<tr><td>objective_function_8db5f7ce</td><td>TERMINATED</td><td>127.0.0.1:10336</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.287031</td><td style=\"text-align: right;\">              0.233363</td><td style=\"text-align: right;\">          1.03524 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">8.07757e-05</td><td style=\"text-align: right;\">0.000109657</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         306.967</td><td style=\"text-align: right;\">0.894312</td><td style=\"text-align: right;\"> 0.00384896 </td></tr>\n",
       "<tr><td>objective_function_8083bd80</td><td>TERMINATED</td><td>127.0.0.1:27112</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.286564</td><td style=\"text-align: right;\">              0.249467</td><td style=\"text-align: right;\">          1.09916 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">9.33517e-05</td><td style=\"text-align: right;\">0.00010001 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         302.951</td><td style=\"text-align: right;\">0.896094</td><td style=\"text-align: right;\"> 0.00503582 </td></tr>\n",
       "<tr><td>objective_function_085bd25e</td><td>TERMINATED</td><td>127.0.0.1:27036</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.288636</td><td style=\"text-align: right;\">              0.402787</td><td style=\"text-align: right;\">          0.924843</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">3.98542e-06</td><td style=\"text-align: right;\">0.00106243 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">        1068.39 </td><td style=\"text-align: right;\">0.928538</td><td style=\"text-align: right;\"> 0.000289677</td></tr>\n",
       "<tr><td>objective_function_56df223a</td><td>TERMINATED</td><td>127.0.0.1:424  </td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.305105</td><td style=\"text-align: right;\">              0.492536</td><td style=\"text-align: right;\">          0.870636</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">9.40483e-05</td><td style=\"text-align: right;\">0.00091196 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         902.567</td><td style=\"text-align: right;\">0.917363</td><td style=\"text-align: right;\"> 0.000227398</td></tr>\n",
       "<tr><td>objective_function_4088ff76</td><td>TERMINATED</td><td>127.0.0.1:7332 </td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.409496</td><td style=\"text-align: right;\">              0.399286</td><td style=\"text-align: right;\">          0.810618</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">4.43348e-06</td><td style=\"text-align: right;\">0.00133124 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         314.504</td><td style=\"text-align: right;\">0.902128</td><td style=\"text-align: right;\"> 0.00256015 </td></tr>\n",
       "<tr><td>objective_function_436386a6</td><td>TERMINATED</td><td>127.0.0.1:13656</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.10231 </td><td style=\"text-align: right;\">              0.397382</td><td style=\"text-align: right;\">          0.81931 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">4.53734e-06</td><td style=\"text-align: right;\">0.00131871 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         982.933</td><td style=\"text-align: right;\">0.934312</td><td style=\"text-align: right;\"> 0.000396748</td></tr>\n",
       "<tr><td>objective_function_a90c719c</td><td>TERMINATED</td><td>127.0.0.1:9288 </td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.335662</td><td style=\"text-align: right;\">              0.381067</td><td style=\"text-align: right;\">          0.787411</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">5.92933e-06</td><td style=\"text-align: right;\">0.00111244 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">        1009.8  </td><td style=\"text-align: right;\">0.923284</td><td style=\"text-align: right;\"> 5.9066e-05 </td></tr>\n",
       "<tr><td>objective_function_62b4c37b</td><td>TERMINATED</td><td>127.0.0.1:25596</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.260186</td><td style=\"text-align: right;\">              0.38045 </td><td style=\"text-align: right;\">          1.40513 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">6.25375e-07</td><td style=\"text-align: right;\">0.000231832</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         604.95 </td><td style=\"text-align: right;\">0.919486</td><td style=\"text-align: right;\"> 0.000373727</td></tr>\n",
       "<tr><td>objective_function_7f7ac843</td><td>TERMINATED</td><td>127.0.0.1:20044</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.103975</td><td style=\"text-align: right;\">              0.380478</td><td style=\"text-align: right;\">          0.601417</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">6.39408e-07</td><td style=\"text-align: right;\">0.0047465  </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         602.959</td><td style=\"text-align: right;\">0.91273 </td><td style=\"text-align: right;\"> 0.00121496 </td></tr>\n",
       "<tr><td>objective_function_584a0347</td><td>TERMINATED</td><td>127.0.0.1:24780</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.346051</td><td style=\"text-align: right;\">              0.376845</td><td style=\"text-align: right;\">          0.589163</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">9.26453e-07</td><td style=\"text-align: right;\">0.00471588 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         519.397</td><td style=\"text-align: right;\">0.916192</td><td style=\"text-align: right;\"> 0.000998566</td></tr>\n",
       "<tr><td>objective_function_e95a3e0e</td><td>ERROR     </td><td>127.0.0.1:8500 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">      0.16291 </td><td style=\"text-align: right;\">              0.392942</td><td style=\"text-align: right;\">          1.58305 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">5.37727e-06</td><td style=\"text-align: right;\">0.000203684</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">            </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 13:16:20,237\tERROR tune_controller.py:1331 -- Trial task failed for trial objective_function_e95a3e0e\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\site-packages\\ray\\_private\\worker.py\", line 2961, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "  File \"e:\\miniconda3\\envs\\an2dl-kaggle\\lib\\site-packages\\ray\\_private\\worker.py\", line 1028, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: 61dcc2f9e738d0909d5e43ea01000000\n",
      "\tpid: 8500\n",
      "\tnamespace: 1fdec5ad-ba16-45da-a568-64f6bf132232\n",
      "\tip: 127.0.0.1\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. Lease ID: 070000009ca2d642ed054d76eb9cd5a4b62372639736579b22241931be239f5e Worker ID: fe4ee07d77610c07abf648aa279af55d0738758663246dbd58e323d2 Node ID: 282eac2b05794fa15b5a8bc9f57166a98b0e3ea68ebce9d1e659367b Worker IP address: 127.0.0.1 Worker port: 57865 Worker PID: 8500 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 14:18:00,264\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/Karim Negm/ray_results/pirate_pain_robust_search_v13' in 0.0430s.\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [objective_function_e95a3e0e])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m ray\u001b[38;5;241m.\u001b[39minit(num_cpus\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mcpu_count(), num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ignore_reinit_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, log_to_driver\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Starting HPO with Focal Loss and RNN-Type search ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mX_train_w\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_w_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_w\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_w_torch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mX_val_w\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val_w_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_w\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val_w_torch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                         \u001b[49m\u001b[43malpha_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresources_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_alg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOptunaSearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_f1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mASHAScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_f1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrace_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpirate_pain_robust_search_v13\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial_dirname_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshort_trial_name\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Search Complete ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m analysis\u001b[38;5;241m.\u001b[39mget_best_trial(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\miniconda3\\envs\\an2dl-kaggle\\lib\\site-packages\\ray\\tune\\tune.py:1035\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incomplete_trials:\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failed_trial \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m-> 1035\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1037\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[1;31mTuneError\u001b[0m: ('Trials did not complete', [objective_function_e95a3e0e])"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"rnn_type\": tune.choice(['GRU', 'LSTM']),\n",
    "    \"focal_loss_gamma\": tune.uniform(0.5, 3.0),\n",
    "    \"lr\": tune.loguniform(1e-5, 5e-3),\n",
    "    \"batch_size\": tune.choice([64, 128]),\n",
    "    \"hidden_size\": tune.choice([256, 384, 512]), \n",
    "    \"num_layers\": tune.choice([2, 3]),\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.5), \n",
    "    \"feature_dropout_rate\": tune.uniform(0.1, 0.5),\n",
    "    \"bidirectional\": tune.choice([False]), \n",
    "    \"l2_lambda\": tune.loguniform(1e-7, 1e-4),\n",
    "    \"conv_out_channels\": tune.choice([128]), \n",
    "    \"conv_kernel_size\": tune.choice([5])\n",
    "}\n",
    "\n",
    "def short_trial_name(trial): return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "if ray.is_initialized(): ray.shutdown()\n",
    "ray.init(num_cpus=os.cpu_count(), num_gpus=1, ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "print(\"--- Starting HPO with Focal Loss and RNN-Type search ---\")\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(objective_function, \n",
    "                         X_train_w=X_train_w_torch, y_train_w=y_train_w_torch,\n",
    "                         X_val_w=X_val_w_torch, y_val_w=y_val_w_torch,\n",
    "                         alpha_tensor=alpha_tensor),\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 0.25},\n",
    "    config=search_space, \n",
    "    num_samples=30, \n",
    "    search_alg=OptunaSearch(metric=\"val_f1\", mode=\"max\"),\n",
    "    scheduler=ASHAScheduler(metric=\"val_f1\", mode=\"max\", grace_period=25, reduction_factor=2),\n",
    "    name=\"pirate_pain_robust_search_v13\", \n",
    "    verbose=1,\n",
    "    trial_dirname_creator=short_trial_name\n",
    ")\n",
    "\n",
    "print(\"\\n--- Search Complete ---\\n\")\n",
    "best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "\n",
    "if best_trial:\n",
    "    FINAL_CONFIG = best_trial.config\n",
    "    FINAL_BEST_VAL_F1 = best_trial.last_result.get(\"val_f1\", 0.0) \n",
    "    print(f\"Best validation F1 score from HPO: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(FINAL_CONFIG)\n",
    "else:\n",
    "    print(\"ERROR: No trials completed successfully. Using a default config.\")\n",
    "    FINAL_CONFIG = {'rnn_type': 'GRU', 'focal_loss_gamma': 2.0, 'lr': 0.001, 'batch_size': 128, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.3, 'feature_dropout_rate': 0.3, 'bidirectional': True, 'l2_lambda': 1e-06, 'conv_out_channels': 128, 'conv_kernel_size': 5}\n",
    "    FINAL_BEST_VAL_F1 = 0.0\n",
    "\n",
    "FINAL_CONFIG['window_size'] = WINDOW_SIZE\n",
    "FINAL_CONFIG['stride'] = STRIDE\n",
    "\n",
    "del X_train_w_torch, y_train_w_torch, X_val_w_torch, y_val_w_torch, X_train_hpo, y_train_hpo, X_val_hpo, y_val_hpo\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c0bb8",
   "metadata": {},
   "source": [
    "## üèÜ 6. Phase 2: Repeated K-Fold Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13308e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèÜ Final Configuration Set --- \n",
      "Best Val F1 from HPO search: 0.9234\n",
      "{'rnn_type': 'LSTM', 'focal_loss_gamma': 1.9506316099182592, 'lr': 0.0002806965440994372, 'batch_size': 64, 'hidden_size': 384, 'num_layers': 2, 'dropout_rate': 0.36451549840324504, 'feature_dropout_rate': 0.2846256346854239, 'bidirectional': True, 'l2_lambda': 1.2586118487759714e-06, 'conv_out_channels': 128, 'conv_kernel_size': 5, 'window_size': 10, 'stride': 2}\n",
      "Submission name will be: submission_Robust-LSTM_H384_L2_v13.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"--- üèÜ Final Configuration Set --- \")\n",
    "print(f\"Best Val F1 from HPO search: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "print(FINAL_CONFIG)\n",
    "\n",
    "# --- MODIFIED: Use repeats for a more robust ensemble ---\n",
    "N_SPLITS = 5\n",
    "N_REPEATS = 3\n",
    "TOTAL_FOLDS = N_SPLITS * N_REPEATS\n",
    "\n",
    "FINAL_EXPERIMENT_NAME = f\"Robust-{FINAL_CONFIG['rnn_type']}_H{FINAL_CONFIG['hidden_size']}_L{FINAL_CONFIG['num_layers']}_v13\"\n",
    "submission_filename_base = f\"submission_{FINAL_EXPERIMENT_NAME}.csv\"\n",
    "print(f\"Submission name will be: {submission_filename_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a4a8bac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission name will be: submission_Robust-LSTM_H384_L2_v13.csv\n"
     ]
    }
   ],
   "source": [
    "# Manual config so we can re-evaluate metrics without re-running HPO\n",
    "FINAL_CONFIG = {'rnn_type': 'LSTM', 'focal_loss_gamma': 1.9506316099182592, 'lr': 0.0002806965440994372, 'batch_size': 64, 'hidden_size': 384, 'num_layers': 2, 'dropout_rate': 0.36451549840324504, 'feature_dropout_rate': 0.2846256346854239, 'bidirectional': True, 'l2_lambda': 1.2586118487759714e-06, 'conv_out_channels': 128, 'conv_kernel_size': 5, 'window_size': 10, 'stride': 2}\n",
    "FINAL_BEST_VAL_F1 = 0.9234\n",
    "N_SPLITS = 5\n",
    "N_REPEATS = 3\n",
    "TOTAL_FOLDS = N_SPLITS * N_REPEATS\n",
    "FINAL_EXPERIMENT_NAME = f\"Robust-{FINAL_CONFIG['rnn_type']}_H{FINAL_CONFIG['hidden_size']}_L{FINAL_CONFIG['num_layers']}_v13\"\n",
    "submission_filename_base = f\"submission_{FINAL_EXPERIMENT_NAME}.csv\"\n",
    "print(f\"Submission name will be: {submission_filename_base}\")\n",
    "\n",
    "# --- MODIFIED: Use RepeatedStratifiedKFold --- \n",
    "rskf = RepeatedStratifiedKFold(n_splits=N_SPLITS, n_repeats=N_REPEATS, random_state=SEED)\n",
    "fold_val_f1_list = []\n",
    "EPOCHS = 250\n",
    "\n",
    "# --- 1. Define feature indices for dual scaling ---\n",
    "spiky_indices_orig = list(range(13, 26)) \n",
    "normal_indices_orig = list(range(13)) + list(range(26, 30)) + [34] \n",
    "categorical_indices_orig = list(range(30, 34)) + [35]\n",
    "\n",
    "# --- 2. Define reordered indices ---\n",
    "continuous_indices_orig_all = normal_indices_orig + spiky_indices_orig\n",
    "reordered_mapping = {orig_idx: new_idx for new_idx, orig_idx in enumerate(continuous_indices_orig_all)}\n",
    "normal_indices_reordered = [reordered_mapping[i] for i in normal_indices_orig]\n",
    "spiky_indices_reordered = [reordered_mapping[i] for i in spiky_indices_orig]\n",
    "\n",
    "# --- 3. Reorder the main training data *before* the loop ---\n",
    "X_train_main_reordered = np.concatenate([\n",
    "    X_train[:, :, continuous_indices_orig_all],\n",
    "    X_train[:, :, categorical_indices_orig]\n",
    "], axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bda55dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/15 --- (Robust-LSTM_H384_L2_v13_fold_1) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_1 ---\n",
      "Epoch   3/250 | Best Val F1: 0.7928 | Current Val F1: 0.7928 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8029 | Current Val F1: 0.8029 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.8398 | Current Val F1: 0.8398 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.8418 | Current Val F1: 0.8418 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.8545 | Current Val F1: 0.8545 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.8545 | Current Val F1: 0.8475 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.8638 | Current Val F1: 0.8595 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.8830 | Current Val F1: 0.8830 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.8830 | Current Val F1: 0.8670 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.8833 | Current Val F1: 0.8775 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.8845 | Current Val F1: 0.8770 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.8845 | Current Val F1: 0.8791 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.8845 | Current Val F1: 0.8670 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.8845 | Current Val F1: 0.8805 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.8845 | Current Val F1: 0.8769 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.8900 | Current Val F1: 0.8765 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.8900 | Current Val F1: 0.8842 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.8900 | Current Val F1: 0.8730 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.8900 | Current Val F1: 0.8752 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.8900 | Current Val F1: 0.8834 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.8901 | Current Val F1: 0.8901 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.8901 | Current Val F1: 0.8846 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.8901 | Current Val F1: 0.8769 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.8901 | Current Val F1: 0.8756 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.8901 | Current Val F1: 0.8795 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.8901 | Current Val F1: 0.8529 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.8901 | Current Val F1: 0.8780 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.8901 | Current Val F1: 0.8833 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.8901 | Current Val F1: 0.8538 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.8901 | Current Val F1: 0.8696 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.8901 | Current Val F1: 0.8765 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.8901 | Current Val F1: 0.8717 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.8901 | Current Val F1: 0.8730 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.8901 | Current Val F1: 0.8772 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.8901 | Current Val F1: 0.8715 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.8901 | Current Val F1: 0.8700 | LR: 0.000257\n",
      "Epoch 111/250 | Best Val F1: 0.8901 | Current Val F1: 0.8651 | LR: 0.000252\n",
      "Epoch 114/250 | Best Val F1: 0.8901 | Current Val F1: 0.8674 | LR: 0.000248\n",
      "Epoch 117/250 | Best Val F1: 0.8901 | Current Val F1: 0.8680 | LR: 0.000243\n",
      "Epoch 120/250 | Best Val F1: 0.8901 | Current Val F1: 0.8743 | LR: 0.000237\n",
      "Epoch 123/250 | Best Val F1: 0.8901 | Current Val F1: 0.8674 | LR: 0.000232\n",
      "Epoch 126/250 | Best Val F1: 0.8901 | Current Val F1: 0.8660 | LR: 0.000226\n",
      "Epoch 129/250 | Best Val F1: 0.8901 | Current Val F1: 0.8661 | LR: 0.000220\n",
      "Epoch 132/250 | Best Val F1: 0.8901 | Current Val F1: 0.8660 | LR: 0.000213\n",
      "Epoch 135/250 | Best Val F1: 0.8901 | Current Val F1: 0.8712 | LR: 0.000207\n",
      "Early stopping at epoch 138. Best F1: 0.8901\n",
      "--- Finished Training --- Best F1: 0.8901\n",
      "Fold 1 Final Val F1: 0.8901\n",
      "\n",
      "--- Fold 2/15 --- (Robust-LSTM_H384_L2_v13_fold_2) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_2 ---\n",
      "Epoch   3/250 | Best Val F1: 0.8253 | Current Val F1: 0.8253 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8726 | Current Val F1: 0.8726 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.8823 | Current Val F1: 0.8788 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.8905 | Current Val F1: 0.8905 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.9263 | Current Val F1: 0.9245 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.9398 | Current Val F1: 0.9398 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.9398 | Current Val F1: 0.9331 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.9398 | Current Val F1: 0.9340 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.9398 | Current Val F1: 0.9205 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.9398 | Current Val F1: 0.9339 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.9398 | Current Val F1: 0.9251 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.9398 | Current Val F1: 0.9273 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.9398 | Current Val F1: 0.9184 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.9398 | Current Val F1: 0.9306 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.9398 | Current Val F1: 0.9237 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.9446 | Current Val F1: 0.9282 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.9446 | Current Val F1: 0.9300 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.9446 | Current Val F1: 0.9190 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.9446 | Current Val F1: 0.9154 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.9446 | Current Val F1: 0.9243 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.9446 | Current Val F1: 0.9238 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.9470 | Current Val F1: 0.9318 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.9470 | Current Val F1: 0.9260 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.9470 | Current Val F1: 0.9219 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.9470 | Current Val F1: 0.9311 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.9470 | Current Val F1: 0.9114 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.9470 | Current Val F1: 0.9279 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.9470 | Current Val F1: 0.9291 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.9470 | Current Val F1: 0.9161 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.9470 | Current Val F1: 0.9285 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.9470 | Current Val F1: 0.9167 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.9470 | Current Val F1: 0.9198 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.9470 | Current Val F1: 0.9241 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.9470 | Current Val F1: 0.9291 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.9470 | Current Val F1: 0.9290 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.9470 | Current Val F1: 0.9248 | LR: 0.000257\n",
      "Epoch 111/250 | Best Val F1: 0.9470 | Current Val F1: 0.9156 | LR: 0.000252\n",
      "Epoch 114/250 | Best Val F1: 0.9470 | Current Val F1: 0.9269 | LR: 0.000248\n",
      "Epoch 117/250 | Best Val F1: 0.9470 | Current Val F1: 0.9273 | LR: 0.000243\n",
      "Epoch 120/250 | Best Val F1: 0.9470 | Current Val F1: 0.9335 | LR: 0.000237\n",
      "Epoch 123/250 | Best Val F1: 0.9470 | Current Val F1: 0.9397 | LR: 0.000232\n",
      "Epoch 126/250 | Best Val F1: 0.9470 | Current Val F1: 0.9319 | LR: 0.000226\n",
      "Epoch 129/250 | Best Val F1: 0.9470 | Current Val F1: 0.9241 | LR: 0.000220\n",
      "Epoch 132/250 | Best Val F1: 0.9470 | Current Val F1: 0.9193 | LR: 0.000213\n",
      "Epoch 135/250 | Best Val F1: 0.9470 | Current Val F1: 0.9234 | LR: 0.000207\n",
      "Epoch 138/250 | Best Val F1: 0.9470 | Current Val F1: 0.9173 | LR: 0.000200\n",
      "Early stopping at epoch 140. Best F1: 0.9470\n",
      "--- Finished Training --- Best F1: 0.9470\n",
      "Fold 2 Final Val F1: 0.9470\n",
      "\n",
      "--- Fold 3/15 --- (Robust-LSTM_H384_L2_v13_fold_3) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_3 ---\n",
      "Epoch   3/250 | Best Val F1: 0.8202 | Current Val F1: 0.8202 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8655 | Current Val F1: 0.8655 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.9171 | Current Val F1: 0.9171 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.9266 | Current Val F1: 0.9266 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.9372 | Current Val F1: 0.9372 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.9372 | Current Val F1: 0.9313 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.9372 | Current Val F1: 0.9305 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.9372 | Current Val F1: 0.9310 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.9372 | Current Val F1: 0.9326 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.9389 | Current Val F1: 0.9358 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.9389 | Current Val F1: 0.9344 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.9458 | Current Val F1: 0.9276 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.9458 | Current Val F1: 0.9243 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.9458 | Current Val F1: 0.9402 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.9458 | Current Val F1: 0.9434 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.9479 | Current Val F1: 0.9389 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.9479 | Current Val F1: 0.9463 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.9479 | Current Val F1: 0.9470 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.9503 | Current Val F1: 0.9503 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.9514 | Current Val F1: 0.9514 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.9514 | Current Val F1: 0.9423 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.9514 | Current Val F1: 0.9475 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.9514 | Current Val F1: 0.9450 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.9514 | Current Val F1: 0.9460 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.9514 | Current Val F1: 0.9401 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.9514 | Current Val F1: 0.9383 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.9514 | Current Val F1: 0.9430 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.9514 | Current Val F1: 0.9359 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.9514 | Current Val F1: 0.9410 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.9514 | Current Val F1: 0.9360 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.9514 | Current Val F1: 0.9366 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.9514 | Current Val F1: 0.9373 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.9514 | Current Val F1: 0.9401 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.9514 | Current Val F1: 0.9357 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.9514 | Current Val F1: 0.9372 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.9554 | Current Val F1: 0.9471 | LR: 0.000257\n",
      "Epoch 111/250 | Best Val F1: 0.9554 | Current Val F1: 0.9414 | LR: 0.000252\n",
      "Epoch 114/250 | Best Val F1: 0.9554 | Current Val F1: 0.9437 | LR: 0.000248\n",
      "Epoch 117/250 | Best Val F1: 0.9554 | Current Val F1: 0.9426 | LR: 0.000243\n",
      "Epoch 120/250 | Best Val F1: 0.9554 | Current Val F1: 0.9366 | LR: 0.000237\n",
      "Epoch 123/250 | Best Val F1: 0.9554 | Current Val F1: 0.9434 | LR: 0.000232\n",
      "Epoch 126/250 | Best Val F1: 0.9554 | Current Val F1: 0.9420 | LR: 0.000226\n",
      "Epoch 129/250 | Best Val F1: 0.9554 | Current Val F1: 0.9449 | LR: 0.000220\n",
      "Epoch 132/250 | Best Val F1: 0.9554 | Current Val F1: 0.9359 | LR: 0.000213\n",
      "Epoch 135/250 | Best Val F1: 0.9554 | Current Val F1: 0.9411 | LR: 0.000207\n",
      "Epoch 138/250 | Best Val F1: 0.9554 | Current Val F1: 0.9365 | LR: 0.000200\n",
      "Epoch 141/250 | Best Val F1: 0.9554 | Current Val F1: 0.9384 | LR: 0.000193\n",
      "Epoch 144/250 | Best Val F1: 0.9554 | Current Val F1: 0.9375 | LR: 0.000186\n",
      "Epoch 147/250 | Best Val F1: 0.9554 | Current Val F1: 0.9441 | LR: 0.000179\n",
      "Epoch 150/250 | Best Val F1: 0.9554 | Current Val F1: 0.9421 | LR: 0.000172\n",
      "Epoch 153/250 | Best Val F1: 0.9554 | Current Val F1: 0.9453 | LR: 0.000164\n",
      "Epoch 156/250 | Best Val F1: 0.9554 | Current Val F1: 0.9382 | LR: 0.000157\n",
      "Epoch 159/250 | Best Val F1: 0.9554 | Current Val F1: 0.9425 | LR: 0.000149\n",
      "Epoch 162/250 | Best Val F1: 0.9554 | Current Val F1: 0.9463 | LR: 0.000142\n",
      "Epoch 165/250 | Best Val F1: 0.9554 | Current Val F1: 0.9489 | LR: 0.000134\n",
      "Epoch 168/250 | Best Val F1: 0.9554 | Current Val F1: 0.9465 | LR: 0.000127\n",
      "Epoch 171/250 | Best Val F1: 0.9554 | Current Val F1: 0.9512 | LR: 0.000119\n",
      "Epoch 174/250 | Best Val F1: 0.9554 | Current Val F1: 0.9425 | LR: 0.000112\n",
      "Epoch 177/250 | Best Val F1: 0.9554 | Current Val F1: 0.9430 | LR: 0.000104\n",
      "Epoch 180/250 | Best Val F1: 0.9554 | Current Val F1: 0.9403 | LR: 0.000097\n",
      "Early stopping at epoch 182. Best F1: 0.9554\n",
      "--- Finished Training --- Best F1: 0.9554\n",
      "Fold 3 Final Val F1: 0.9554\n",
      "\n",
      "--- Fold 4/15 --- (Robust-LSTM_H384_L2_v13_fold_4) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_4 ---\n",
      "Epoch   3/250 | Best Val F1: 0.8190 | Current Val F1: 0.8190 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8395 | Current Val F1: 0.8395 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.8589 | Current Val F1: 0.8589 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.8816 | Current Val F1: 0.8816 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.9004 | Current Val F1: 0.8948 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.9114 | Current Val F1: 0.9114 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.9270 | Current Val F1: 0.9270 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.9270 | Current Val F1: 0.9134 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.9282 | Current Val F1: 0.9282 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.9374 | Current Val F1: 0.9297 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.9374 | Current Val F1: 0.9208 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.9374 | Current Val F1: 0.9317 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.9407 | Current Val F1: 0.9321 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.9407 | Current Val F1: 0.9356 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.9414 | Current Val F1: 0.9414 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.9414 | Current Val F1: 0.9216 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.9414 | Current Val F1: 0.9171 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.9414 | Current Val F1: 0.9213 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.9427 | Current Val F1: 0.9237 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.9427 | Current Val F1: 0.9296 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.9427 | Current Val F1: 0.9365 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.9427 | Current Val F1: 0.9312 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.9427 | Current Val F1: 0.9222 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.9427 | Current Val F1: 0.9290 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.9427 | Current Val F1: 0.9268 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.9427 | Current Val F1: 0.9299 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.9427 | Current Val F1: 0.9399 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.9427 | Current Val F1: 0.9339 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.9427 | Current Val F1: 0.9302 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.9427 | Current Val F1: 0.9246 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.9427 | Current Val F1: 0.9297 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.9427 | Current Val F1: 0.9271 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.9427 | Current Val F1: 0.9348 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.9427 | Current Val F1: 0.9380 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.9427 | Current Val F1: 0.9338 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.9427 | Current Val F1: 0.9312 | LR: 0.000257\n",
      "Epoch 111/250 | Best Val F1: 0.9427 | Current Val F1: 0.9291 | LR: 0.000252\n",
      "Epoch 114/250 | Best Val F1: 0.9427 | Current Val F1: 0.9309 | LR: 0.000248\n",
      "Epoch 117/250 | Best Val F1: 0.9427 | Current Val F1: 0.9258 | LR: 0.000243\n",
      "Epoch 120/250 | Best Val F1: 0.9427 | Current Val F1: 0.9342 | LR: 0.000237\n",
      "Epoch 123/250 | Best Val F1: 0.9427 | Current Val F1: 0.9271 | LR: 0.000232\n",
      "Epoch 126/250 | Best Val F1: 0.9427 | Current Val F1: 0.9292 | LR: 0.000226\n",
      "Epoch 129/250 | Best Val F1: 0.9427 | Current Val F1: 0.9283 | LR: 0.000220\n",
      "Early stopping at epoch 130. Best F1: 0.9427\n",
      "--- Finished Training --- Best F1: 0.9427\n",
      "Fold 4 Final Val F1: 0.9427\n",
      "\n",
      "--- Fold 5/15 --- (Robust-LSTM_H384_L2_v13_fold_5) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_5 ---\n",
      "Epoch   3/250 | Best Val F1: 0.8017 | Current Val F1: 0.8017 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8111 | Current Val F1: 0.8083 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.8529 | Current Val F1: 0.8529 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.8572 | Current Val F1: 0.8570 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.8628 | Current Val F1: 0.8575 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.8758 | Current Val F1: 0.8751 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.8870 | Current Val F1: 0.8870 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.8875 | Current Val F1: 0.8778 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.8897 | Current Val F1: 0.8840 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.8969 | Current Val F1: 0.8911 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.9019 | Current Val F1: 0.8972 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.9019 | Current Val F1: 0.8926 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.9054 | Current Val F1: 0.8837 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.9054 | Current Val F1: 0.8978 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.9054 | Current Val F1: 0.8967 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.9054 | Current Val F1: 0.8993 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.9090 | Current Val F1: 0.9090 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.9090 | Current Val F1: 0.8965 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.9090 | Current Val F1: 0.9009 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.9090 | Current Val F1: 0.8926 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.9090 | Current Val F1: 0.8939 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.9090 | Current Val F1: 0.8939 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.9090 | Current Val F1: 0.8906 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.9090 | Current Val F1: 0.9021 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.9090 | Current Val F1: 0.8905 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.9090 | Current Val F1: 0.8916 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.9099 | Current Val F1: 0.9099 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.9099 | Current Val F1: 0.9027 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.9099 | Current Val F1: 0.9007 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.9099 | Current Val F1: 0.8937 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.9099 | Current Val F1: 0.8992 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.9099 | Current Val F1: 0.8950 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.9099 | Current Val F1: 0.8966 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.9099 | Current Val F1: 0.8907 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.9099 | Current Val F1: 0.8941 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.9099 | Current Val F1: 0.8939 | LR: 0.000257\n",
      "Epoch 111/250 | Best Val F1: 0.9099 | Current Val F1: 0.8872 | LR: 0.000252\n",
      "Epoch 114/250 | Best Val F1: 0.9099 | Current Val F1: 0.8904 | LR: 0.000248\n",
      "Epoch 117/250 | Best Val F1: 0.9099 | Current Val F1: 0.8947 | LR: 0.000243\n",
      "Epoch 120/250 | Best Val F1: 0.9099 | Current Val F1: 0.8901 | LR: 0.000237\n",
      "Epoch 123/250 | Best Val F1: 0.9099 | Current Val F1: 0.8963 | LR: 0.000232\n",
      "Epoch 126/250 | Best Val F1: 0.9099 | Current Val F1: 0.8928 | LR: 0.000226\n",
      "Epoch 129/250 | Best Val F1: 0.9099 | Current Val F1: 0.8978 | LR: 0.000220\n",
      "Epoch 132/250 | Best Val F1: 0.9099 | Current Val F1: 0.8913 | LR: 0.000213\n",
      "Epoch 135/250 | Best Val F1: 0.9099 | Current Val F1: 0.8902 | LR: 0.000207\n",
      "Epoch 138/250 | Best Val F1: 0.9099 | Current Val F1: 0.8908 | LR: 0.000200\n",
      "Epoch 141/250 | Best Val F1: 0.9099 | Current Val F1: 0.8970 | LR: 0.000193\n",
      "Epoch 144/250 | Best Val F1: 0.9099 | Current Val F1: 0.8904 | LR: 0.000186\n",
      "Epoch 147/250 | Best Val F1: 0.9099 | Current Val F1: 0.9001 | LR: 0.000179\n",
      "Epoch 150/250 | Best Val F1: 0.9099 | Current Val F1: 0.8976 | LR: 0.000172\n",
      "Epoch 153/250 | Best Val F1: 0.9099 | Current Val F1: 0.8903 | LR: 0.000164\n",
      "Early stopping at epoch 156. Best F1: 0.9099\n",
      "--- Finished Training --- Best F1: 0.9099\n",
      "Fold 5 Final Val F1: 0.9099\n",
      "\n",
      "--- Fold 6/15 --- (Robust-LSTM_H384_L2_v13_fold_6) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_6 ---\n",
      "Epoch   3/250 | Best Val F1: 0.7927 | Current Val F1: 0.7927 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8059 | Current Val F1: 0.8059 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.8376 | Current Val F1: 0.8338 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.8733 | Current Val F1: 0.8733 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.8753 | Current Val F1: 0.8729 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.8852 | Current Val F1: 0.8852 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.8852 | Current Val F1: 0.8760 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.8852 | Current Val F1: 0.8584 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.8879 | Current Val F1: 0.8761 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.8879 | Current Val F1: 0.8827 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.8879 | Current Val F1: 0.8788 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.8879 | Current Val F1: 0.8771 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.8916 | Current Val F1: 0.8873 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.8916 | Current Val F1: 0.8913 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.8916 | Current Val F1: 0.8912 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.8943 | Current Val F1: 0.8920 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.8943 | Current Val F1: 0.8760 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.8943 | Current Val F1: 0.8842 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.8943 | Current Val F1: 0.8841 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.8943 | Current Val F1: 0.8803 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.8943 | Current Val F1: 0.8709 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.8943 | Current Val F1: 0.8780 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.8943 | Current Val F1: 0.8809 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.8943 | Current Val F1: 0.8856 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.8943 | Current Val F1: 0.8690 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.8943 | Current Val F1: 0.8765 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.8943 | Current Val F1: 0.8915 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.8943 | Current Val F1: 0.8799 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.8943 | Current Val F1: 0.8863 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.8943 | Current Val F1: 0.8760 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.8943 | Current Val F1: 0.8824 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.8943 | Current Val F1: 0.8784 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.8943 | Current Val F1: 0.8858 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.8943 | Current Val F1: 0.8909 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.8943 | Current Val F1: 0.8859 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.8944 | Current Val F1: 0.8944 | LR: 0.000257\n",
      "Epoch 111/250 | Best Val F1: 0.8992 | Current Val F1: 0.8992 | LR: 0.000252\n",
      "Epoch 114/250 | Best Val F1: 0.8992 | Current Val F1: 0.8987 | LR: 0.000248\n",
      "Epoch 117/250 | Best Val F1: 0.8992 | Current Val F1: 0.8878 | LR: 0.000243\n",
      "Epoch 120/250 | Best Val F1: 0.8992 | Current Val F1: 0.8894 | LR: 0.000237\n",
      "Epoch 123/250 | Best Val F1: 0.8992 | Current Val F1: 0.8857 | LR: 0.000232\n",
      "Epoch 126/250 | Best Val F1: 0.8992 | Current Val F1: 0.8866 | LR: 0.000226\n",
      "Epoch 129/250 | Best Val F1: 0.8992 | Current Val F1: 0.8880 | LR: 0.000220\n",
      "Epoch 132/250 | Best Val F1: 0.8992 | Current Val F1: 0.8895 | LR: 0.000213\n",
      "Epoch 135/250 | Best Val F1: 0.8992 | Current Val F1: 0.8856 | LR: 0.000207\n",
      "Epoch 138/250 | Best Val F1: 0.8992 | Current Val F1: 0.8860 | LR: 0.000200\n",
      "Epoch 141/250 | Best Val F1: 0.8992 | Current Val F1: 0.8854 | LR: 0.000193\n",
      "Epoch 144/250 | Best Val F1: 0.8992 | Current Val F1: 0.8843 | LR: 0.000186\n",
      "Epoch 147/250 | Best Val F1: 0.8992 | Current Val F1: 0.8836 | LR: 0.000179\n",
      "Epoch 150/250 | Best Val F1: 0.8992 | Current Val F1: 0.8812 | LR: 0.000172\n",
      "Epoch 153/250 | Best Val F1: 0.8992 | Current Val F1: 0.8836 | LR: 0.000164\n",
      "Epoch 156/250 | Best Val F1: 0.8992 | Current Val F1: 0.8790 | LR: 0.000157\n",
      "Epoch 159/250 | Best Val F1: 0.8992 | Current Val F1: 0.8793 | LR: 0.000149\n",
      "Epoch 162/250 | Best Val F1: 0.8992 | Current Val F1: 0.8815 | LR: 0.000142\n",
      "Epoch 165/250 | Best Val F1: 0.8992 | Current Val F1: 0.8835 | LR: 0.000134\n",
      "Epoch 168/250 | Best Val F1: 0.8992 | Current Val F1: 0.8832 | LR: 0.000127\n",
      "Epoch 171/250 | Best Val F1: 0.8992 | Current Val F1: 0.8830 | LR: 0.000119\n",
      "Epoch 174/250 | Best Val F1: 0.8992 | Current Val F1: 0.8852 | LR: 0.000112\n",
      "Epoch 177/250 | Best Val F1: 0.8992 | Current Val F1: 0.8884 | LR: 0.000104\n",
      "Epoch 180/250 | Best Val F1: 0.8992 | Current Val F1: 0.8834 | LR: 0.000097\n",
      "Epoch 183/250 | Best Val F1: 0.8992 | Current Val F1: 0.8837 | LR: 0.000090\n",
      "Early stopping at epoch 186. Best F1: 0.8992\n",
      "--- Finished Training --- Best F1: 0.8992\n",
      "Fold 6 Final Val F1: 0.8992\n",
      "\n",
      "--- Fold 7/15 --- (Robust-LSTM_H384_L2_v13_fold_7) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_7 ---\n",
      "Epoch   3/250 | Best Val F1: 0.8448 | Current Val F1: 0.8448 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.9002 | Current Val F1: 0.9002 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.9128 | Current Val F1: 0.9128 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.9129 | Current Val F1: 0.9062 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.9164 | Current Val F1: 0.9094 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.9293 | Current Val F1: 0.9293 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.9293 | Current Val F1: 0.9213 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.9293 | Current Val F1: 0.9207 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.9293 | Current Val F1: 0.9041 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.9293 | Current Val F1: 0.9267 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.9293 | Current Val F1: 0.9193 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.9293 | Current Val F1: 0.9190 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.9293 | Current Val F1: 0.9278 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.9352 | Current Val F1: 0.9352 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.9352 | Current Val F1: 0.9283 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.9352 | Current Val F1: 0.9210 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.9352 | Current Val F1: 0.9234 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.9352 | Current Val F1: 0.9280 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.9352 | Current Val F1: 0.9305 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.9352 | Current Val F1: 0.9313 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.9352 | Current Val F1: 0.9259 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.9352 | Current Val F1: 0.9266 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.9352 | Current Val F1: 0.9349 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.9392 | Current Val F1: 0.9392 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.9392 | Current Val F1: 0.9334 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.9392 | Current Val F1: 0.9331 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.9487 | Current Val F1: 0.9419 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.9487 | Current Val F1: 0.9301 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.9487 | Current Val F1: 0.9259 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.9487 | Current Val F1: 0.9230 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.9487 | Current Val F1: 0.9358 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.9487 | Current Val F1: 0.9313 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.9487 | Current Val F1: 0.9278 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.9487 | Current Val F1: 0.9320 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.9487 | Current Val F1: 0.9267 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.9487 | Current Val F1: 0.9247 | LR: 0.000257\n",
      "Epoch 111/250 | Best Val F1: 0.9487 | Current Val F1: 0.9337 | LR: 0.000252\n",
      "Epoch 114/250 | Best Val F1: 0.9487 | Current Val F1: 0.9255 | LR: 0.000248\n",
      "Epoch 117/250 | Best Val F1: 0.9487 | Current Val F1: 0.9149 | LR: 0.000243\n",
      "Epoch 120/250 | Best Val F1: 0.9487 | Current Val F1: 0.9231 | LR: 0.000237\n",
      "Epoch 123/250 | Best Val F1: 0.9487 | Current Val F1: 0.9221 | LR: 0.000232\n",
      "Epoch 126/250 | Best Val F1: 0.9487 | Current Val F1: 0.9209 | LR: 0.000226\n",
      "Epoch 129/250 | Best Val F1: 0.9487 | Current Val F1: 0.9266 | LR: 0.000220\n",
      "Epoch 132/250 | Best Val F1: 0.9487 | Current Val F1: 0.9232 | LR: 0.000213\n",
      "Epoch 135/250 | Best Val F1: 0.9487 | Current Val F1: 0.9264 | LR: 0.000207\n",
      "Epoch 138/250 | Best Val F1: 0.9487 | Current Val F1: 0.9276 | LR: 0.000200\n",
      "Epoch 141/250 | Best Val F1: 0.9487 | Current Val F1: 0.9255 | LR: 0.000193\n",
      "Epoch 144/250 | Best Val F1: 0.9487 | Current Val F1: 0.9277 | LR: 0.000186\n",
      "Epoch 147/250 | Best Val F1: 0.9487 | Current Val F1: 0.9294 | LR: 0.000179\n",
      "Epoch 150/250 | Best Val F1: 0.9487 | Current Val F1: 0.9247 | LR: 0.000172\n",
      "Epoch 153/250 | Best Val F1: 0.9487 | Current Val F1: 0.9265 | LR: 0.000164\n",
      "Early stopping at epoch 154. Best F1: 0.9487\n",
      "--- Finished Training --- Best F1: 0.9487\n",
      "Fold 7 Final Val F1: 0.9487\n",
      "\n",
      "--- Fold 8/15 --- (Robust-LSTM_H384_L2_v13_fold_8) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_8 ---\n",
      "Epoch   3/250 | Best Val F1: 0.8235 | Current Val F1: 0.8235 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8420 | Current Val F1: 0.8420 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.8798 | Current Val F1: 0.8798 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.9115 | Current Val F1: 0.9115 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.9200 | Current Val F1: 0.9200 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.9268 | Current Val F1: 0.9268 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.9275 | Current Val F1: 0.9275 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.9376 | Current Val F1: 0.9376 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.9393 | Current Val F1: 0.9393 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.9412 | Current Val F1: 0.9412 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.9412 | Current Val F1: 0.9378 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.9412 | Current Val F1: 0.9279 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.9412 | Current Val F1: 0.9397 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.9412 | Current Val F1: 0.9316 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.9418 | Current Val F1: 0.9418 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.9418 | Current Val F1: 0.9354 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.9500 | Current Val F1: 0.9500 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.9500 | Current Val F1: 0.9397 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.9500 | Current Val F1: 0.9341 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.9500 | Current Val F1: 0.9445 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.9500 | Current Val F1: 0.9481 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.9500 | Current Val F1: 0.9426 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.9500 | Current Val F1: 0.9417 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.9507 | Current Val F1: 0.9445 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.9507 | Current Val F1: 0.9412 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.9507 | Current Val F1: 0.9441 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.9507 | Current Val F1: 0.9455 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.9516 | Current Val F1: 0.9516 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.9583 | Current Val F1: 0.9514 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.9583 | Current Val F1: 0.9484 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.9583 | Current Val F1: 0.9370 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.9583 | Current Val F1: 0.9407 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.9583 | Current Val F1: 0.9489 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.9583 | Current Val F1: 0.9451 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.9583 | Current Val F1: 0.9507 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.9583 | Current Val F1: 0.9449 | LR: 0.000257\n",
      "Epoch 111/250 | Best Val F1: 0.9583 | Current Val F1: 0.9367 | LR: 0.000252\n",
      "Epoch 114/250 | Best Val F1: 0.9583 | Current Val F1: 0.9434 | LR: 0.000248\n",
      "Epoch 117/250 | Best Val F1: 0.9583 | Current Val F1: 0.9465 | LR: 0.000243\n",
      "Epoch 120/250 | Best Val F1: 0.9583 | Current Val F1: 0.9398 | LR: 0.000237\n",
      "Epoch 123/250 | Best Val F1: 0.9583 | Current Val F1: 0.9395 | LR: 0.000232\n",
      "Epoch 126/250 | Best Val F1: 0.9583 | Current Val F1: 0.9415 | LR: 0.000226\n",
      "Epoch 129/250 | Best Val F1: 0.9583 | Current Val F1: 0.9395 | LR: 0.000220\n",
      "Epoch 132/250 | Best Val F1: 0.9583 | Current Val F1: 0.9439 | LR: 0.000213\n",
      "Epoch 135/250 | Best Val F1: 0.9583 | Current Val F1: 0.9488 | LR: 0.000207\n",
      "Epoch 138/250 | Best Val F1: 0.9583 | Current Val F1: 0.9497 | LR: 0.000200\n",
      "Epoch 141/250 | Best Val F1: 0.9583 | Current Val F1: 0.9461 | LR: 0.000193\n",
      "Epoch 144/250 | Best Val F1: 0.9583 | Current Val F1: 0.9478 | LR: 0.000186\n",
      "Epoch 147/250 | Best Val F1: 0.9583 | Current Val F1: 0.9502 | LR: 0.000179\n",
      "Epoch 150/250 | Best Val F1: 0.9583 | Current Val F1: 0.9473 | LR: 0.000172\n",
      "Epoch 153/250 | Best Val F1: 0.9583 | Current Val F1: 0.9493 | LR: 0.000164\n",
      "Epoch 156/250 | Best Val F1: 0.9583 | Current Val F1: 0.9506 | LR: 0.000157\n",
      "Epoch 159/250 | Best Val F1: 0.9583 | Current Val F1: 0.9443 | LR: 0.000149\n",
      "Early stopping at epoch 160. Best F1: 0.9583\n",
      "--- Finished Training --- Best F1: 0.9583\n",
      "Fold 8 Final Val F1: 0.9583\n",
      "\n",
      "--- Fold 9/15 --- (Robust-LSTM_H384_L2_v13_fold_9) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_9 ---\n",
      "Epoch   3/250 | Best Val F1: 0.8112 | Current Val F1: 0.8112 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8446 | Current Val F1: 0.8446 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.8742 | Current Val F1: 0.8742 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.9049 | Current Val F1: 0.9049 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.9049 | Current Val F1: 0.9038 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.9122 | Current Val F1: 0.9122 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.9272 | Current Val F1: 0.9272 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.9313 | Current Val F1: 0.9256 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.9393 | Current Val F1: 0.9279 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.9400 | Current Val F1: 0.9400 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.9400 | Current Val F1: 0.9357 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.9407 | Current Val F1: 0.9369 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.9495 | Current Val F1: 0.9495 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.9495 | Current Val F1: 0.9425 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.9495 | Current Val F1: 0.9397 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.9495 | Current Val F1: 0.9337 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.9495 | Current Val F1: 0.9294 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.9495 | Current Val F1: 0.9393 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.9495 | Current Val F1: 0.9324 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.9495 | Current Val F1: 0.9225 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.9495 | Current Val F1: 0.9356 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.9495 | Current Val F1: 0.9347 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.9495 | Current Val F1: 0.9398 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.9495 | Current Val F1: 0.9389 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.9495 | Current Val F1: 0.9382 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.9495 | Current Val F1: 0.9377 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.9495 | Current Val F1: 0.9373 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.9495 | Current Val F1: 0.9361 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.9495 | Current Val F1: 0.9336 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.9495 | Current Val F1: 0.9382 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.9495 | Current Val F1: 0.9367 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.9495 | Current Val F1: 0.9351 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.9495 | Current Val F1: 0.9332 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.9495 | Current Val F1: 0.9267 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.9495 | Current Val F1: 0.9295 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.9495 | Current Val F1: 0.9237 | LR: 0.000257\n",
      "Epoch 111/250 | Best Val F1: 0.9495 | Current Val F1: 0.9241 | LR: 0.000252\n",
      "Early stopping at epoch 114. Best F1: 0.9495\n",
      "--- Finished Training --- Best F1: 0.9495\n",
      "Fold 9 Final Val F1: 0.9495\n",
      "\n",
      "--- Fold 10/15 --- (Robust-LSTM_H384_L2_v13_fold_10) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_10 ---\n",
      "Epoch   3/250 | Best Val F1: 0.8040 | Current Val F1: 0.8040 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8506 | Current Val F1: 0.8506 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.8622 | Current Val F1: 0.8596 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.8668 | Current Val F1: 0.8634 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.8817 | Current Val F1: 0.8817 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.8927 | Current Val F1: 0.8927 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.8958 | Current Val F1: 0.8913 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.9027 | Current Val F1: 0.9006 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.9027 | Current Val F1: 0.8914 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.9164 | Current Val F1: 0.9164 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.9164 | Current Val F1: 0.9078 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.9214 | Current Val F1: 0.9167 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.9214 | Current Val F1: 0.9098 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.9214 | Current Val F1: 0.9043 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.9214 | Current Val F1: 0.9189 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.9214 | Current Val F1: 0.9073 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.9230 | Current Val F1: 0.9198 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.9230 | Current Val F1: 0.9139 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.9258 | Current Val F1: 0.9251 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.9258 | Current Val F1: 0.9150 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.9258 | Current Val F1: 0.9237 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.9258 | Current Val F1: 0.9174 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.9258 | Current Val F1: 0.9035 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.9258 | Current Val F1: 0.9155 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.9258 | Current Val F1: 0.9182 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.9258 | Current Val F1: 0.9169 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.9258 | Current Val F1: 0.9133 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.9258 | Current Val F1: 0.9125 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.9258 | Current Val F1: 0.9192 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.9258 | Current Val F1: 0.9082 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.9258 | Current Val F1: 0.9166 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.9258 | Current Val F1: 0.9153 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.9258 | Current Val F1: 0.9113 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.9258 | Current Val F1: 0.9128 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.9258 | Current Val F1: 0.9214 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.9258 | Current Val F1: 0.9221 | LR: 0.000257\n",
      "Epoch 111/250 | Best Val F1: 0.9258 | Current Val F1: 0.9179 | LR: 0.000252\n",
      "Epoch 114/250 | Best Val F1: 0.9259 | Current Val F1: 0.9241 | LR: 0.000248\n",
      "Epoch 117/250 | Best Val F1: 0.9259 | Current Val F1: 0.9237 | LR: 0.000243\n",
      "Epoch 120/250 | Best Val F1: 0.9259 | Current Val F1: 0.9209 | LR: 0.000237\n",
      "Epoch 123/250 | Best Val F1: 0.9259 | Current Val F1: 0.9251 | LR: 0.000232\n",
      "Epoch 126/250 | Best Val F1: 0.9259 | Current Val F1: 0.9239 | LR: 0.000226\n",
      "Epoch 129/250 | Best Val F1: 0.9259 | Current Val F1: 0.9204 | LR: 0.000220\n",
      "Epoch 132/250 | Best Val F1: 0.9259 | Current Val F1: 0.9232 | LR: 0.000213\n",
      "Epoch 135/250 | Best Val F1: 0.9259 | Current Val F1: 0.9214 | LR: 0.000207\n",
      "Epoch 138/250 | Best Val F1: 0.9259 | Current Val F1: 0.9224 | LR: 0.000200\n",
      "Epoch 141/250 | Best Val F1: 0.9259 | Current Val F1: 0.9206 | LR: 0.000193\n",
      "Epoch 144/250 | Best Val F1: 0.9259 | Current Val F1: 0.9115 | LR: 0.000186\n",
      "Epoch 147/250 | Best Val F1: 0.9259 | Current Val F1: 0.9152 | LR: 0.000179\n",
      "Epoch 150/250 | Best Val F1: 0.9259 | Current Val F1: 0.9206 | LR: 0.000172\n",
      "Epoch 153/250 | Best Val F1: 0.9259 | Current Val F1: 0.9206 | LR: 0.000164\n",
      "Epoch 156/250 | Best Val F1: 0.9259 | Current Val F1: 0.9209 | LR: 0.000157\n",
      "Epoch 159/250 | Best Val F1: 0.9259 | Current Val F1: 0.9208 | LR: 0.000149\n",
      "Epoch 162/250 | Best Val F1: 0.9259 | Current Val F1: 0.9222 | LR: 0.000142\n",
      "Epoch 165/250 | Best Val F1: 0.9259 | Current Val F1: 0.9190 | LR: 0.000134\n",
      "Epoch 168/250 | Best Val F1: 0.9259 | Current Val F1: 0.9196 | LR: 0.000127\n",
      "Epoch 171/250 | Best Val F1: 0.9259 | Current Val F1: 0.9197 | LR: 0.000119\n",
      "Epoch 174/250 | Best Val F1: 0.9259 | Current Val F1: 0.9192 | LR: 0.000112\n",
      "Epoch 177/250 | Best Val F1: 0.9259 | Current Val F1: 0.9192 | LR: 0.000104\n",
      "Epoch 180/250 | Best Val F1: 0.9259 | Current Val F1: 0.9201 | LR: 0.000097\n",
      "Epoch 183/250 | Best Val F1: 0.9259 | Current Val F1: 0.9201 | LR: 0.000090\n",
      "Epoch 186/250 | Best Val F1: 0.9259 | Current Val F1: 0.9187 | LR: 0.000083\n",
      "Early stopping at epoch 187. Best F1: 0.9259\n",
      "--- Finished Training --- Best F1: 0.9259\n",
      "Fold 10 Final Val F1: 0.9259\n",
      "\n",
      "--- Fold 11/15 --- (Robust-LSTM_H384_L2_v13_fold_11) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_11 ---\n",
      "Epoch   3/250 | Best Val F1: 0.8014 | Current Val F1: 0.8014 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8282 | Current Val F1: 0.8282 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.8373 | Current Val F1: 0.8373 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.8475 | Current Val F1: 0.8475 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.8584 | Current Val F1: 0.8550 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.8663 | Current Val F1: 0.8577 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.8829 | Current Val F1: 0.8730 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.8829 | Current Val F1: 0.8778 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.8845 | Current Val F1: 0.8826 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.8944 | Current Val F1: 0.8914 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.8944 | Current Val F1: 0.8897 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.9000 | Current Val F1: 0.8879 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.9000 | Current Val F1: 0.8795 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.9000 | Current Val F1: 0.8941 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.9000 | Current Val F1: 0.8957 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.9000 | Current Val F1: 0.8783 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.9000 | Current Val F1: 0.8935 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.9000 | Current Val F1: 0.8910 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.9018 | Current Val F1: 0.8996 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.9018 | Current Val F1: 0.8788 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.9018 | Current Val F1: 0.8900 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.9018 | Current Val F1: 0.8896 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.9018 | Current Val F1: 0.8819 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.9018 | Current Val F1: 0.8858 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.9018 | Current Val F1: 0.8950 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.9018 | Current Val F1: 0.8911 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.9018 | Current Val F1: 0.8852 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.9018 | Current Val F1: 0.8839 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.9029 | Current Val F1: 0.9029 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.9029 | Current Val F1: 0.8923 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.9029 | Current Val F1: 0.8841 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.9029 | Current Val F1: 0.8791 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.9029 | Current Val F1: 0.8923 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.9029 | Current Val F1: 0.8959 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.9029 | Current Val F1: 0.8909 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.9029 | Current Val F1: 0.8897 | LR: 0.000257\n",
      "Epoch 111/250 | Best Val F1: 0.9029 | Current Val F1: 0.8920 | LR: 0.000252\n",
      "Epoch 114/250 | Best Val F1: 0.9029 | Current Val F1: 0.8966 | LR: 0.000248\n",
      "Epoch 117/250 | Best Val F1: 0.9029 | Current Val F1: 0.8981 | LR: 0.000243\n",
      "Epoch 120/250 | Best Val F1: 0.9029 | Current Val F1: 0.8915 | LR: 0.000237\n",
      "Epoch 123/250 | Best Val F1: 0.9029 | Current Val F1: 0.8957 | LR: 0.000232\n",
      "Epoch 126/250 | Best Val F1: 0.9029 | Current Val F1: 0.8969 | LR: 0.000226\n",
      "Epoch 129/250 | Best Val F1: 0.9029 | Current Val F1: 0.8954 | LR: 0.000220\n",
      "Epoch 132/250 | Best Val F1: 0.9029 | Current Val F1: 0.8837 | LR: 0.000213\n",
      "Epoch 135/250 | Best Val F1: 0.9029 | Current Val F1: 0.8906 | LR: 0.000207\n",
      "Epoch 138/250 | Best Val F1: 0.9029 | Current Val F1: 0.8904 | LR: 0.000200\n",
      "Epoch 141/250 | Best Val F1: 0.9029 | Current Val F1: 0.8864 | LR: 0.000193\n",
      "Epoch 144/250 | Best Val F1: 0.9029 | Current Val F1: 0.8924 | LR: 0.000186\n",
      "Epoch 147/250 | Best Val F1: 0.9029 | Current Val F1: 0.8879 | LR: 0.000179\n",
      "Epoch 150/250 | Best Val F1: 0.9029 | Current Val F1: 0.8894 | LR: 0.000172\n",
      "Epoch 153/250 | Best Val F1: 0.9029 | Current Val F1: 0.8928 | LR: 0.000164\n",
      "Epoch 156/250 | Best Val F1: 0.9029 | Current Val F1: 0.8944 | LR: 0.000157\n",
      "Epoch 159/250 | Best Val F1: 0.9029 | Current Val F1: 0.8938 | LR: 0.000149\n",
      "Early stopping at epoch 162. Best F1: 0.9029\n",
      "--- Finished Training --- Best F1: 0.9029\n",
      "Fold 11 Final Val F1: 0.9029\n",
      "\n",
      "--- Fold 12/15 --- (Robust-LSTM_H384_L2_v13_fold_12) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_12 ---\n",
      "Epoch   3/250 | Best Val F1: 0.8079 | Current Val F1: 0.8079 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8192 | Current Val F1: 0.8192 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.8536 | Current Val F1: 0.8536 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.8654 | Current Val F1: 0.8654 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.8801 | Current Val F1: 0.8801 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.8827 | Current Val F1: 0.8721 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.8859 | Current Val F1: 0.8859 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.8865 | Current Val F1: 0.8865 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.8934 | Current Val F1: 0.8836 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.8934 | Current Val F1: 0.8776 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.9026 | Current Val F1: 0.9026 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.9026 | Current Val F1: 0.8771 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.9026 | Current Val F1: 0.8946 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.9026 | Current Val F1: 0.8857 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.9026 | Current Val F1: 0.8959 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.9026 | Current Val F1: 0.8865 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.9026 | Current Val F1: 0.8919 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.9037 | Current Val F1: 0.8812 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.9037 | Current Val F1: 0.8764 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.9037 | Current Val F1: 0.8942 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.9037 | Current Val F1: 0.8734 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.9037 | Current Val F1: 0.8944 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.9037 | Current Val F1: 0.8820 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.9037 | Current Val F1: 0.8991 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.9037 | Current Val F1: 0.8797 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.9037 | Current Val F1: 0.8988 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.9037 | Current Val F1: 0.8970 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.9037 | Current Val F1: 0.8941 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.9037 | Current Val F1: 0.8936 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.9037 | Current Val F1: 0.8957 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.9037 | Current Val F1: 0.8883 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.9037 | Current Val F1: 0.8892 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.9037 | Current Val F1: 0.8954 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.9037 | Current Val F1: 0.9029 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.9037 | Current Val F1: 0.8996 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.9037 | Current Val F1: 0.9018 | LR: 0.000257\n",
      "Epoch 111/250 | Best Val F1: 0.9037 | Current Val F1: 0.8971 | LR: 0.000252\n",
      "Epoch 114/250 | Best Val F1: 0.9037 | Current Val F1: 0.8892 | LR: 0.000248\n",
      "Epoch 117/250 | Best Val F1: 0.9037 | Current Val F1: 0.8969 | LR: 0.000243\n",
      "Epoch 120/250 | Best Val F1: 0.9037 | Current Val F1: 0.8932 | LR: 0.000237\n",
      "Epoch 123/250 | Best Val F1: 0.9037 | Current Val F1: 0.8987 | LR: 0.000232\n",
      "Epoch 126/250 | Best Val F1: 0.9037 | Current Val F1: 0.9006 | LR: 0.000226\n",
      "Early stopping at epoch 128. Best F1: 0.9037\n",
      "--- Finished Training --- Best F1: 0.9037\n",
      "Fold 12 Final Val F1: 0.9037\n",
      "\n",
      "--- Fold 13/15 --- (Robust-LSTM_H384_L2_v13_fold_13) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_13 ---\n",
      "Epoch   3/250 | Best Val F1: 0.7927 | Current Val F1: 0.7927 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8619 | Current Val F1: 0.8619 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.8967 | Current Val F1: 0.8967 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.9028 | Current Val F1: 0.9028 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.9108 | Current Val F1: 0.9106 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.9211 | Current Val F1: 0.9211 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.9234 | Current Val F1: 0.9189 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.9234 | Current Val F1: 0.9177 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.9234 | Current Val F1: 0.9209 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.9294 | Current Val F1: 0.9151 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.9294 | Current Val F1: 0.9256 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.9294 | Current Val F1: 0.9177 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.9294 | Current Val F1: 0.9195 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.9294 | Current Val F1: 0.9017 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.9294 | Current Val F1: 0.9223 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.9294 | Current Val F1: 0.9157 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.9294 | Current Val F1: 0.9168 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.9294 | Current Val F1: 0.9098 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.9294 | Current Val F1: 0.9151 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.9294 | Current Val F1: 0.9177 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.9294 | Current Val F1: 0.9125 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.9294 | Current Val F1: 0.9171 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.9294 | Current Val F1: 0.9186 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.9294 | Current Val F1: 0.9110 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.9294 | Current Val F1: 0.9158 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.9294 | Current Val F1: 0.9190 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.9294 | Current Val F1: 0.9134 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.9294 | Current Val F1: 0.9137 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.9294 | Current Val F1: 0.9210 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.9294 | Current Val F1: 0.9213 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.9294 | Current Val F1: 0.9144 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.9294 | Current Val F1: 0.9124 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.9294 | Current Val F1: 0.9132 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.9294 | Current Val F1: 0.9104 | LR: 0.000265\n",
      "Early stopping at epoch 104. Best F1: 0.9294\n",
      "--- Finished Training --- Best F1: 0.9294\n",
      "Fold 13 Final Val F1: 0.9294\n",
      "\n",
      "--- Fold 14/15 --- (Robust-LSTM_H384_L2_v13_fold_14) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_14 ---\n",
      "Epoch   3/250 | Best Val F1: 0.8276 | Current Val F1: 0.8276 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8848 | Current Val F1: 0.8848 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.9142 | Current Val F1: 0.9142 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.9256 | Current Val F1: 0.9256 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.9270 | Current Val F1: 0.9267 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.9371 | Current Val F1: 0.9371 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.9371 | Current Val F1: 0.9259 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.9371 | Current Val F1: 0.9353 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.9371 | Current Val F1: 0.9320 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.9371 | Current Val F1: 0.9315 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.9405 | Current Val F1: 0.9405 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.9461 | Current Val F1: 0.9461 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.9461 | Current Val F1: 0.9455 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.9461 | Current Val F1: 0.9292 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.9461 | Current Val F1: 0.9359 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.9461 | Current Val F1: 0.9340 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.9461 | Current Val F1: 0.9296 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.9461 | Current Val F1: 0.9244 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.9461 | Current Val F1: 0.9246 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.9461 | Current Val F1: 0.9283 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.9461 | Current Val F1: 0.9397 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.9461 | Current Val F1: 0.9342 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.9461 | Current Val F1: 0.9254 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.9461 | Current Val F1: 0.9285 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.9461 | Current Val F1: 0.9282 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.9461 | Current Val F1: 0.9280 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.9461 | Current Val F1: 0.9258 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.9461 | Current Val F1: 0.9282 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.9461 | Current Val F1: 0.9228 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.9461 | Current Val F1: 0.9237 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.9461 | Current Val F1: 0.9310 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.9461 | Current Val F1: 0.9248 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.9461 | Current Val F1: 0.9294 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.9461 | Current Val F1: 0.9290 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.9461 | Current Val F1: 0.9268 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.9461 | Current Val F1: 0.9295 | LR: 0.000257\n",
      "Early stopping at epoch 111. Best F1: 0.9461\n",
      "--- Finished Training --- Best F1: 0.9461\n",
      "Fold 14 Final Val F1: 0.9461\n",
      "\n",
      "--- Fold 15/15 --- (Robust-LSTM_H384_L2_v13_fold_15) ---\n",
      "--- Starting Training: Robust-LSTM_H384_L2_v13_fold_15 ---\n",
      "Epoch   3/250 | Best Val F1: 0.8054 | Current Val F1: 0.8054 | LR: 0.000012\n",
      "Epoch   6/250 | Best Val F1: 0.8495 | Current Val F1: 0.8495 | LR: 0.000015\n",
      "Epoch   9/250 | Best Val F1: 0.8650 | Current Val F1: 0.8650 | LR: 0.000021\n",
      "Epoch  12/250 | Best Val F1: 0.8963 | Current Val F1: 0.8963 | LR: 0.000028\n",
      "Epoch  15/250 | Best Val F1: 0.9251 | Current Val F1: 0.9229 | LR: 0.000037\n",
      "Epoch  18/250 | Best Val F1: 0.9282 | Current Val F1: 0.9228 | LR: 0.000048\n",
      "Epoch  21/250 | Best Val F1: 0.9334 | Current Val F1: 0.9334 | LR: 0.000060\n",
      "Epoch  24/250 | Best Val F1: 0.9348 | Current Val F1: 0.9348 | LR: 0.000074\n",
      "Epoch  27/250 | Best Val F1: 0.9348 | Current Val F1: 0.9152 | LR: 0.000089\n",
      "Epoch  30/250 | Best Val F1: 0.9360 | Current Val F1: 0.9360 | LR: 0.000104\n",
      "Epoch  33/250 | Best Val F1: 0.9360 | Current Val F1: 0.9266 | LR: 0.000121\n",
      "Epoch  36/250 | Best Val F1: 0.9406 | Current Val F1: 0.9406 | LR: 0.000138\n",
      "Epoch  39/250 | Best Val F1: 0.9406 | Current Val F1: 0.9285 | LR: 0.000154\n",
      "Epoch  42/250 | Best Val F1: 0.9406 | Current Val F1: 0.9259 | LR: 0.000171\n",
      "Epoch  45/250 | Best Val F1: 0.9406 | Current Val F1: 0.9090 | LR: 0.000188\n",
      "Epoch  48/250 | Best Val F1: 0.9406 | Current Val F1: 0.9305 | LR: 0.000203\n",
      "Epoch  51/250 | Best Val F1: 0.9406 | Current Val F1: 0.9320 | LR: 0.000218\n",
      "Epoch  54/250 | Best Val F1: 0.9406 | Current Val F1: 0.9186 | LR: 0.000232\n",
      "Epoch  57/250 | Best Val F1: 0.9406 | Current Val F1: 0.9200 | LR: 0.000244\n",
      "Epoch  60/250 | Best Val F1: 0.9406 | Current Val F1: 0.9327 | LR: 0.000255\n",
      "Epoch  63/250 | Best Val F1: 0.9406 | Current Val F1: 0.9216 | LR: 0.000264\n",
      "Epoch  66/250 | Best Val F1: 0.9406 | Current Val F1: 0.9268 | LR: 0.000271\n",
      "Epoch  69/250 | Best Val F1: 0.9406 | Current Val F1: 0.9140 | LR: 0.000276\n",
      "Epoch  72/250 | Best Val F1: 0.9406 | Current Val F1: 0.9235 | LR: 0.000280\n",
      "Epoch  75/250 | Best Val F1: 0.9406 | Current Val F1: 0.9277 | LR: 0.000281\n",
      "Epoch  78/250 | Best Val F1: 0.9406 | Current Val F1: 0.9140 | LR: 0.000280\n",
      "Epoch  81/250 | Best Val F1: 0.9406 | Current Val F1: 0.9312 | LR: 0.000280\n",
      "Epoch  84/250 | Best Val F1: 0.9406 | Current Val F1: 0.9195 | LR: 0.000279\n",
      "Epoch  87/250 | Best Val F1: 0.9406 | Current Val F1: 0.9164 | LR: 0.000277\n",
      "Epoch  90/250 | Best Val F1: 0.9406 | Current Val F1: 0.9291 | LR: 0.000276\n",
      "Epoch  93/250 | Best Val F1: 0.9406 | Current Val F1: 0.9196 | LR: 0.000273\n",
      "Epoch  96/250 | Best Val F1: 0.9406 | Current Val F1: 0.9294 | LR: 0.000271\n",
      "Epoch  99/250 | Best Val F1: 0.9406 | Current Val F1: 0.9247 | LR: 0.000268\n",
      "Epoch 102/250 | Best Val F1: 0.9406 | Current Val F1: 0.9299 | LR: 0.000265\n",
      "Epoch 105/250 | Best Val F1: 0.9406 | Current Val F1: 0.9292 | LR: 0.000261\n",
      "Epoch 108/250 | Best Val F1: 0.9406 | Current Val F1: 0.9230 | LR: 0.000257\n",
      "Early stopping at epoch 111. Best F1: 0.9406\n",
      "--- Finished Training --- Best F1: 0.9406\n",
      "Fold 15 Final Val F1: 0.9406\n",
      "\n",
      "--- üèÜ Repeated K-Fold Training Complete --- Average F1: 0.9300 | Std Dev: 0.0222\n"
     ]
    }
   ],
   "source": [
    "# --- MODIFIED: Use RepeatedStratifiedKFold --- \n",
    "rskf = RepeatedStratifiedKFold(n_splits=N_SPLITS, n_repeats=N_REPEATS, random_state=SEED)\n",
    "fold_val_f1_list = []\n",
    "EPOCHS = 250\n",
    "\n",
    "# --- 1. Define feature indices for dual scaling ---\n",
    "spiky_indices_orig = list(range(13, 26)) \n",
    "normal_indices_orig = list(range(13)) + list(range(26, 30)) + [34] \n",
    "categorical_indices_orig = list(range(30, 34)) + [35]\n",
    "\n",
    "# --- 2. Define reordered indices ---\n",
    "continuous_indices_orig_all = normal_indices_orig + spiky_indices_orig\n",
    "reordered_mapping = {orig_idx: new_idx for new_idx, orig_idx in enumerate(continuous_indices_orig_all)}\n",
    "normal_indices_reordered = [reordered_mapping[i] for i in normal_indices_orig]\n",
    "spiky_indices_reordered = [reordered_mapping[i] for i in spiky_indices_orig]\n",
    "\n",
    "# --- 3. Reorder the main training data *before* the loop ---\n",
    "X_train_main_reordered = np.concatenate([\n",
    "    X_train[:, :, continuous_indices_orig_all],\n",
    "    X_train[:, :, categorical_indices_orig]\n",
    "], axis=2)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(rskf.split(X_train_main_reordered, y_train)):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    print(f\"\\n--- Fold {fold+1}/{TOTAL_FOLDS} --- ({fold_name}) ---\")\n",
    "    \n",
    "    X_train_fold, y_train_fold = X_train_main_reordered[train_idx], y_train[train_idx]\n",
    "    X_val_fold, y_val_fold = X_train_main_reordered[val_idx], y_train[val_idx]\n",
    "\n",
    "    # --- 4. Define the CORRECTED ColumnTransformer for this fold ---\n",
    "    preprocessor_fold = ColumnTransformer(\n",
    "        [\n",
    "            ('normal_scaler', StandardScaler(), normal_indices_reordered),\n",
    "            ('spiky_scaler', MinMaxScaler(), spiky_indices_reordered) \n",
    "        ], \n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    # Fit the preprocessor ONLY on the training data for this fold\n",
    "    ns, ts, f = X_train_fold.shape\n",
    "    X_train_scaled = preprocessor_fold.fit_transform(X_train_fold.reshape(ns*ts, f)).reshape(ns, ts, f)\n",
    "    ns_val, ts_val, f_val = X_val_fold.shape\n",
    "    X_val_scaled = preprocessor_fold.transform(X_val_fold.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "    \n",
    "    X_train_w, y_train_w, _ = create_sliding_windows(X_train_scaled, y_train_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(X_val_scaled, y_val_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    \n",
    "    train_loader = make_loader(TensorDataset(torch.from_numpy(X_train_w).float(), torch.from_numpy(y_train_w).long()), FINAL_CONFIG['batch_size'], True, True)\n",
    "    val_loader = make_loader(TensorDataset(torch.from_numpy(X_val_w).float(), torch.from_numpy(y_val_w).long()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "    model_config_kfold = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "    model_fold = RecurrentClassifier(**model_config_kfold, num_classes=N_CLASSES).to(device)\n",
    "    model_fold = torch.compile(model_fold, backend=\"eager\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model_fold.parameters(), lr=FINAL_CONFIG['lr'], weight_decay=FINAL_CONFIG['l2_lambda'])\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=FINAL_CONFIG['lr'], epochs=EPOCHS, steps_per_epoch=len(train_loader))\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = FocalLoss(alpha=alpha_tensor, gamma=FINAL_CONFIG['focal_loss_gamma'])\n",
    "\n",
    "    model_fold_uncompiled = fit(model_fold, train_loader, val_loader, EPOCHS, criterion, optimizer, scheduler, scaler, device, 75, fold_name)\n",
    "    \n",
    "    _, val_f1 = validate_one_epoch(model_fold_uncompiled, val_loader, criterion, device)\n",
    "    fold_val_f1_list.append(val_f1)\n",
    "    print(f\"Fold {fold+1} Final Val F1: {val_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n--- üèÜ Repeated K-Fold Training Complete --- Average F1: {np.mean(fold_val_f1_list):.4f} | Std Dev: {np.std(fold_val_f1_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8fca0e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Regenerating Fold Indices for Analysis ---\n",
      "Successfully regenerated indices for 15 folds.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Regenerating Fold Indices for Analysis ---\")\n",
    "\n",
    "fold_indices = []\n",
    "for train_idx, val_idx in rskf.split(X_train_main_reordered, y_train):\n",
    "    fold_indices.append({'train': train_idx, 'val': val_idx})\n",
    "\n",
    "print(f\"Successfully regenerated indices for {len(fold_indices)} folds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed43c62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Good Folds ---\n",
      "Label Distribution (Proportions):\n",
      "0    0.084246\n",
      "1    0.143218\n",
      "2    0.772536\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "'is_pirate' Distribution (Proportions):\n",
      "0.0    0.98989\n",
      "1.0    0.01011\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Analyzing Bad Folds ---\n",
      "Label Distribution (Proportions):\n",
      "0    0.084034\n",
      "1    0.142857\n",
      "2    0.773109\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "'is_pirate' Distribution (Proportions):\n",
      "0.0    0.994958\n",
      "1.0    0.005042\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURE THIS ---\n",
    "BAD_FOLD_INDICES = [0,6,7,10,11] \n",
    "GOOD_FOLD_INDICES = list(set(range(TOTAL_FOLDS)) - set(BAD_FOLD_INDICES))\n",
    "# --------------------\n",
    "\n",
    "def analyze_fold_distribution(fold_group_indices, group_name):\n",
    "    print(f\"\\n--- Analyzing {group_name} Folds ---\")\n",
    "    \n",
    "    # Collect all validation samples from this group of folds\n",
    "    all_val_indices = np.concatenate([fold_indices[i]['val'] for i in fold_group_indices])\n",
    "    \n",
    "    # Get the labels for these samples\n",
    "    y_val_group = y_train[all_val_indices]\n",
    "    \n",
    "    # Get the 'is_pirate' status for these samples\n",
    "    # Assumes X_train_main_reordered is available and has pirate status as the last feature\n",
    "    pirate_status_group = X_train_main_reordered[all_val_indices][:, 0, -1] \n",
    "    \n",
    "    print(\"Label Distribution (Proportions):\")\n",
    "    print(pd.Series(y_val_group).value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    print(\"\\n'is_pirate' Distribution (Proportions):\")\n",
    "    print(pd.Series(pirate_status_group).value_counts(normalize=True).sort_index())\n",
    "\n",
    "analyze_fold_distribution(GOOD_FOLD_INDICES, \"Good\")\n",
    "analyze_fold_distribution(BAD_FOLD_INDICES, \"Bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5469121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Mistakes in Fold 1 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.00      0.00      0.00       760\n",
      "    low_pain       0.62      0.08      0.14      1292\n",
      "     no_pain       0.78      1.00      0.88      6992\n",
      "\n",
      "    accuracy                           0.78      9044\n",
      "   macro avg       0.47      0.36      0.34      9044\n",
      "weighted avg       0.70      0.78      0.70      9044\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0   38  722]\n",
      " [   0  100 1192]\n",
      " [   0   23 6969]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 2 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.85      0.84      0.84       760\n",
      "    low_pain       0.96      0.83      0.89      1292\n",
      "     no_pain       0.95      0.98      0.97      6992\n",
      "\n",
      "    accuracy                           0.95      9044\n",
      "   macro avg       0.92      0.88      0.90      9044\n",
      "weighted avg       0.95      0.95      0.95      9044\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 638   10  112]\n",
      " [   5 1069  218]\n",
      " [ 109   31 6852]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 3 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.87      0.81      0.84       760\n",
      "    low_pain       0.98      0.86      0.91      1292\n",
      "     no_pain       0.96      0.99      0.98      6992\n",
      "\n",
      "    accuracy                           0.96      9044\n",
      "   macro avg       0.94      0.89      0.91      9044\n",
      "weighted avg       0.96      0.96      0.96      9044\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 618    7  135]\n",
      " [  56 1106  130]\n",
      " [  39   21 6932]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 4 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.79      0.76      0.77       760\n",
      "    low_pain       0.95      0.87      0.91      1292\n",
      "     no_pain       0.96      0.98      0.97      6992\n",
      "\n",
      "    accuracy                           0.94      9044\n",
      "   macro avg       0.90      0.87      0.88      9044\n",
      "weighted avg       0.94      0.94      0.94      9044\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 577    0  183]\n",
      " [  54 1127  111]\n",
      " [ 103   58 6831]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 5 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.75      0.55      0.64       760\n",
      "    low_pain       0.85      0.72      0.78      1292\n",
      "     no_pain       0.94      0.99      0.97      6916\n",
      "\n",
      "    accuracy                           0.92      8968\n",
      "   macro avg       0.85      0.75      0.79      8968\n",
      "weighted avg       0.91      0.92      0.91      8968\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 421  131  208]\n",
      " [ 129  925  238]\n",
      " [  12   30 6874]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 6 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.79      0.44      0.57       760\n",
      "    low_pain       0.89      0.76      0.82      1292\n",
      "     no_pain       0.92      0.99      0.95      6992\n",
      "\n",
      "    accuracy                           0.91      9044\n",
      "   macro avg       0.87      0.73      0.78      9044\n",
      "weighted avg       0.90      0.91      0.90      9044\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 337   99  324]\n",
      " [   8  982  302]\n",
      " [  79   19 6894]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 7 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.78      0.72      0.75       760\n",
      "    low_pain       0.89      0.90      0.89      1292\n",
      "     no_pain       0.98      0.98      0.98      6992\n",
      "\n",
      "    accuracy                           0.95      9044\n",
      "   macro avg       0.88      0.87      0.87      9044\n",
      "weighted avg       0.95      0.95      0.95      9044\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 546  121   93]\n",
      " [  67 1157   68]\n",
      " [  90   23 6879]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 8 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.89      0.80      0.85       760\n",
      "    low_pain       0.91      0.88      0.89      1292\n",
      "     no_pain       0.98      0.99      0.98      6992\n",
      "\n",
      "    accuracy                           0.96      9044\n",
      "   macro avg       0.93      0.89      0.91      9044\n",
      "weighted avg       0.96      0.96      0.96      9044\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 611   70   79]\n",
      " [  65 1135   92]\n",
      " [  10   44 6938]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 9 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.93      0.78      0.85       760\n",
      "    low_pain       0.90      0.91      0.90      1292\n",
      "     no_pain       0.96      0.98      0.97      6992\n",
      "\n",
      "    accuracy                           0.95      9044\n",
      "   macro avg       0.93      0.89      0.91      9044\n",
      "weighted avg       0.95      0.95      0.95      9044\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 590    3  167]\n",
      " [   8 1175  109]\n",
      " [  35  127 6830]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 10 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.78      0.59      0.67       760\n",
      "    low_pain       0.90      0.88      0.89      1292\n",
      "     no_pain       0.95      0.98      0.96      6916\n",
      "\n",
      "    accuracy                           0.93      8968\n",
      "   macro avg       0.88      0.82      0.84      8968\n",
      "weighted avg       0.93      0.93      0.93      8968\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 450   56  254]\n",
      " [  31 1133  128]\n",
      " [  94   66 6756]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 11 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.81      0.47      0.60       760\n",
      "    low_pain       0.89      0.81      0.85      1292\n",
      "     no_pain       0.92      0.98      0.95      6992\n",
      "\n",
      "    accuracy                           0.91      9044\n",
      "   macro avg       0.87      0.75      0.80      9044\n",
      "weighted avg       0.91      0.91      0.90      9044\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 359    0  401]\n",
      " [  50 1050  192]\n",
      " [  35  134 6823]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 12 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.79      0.76      0.78       760\n",
      "    low_pain       0.77      0.63      0.69      1292\n",
      "     no_pain       0.94      0.97      0.96      6992\n",
      "\n",
      "    accuracy                           0.91      9044\n",
      "   macro avg       0.83      0.79      0.81      9044\n",
      "weighted avg       0.90      0.91      0.90      9044\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 576  134   50]\n",
      " [  85  814  393]\n",
      " [  64  116 6812]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 13 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.80      0.69      0.74       760\n",
      "    low_pain       0.87      0.86      0.86      1292\n",
      "     no_pain       0.95      0.97      0.96      6992\n",
      "\n",
      "    accuracy                           0.93      9044\n",
      "   macro avg       0.87      0.84      0.86      9044\n",
      "weighted avg       0.93      0.93      0.93      9044\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 524   49  187]\n",
      " [  44 1108  140]\n",
      " [  86  121 6785]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 14 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.86      0.61      0.72       760\n",
      "    low_pain       0.90      0.96      0.93      1292\n",
      "     no_pain       0.97      0.98      0.98      6992\n",
      "\n",
      "    accuracy                           0.95      9044\n",
      "   macro avg       0.91      0.85      0.87      9044\n",
      "weighted avg       0.95      0.95      0.95      9044\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 466   68  226]\n",
      " [  36 1243   13]\n",
      " [  41   70 6881]]\n",
      "\n",
      "--- Analyzing Mistakes in Fold 15 ---\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       0.85      0.79      0.82       760\n",
      "    low_pain       0.94      0.76      0.84      1292\n",
      "     no_pain       0.95      1.00      0.97      6916\n",
      "\n",
      "    accuracy                           0.94      8968\n",
      "   macro avg       0.91      0.85      0.88      8968\n",
      "weighted avg       0.94      0.94      0.94      8968\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 601   64   95]\n",
      " [  75  977  240]\n",
      " [  31    0 6885]]\n"
     ]
    }
   ],
   "source": [
    "model_config_final = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "criterion = FocalLoss(alpha=alpha_tensor, gamma=FINAL_CONFIG['focal_loss_gamma']) # Or whichever loss you used\n",
    "\n",
    "# We need the preprocessor fitted on the full training data for consistency\n",
    "preprocessor_final = ColumnTransformer(\n",
    "    [\n",
    "        ('normal_scaler', StandardScaler(), normal_indices_reordered),\n",
    "        ('spiky_scaler', MinMaxScaler(), spiky_indices_reordered) \n",
    "    ], \n",
    "    remainder='passthrough'\n",
    ")\n",
    "preprocessor_final.fit(X_train_main_reordered.reshape(-1, X_train_main_reordered.shape[2]))\n",
    "\n",
    "\n",
    "for fold_idx in range(TOTAL_FOLDS):\n",
    "    print(f\"\\n--- Analyzing Mistakes in Fold {fold_idx+1} ---\")\n",
    "    \n",
    "    # 1. Load the corresponding model\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold_idx+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    \n",
    "    model = RecurrentClassifier(**model_config_final, num_classes=N_CLASSES).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # 2. Get the validation data for THIS fold\n",
    "    val_indices = fold_indices[fold_idx]['val']\n",
    "    X_val_fold = X_train_main_reordered[val_indices]\n",
    "    y_val_fold = y_train[val_indices]\n",
    "    \n",
    "    # 3. Scale and prepare the data loader\n",
    "    ns_val, ts_val, f_val = X_val_fold.shape\n",
    "    X_val_scaled = preprocessor_final.transform(X_val_fold.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(X_val_scaled, y_val_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    val_loader = make_loader(TensorDataset(torch.from_numpy(X_val_w).float(), torch.from_numpy(y_val_w).long()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "    # 4. Generate predictions and report\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            logits = model(x.to(device))\n",
    "            all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "            \n",
    "    preds = np.concatenate(all_preds)\n",
    "    targets = np.concatenate(all_targets)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(targets, preds, target_names=le.classes_))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(targets, preds)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9d380",
   "metadata": {},
   "source": [
    "## üì¨ 7. Phase 3: Ensemble Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b31871e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing test dataset for submission ---\n",
      "Loading model 2/15 from models/Robust-LSTM_H384_L2_v13_fold_2_best_model.pt...\n",
      "Loading model 3/15 from models/Robust-LSTM_H384_L2_v13_fold_3_best_model.pt...\n",
      "Loading model 4/15 from models/Robust-LSTM_H384_L2_v13_fold_4_best_model.pt...\n",
      "Loading model 5/15 from models/Robust-LSTM_H384_L2_v13_fold_5_best_model.pt...\n",
      "Loading model 6/15 from models/Robust-LSTM_H384_L2_v13_fold_6_best_model.pt...\n",
      "Loading model 9/15 from models/Robust-LSTM_H384_L2_v13_fold_9_best_model.pt...\n",
      "Loading model 10/15 from models/Robust-LSTM_H384_L2_v13_fold_10_best_model.pt...\n",
      "Loading model 13/15 from models/Robust-LSTM_H384_L2_v13_fold_13_best_model.pt...\n",
      "Loading model 14/15 from models/Robust-LSTM_H384_L2_v13_fold_14_best_model.pt...\n",
      "Loading model 15/15 from models/Robust-LSTM_H384_L2_v13_fold_15_best_model.pt...\n",
      "\n",
      "Successfully saved to submissions\\submission_Robust-LSTM_H384_L2_v13.csv!\n",
      "  sample_index    label\n",
      "0          000  no_pain\n",
      "1          001  no_pain\n",
      "2          002  no_pain\n",
      "3          003  no_pain\n",
      "4          004  no_pain\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Preparing test dataset for submission ---\")\n",
    "\n",
    "# --- 1. Define feature indices for dual scaling ---\n",
    "spiky_indices_orig = list(range(13, 26)) \n",
    "normal_indices_orig = list(range(13)) + list(range(26, 30)) + [34] \n",
    "categorical_indices_orig = list(range(30, 34)) + [35]\n",
    "\n",
    "# --- 2. Define reordered indices ---\n",
    "continuous_indices_orig_all = normal_indices_orig + spiky_indices_orig\n",
    "reordered_mapping = {orig_idx: new_idx for new_idx, orig_idx in enumerate(continuous_indices_orig_all)}\n",
    "normal_indices_reordered = [reordered_mapping[i] for i in normal_indices_orig]\n",
    "spiky_indices_reordered = [reordered_mapping[i] for i in spiky_indices_orig]\n",
    "\n",
    "# --- 3. Reorder submission data columns to match training ---\n",
    "X_submission_reordered = np.concatenate([\n",
    "    X_submission_engineered[:, :, continuous_indices_orig_all],\n",
    "    X_submission_engineered[:, :, categorical_indices_orig]], axis=2)\n",
    "\n",
    "# --- 4. Reorder the main training data (to fit the final preprocessor) ---\n",
    "X_train_main_reordered = np.concatenate([\n",
    "    X_train[:, :, continuous_indices_orig_all],\n",
    "    X_train[:, :, categorical_indices_orig]\n",
    "], axis=2)\n",
    "\n",
    "# --- 5. Use a final preprocessor fitted on the ENTIRE main training set ---\n",
    "preprocessor_final = ColumnTransformer(\n",
    "    [\n",
    "        ('normal_scaler', StandardScaler(), normal_indices_reordered),\n",
    "        ('spiky_scaler', MinMaxScaler(), spiky_indices_reordered) \n",
    "    ], \n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "ns, ts, f = X_train_main_reordered.shape\n",
    "preprocessor_final.fit(X_train_main_reordered.reshape(ns * ts, f))\n",
    "\n",
    "# --- 6. Scale the submission data ---\n",
    "ns_sub, ts_sub, f_sub = X_submission_reordered.shape\n",
    "X_submission_scaled = preprocessor_final.transform(X_submission_reordered.reshape(ns_sub * ts_sub, f_sub)).reshape(ns_sub, ts_sub, f_sub)\n",
    "X_submission_w, submission_window_indices = create_sliding_windows(X_submission_scaled, y=None, window_size=FINAL_CONFIG['window_size'], stride=FINAL_CONFIG['stride'])\n",
    "submission_loader = make_loader(TensorDataset(torch.from_numpy(X_submission_w).float()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "model_config_final = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "all_fold_probabilities = []\n",
    "\n",
    "# --- MODIFIED: Use all 15 models for the ensemble ---\n",
    "for fold in GOOD_FOLD_INDICES:\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    print(f\"Loading model {fold+1}/{TOTAL_FOLDS} from {model_path}...\")\n",
    "    model_fold = RecurrentClassifier(**model_config_final, num_classes=N_CLASSES).to(device)\n",
    "    model_fold.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model_fold = torch.compile(model_fold, backend=\"eager\")\n",
    "    model_fold.eval()\n",
    "    \n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in submission_loader:\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                probs = torch.softmax(model_fold(inputs.to(device)), dim=1)\n",
    "                fold_preds.append(probs.cpu().numpy())\n",
    "    all_fold_probabilities.append(np.concatenate(fold_preds))\n",
    "\n",
    "mean_probabilities = np.mean(all_fold_probabilities, axis=0)\n",
    "df_probs = pd.DataFrame(mean_probabilities, columns=[f\"prob_{i}\" for i in range(N_CLASSES)])\n",
    "df_probs['original_index'] = submission_window_indices\n",
    "agg_probs = df_probs.groupby('original_index')[[f\"prob_{i}\" for i in range(N_CLASSES)]].mean().values\n",
    "final_predictions = le.inverse_transform(np.argmax(agg_probs, axis=1))\n",
    "\n",
    "submission_df = pd.DataFrame({'sample_index': sorted(X_test_long_df['sample_index'].unique()), 'label': final_predictions})\n",
    "submission_df['sample_index'] = submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "submission_filepath = os.path.join(\"submissions\", submission_filename_base)\n",
    "submission_df.to_csv(submission_filepath, index=False)\n",
    "print(f\"\\nSuccessfully saved to {submission_filepath}!\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e386d3",
   "metadata": {},
   "source": [
    "## üìú 8. Final Model Evaluation (on Hold-Out Test Set)\n",
    "\n",
    "Here we evaluate the final K-Fold ensemble on the 20% test set we created at the start. This provides an unbiased estimate of the model's performance on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dffdc9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing hold-out test set for final evaluation ---\n",
      "\n",
      "--- Generating predictions on the hold-out test set ---\n",
      "Loading model 1/15 from models/Robust-LSTM_H384_L2_v13_fold_1_best_model.pt...\n",
      "Loading model 2/15 from models/Robust-LSTM_H384_L2_v13_fold_2_best_model.pt...\n",
      "Loading model 3/15 from models/Robust-LSTM_H384_L2_v13_fold_3_best_model.pt...\n",
      "Loading model 4/15 from models/Robust-LSTM_H384_L2_v13_fold_4_best_model.pt...\n",
      "Loading model 5/15 from models/Robust-LSTM_H384_L2_v13_fold_5_best_model.pt...\n",
      "Loading model 6/15 from models/Robust-LSTM_H384_L2_v13_fold_6_best_model.pt...\n",
      "Loading model 7/15 from models/Robust-LSTM_H384_L2_v13_fold_7_best_model.pt...\n",
      "Loading model 8/15 from models/Robust-LSTM_H384_L2_v13_fold_8_best_model.pt...\n",
      "Loading model 9/15 from models/Robust-LSTM_H384_L2_v13_fold_9_best_model.pt...\n",
      "Loading model 10/15 from models/Robust-LSTM_H384_L2_v13_fold_10_best_model.pt...\n",
      "Loading model 11/15 from models/Robust-LSTM_H384_L2_v13_fold_11_best_model.pt...\n",
      "Loading model 12/15 from models/Robust-LSTM_H384_L2_v13_fold_12_best_model.pt...\n",
      "Loading model 13/15 from models/Robust-LSTM_H384_L2_v13_fold_13_best_model.pt...\n",
      "Loading model 14/15 from models/Robust-LSTM_H384_L2_v13_fold_14_best_model.pt...\n",
      "Loading model 15/15 from models/Robust-LSTM_H384_L2_v13_fold_15_best_model.pt...\n",
      "\n",
      "--- Final Ensemble Performance on Hold-Out Test Set ---\n",
      "\n",
      ">>> Weighted F1-Score: 0.9484 <<<\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   high_pain       1.00      0.50      0.67         6\n",
      "    low_pain       1.00      1.00      1.00         9\n",
      "     no_pain       0.95      1.00      0.97        52\n",
      "\n",
      "    accuracy                           0.96        67\n",
      "   macro avg       0.98      0.83      0.88        67\n",
      "weighted avg       0.96      0.96      0.95        67\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"--- Preparing hold-out test set for final evaluation ---\")\n",
    "\n",
    "# --- 1. Define feature indices (must match the preprocessor) ---\n",
    "spiky_indices_orig = list(range(13, 26)) \n",
    "normal_indices_orig = list(range(13)) + list(range(26, 30)) + [34] \n",
    "categorical_indices_orig = list(range(30, 34)) + [35]\n",
    "continuous_indices_orig_all = normal_indices_orig + spiky_indices_orig\n",
    "\n",
    "# --- 2. Re-order test data columns to match training ---\n",
    "X_test_reordered = np.concatenate([\n",
    "    X_test[:, :, continuous_indices_orig_all],\n",
    "    X_test[:, :, categorical_indices_orig]], axis=2)\n",
    "\n",
    "# --- 3. Use the same preprocessor that was fitted on the full training data ---\n",
    "# (The 'preprocessor_final' object from the previous cell is used here)\n",
    "ns_test, ts_test, f_test = X_test_reordered.shape\n",
    "X_test_scaled = preprocessor_final.transform(X_test_reordered.reshape(ns_test * ts_test, f_test)).reshape(ns_test, ts_test, f_test)\n",
    "\n",
    "# --- 4. Create sliding windows for the test set ---\n",
    "X_test_w, _, test_window_indices = create_sliding_windows(X_test_scaled, y=y_test, window_size=FINAL_CONFIG['window_size'], stride=FINAL_CONFIG['stride'])\n",
    "test_loader = make_loader(TensorDataset(torch.from_numpy(X_test_w).float()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "all_fold_test_probabilities = []\n",
    "print(\"\\n--- Generating predictions on the hold-out test set ---\")\n",
    "for fold in range(TOTAL_FOLDS):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    print(f\"Loading model {fold+1}/{TOTAL_FOLDS} from {model_path}...\")\n",
    "    model_fold = RecurrentClassifier(**model_config_final, num_classes=N_CLASSES).to(device)\n",
    "    model_fold.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model_fold.eval()\n",
    "    \n",
    "    fold_test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in test_loader:\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                probs = torch.softmax(model_fold(inputs.to(device)), dim=1)\n",
    "                fold_test_preds.append(probs.cpu().numpy())\n",
    "    all_fold_test_probabilities.append(np.concatenate(fold_test_preds))\n",
    "\n",
    "# --- 5. Aggregate predictions and evaluate ---\n",
    "mean_test_probabilities = np.mean(all_fold_test_probabilities, axis=0)\n",
    "df_test_probs = pd.DataFrame(mean_test_probabilities, columns=[f\"prob_{i}\" for i in range(N_CLASSES)])\n",
    "df_test_probs['original_index'] = test_window_indices\n",
    "agg_test_probs = df_test_probs.groupby('original_index')[[f\"prob_{i}\" for i in range(N_CLASSES)]].mean().values\n",
    "final_test_predictions = np.argmax(agg_test_probs, axis=1)\n",
    "\n",
    "# Get the true labels for the original samples that had windows created\n",
    "true_test_labels = y_test[np.unique(test_window_indices)]\n",
    "\n",
    "print(\"\\n--- Final Ensemble Performance on Hold-Out Test Set ---\")\n",
    "final_f1_score = f1_score(true_test_labels, final_test_predictions, average='weighted')\n",
    "print(f\"\\n>>> Weighted F1-Score: {final_f1_score:.4f} <<<\")\n",
    "\n",
    "# Classification Report\n",
    "class_names = le.classes_\n",
    "report = classification_report(true_test_labels, final_test_predictions, target_names=class_names)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_test_labels, final_test_predictions)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix on Hold-Out Test Set', fontsize=16)\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
