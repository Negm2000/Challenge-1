{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67bb0a3d",
   "metadata": {},
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è (v3: Selective Scaling)**\n",
    "\n",
    "This notebook implements a robust K-Fold Cross-Validation and Ensembling strategy. This version includes a key fix: **Selective Scaling**.\n",
    "\n",
    "**Strategy:**\n",
    "1.  **Feature Engineering:** Create an `is_pirate` binary feature (1 if pirate, 0 otherwise).\n",
    "2.  **Selective Scaling:** Apply `StandardScaler` *only* to the 35 continuous joint/pain features. The `is_pirate` feature is left as a raw 0/1 binary input.\n",
    "3.  **Hyperparameter Search:** Use Ray Tune & Optuna on a single 80/20 split to find a good set of hyperparameters (`FINAL_CONFIG`).\n",
    "4.  **K-Fold Training:** Train `K=5` models on 5 different folds, using the `FINAL_CONFIG`. Each model is trained with early stopping and saved to disk.\n",
    "5.  **Ensemble Prediction:** Load all 5 models, average their (softmax) probabilities on the test set, and aggregate these probabilities for a final, robust submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03291d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Setup & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba22088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 14:21:54,391\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-11-12 14:21:54,617\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GPU ---\n",
      "PyTorch version: 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Sklearn Imports ---\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Ray[tune] & Optuna Imports ---\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from functools import partial\n",
    "\n",
    "# --- Setup Directories & Device ---\n",
    "logs_dir = \"tensorboard\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8582ef",
   "metadata": {},
   "source": [
    "## üîÑ 2. Data Loading & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44203b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Data ---\n",
      "Loaded X_train_full (shape: (661, 160, 35)) and y_train_full (shape: (661,))\n",
      "Loaded X_test_full (shape: (1324, 160, 35))\n",
      "\n",
      "--- 2. Engineering 'is_pirate' Feature ---\n",
      "Created X_train_full_engineered (shape: (661, 160, 36))\n",
      "Created X_test_full_engineered (shape: (1324, 160, 36))\n",
      "N_FEATURES is now: 36\n",
      "\n",
      "--- 3. Calculating Class Weights ---\n",
      "Class counts (0, 1, 2): [511  94  56]\n",
      "Calculated class weights: tensor([0.0643, 0.3493, 0.5864], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Loading Data ---\")\n",
    "\n",
    "# --- Define File Paths and Features ---\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "try:\n",
    "    # Load features and labels\n",
    "    features_long_df = pd.read_csv(X_TRAIN_PATH)\n",
    "    labels_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "    X_test_long_df = pd.read_csv(X_TEST_PATH)\n",
    "    \n",
    "    # --- Define constants ---\n",
    "    N_TIMESTEPS = 160\n",
    "    JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(31)]\n",
    "    PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "    FEATURES = JOINT_FEATURES + PAIN_FEATURES\n",
    "    N_FEATURES_ORIGINAL = len(FEATURES) # This is 35\n",
    "    LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "    N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "    # --- Reshape function ---\n",
    "    def reshape_data(df, features_list, n_timesteps):\n",
    "        df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "        data_2d = df_pivot.values\n",
    "        n_samples = data_2d.shape[0]\n",
    "        data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "        return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "    # --- Load and reshape X_train_full (35 features) ---\n",
    "    X_train_full = reshape_data(\n",
    "        features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())], \n",
    "        FEATURES, \n",
    "        N_TIMESTEPS\n",
    "    )\n",
    "    \n",
    "    # --- Load and reshape X_test (35 features) ---\n",
    "    X_test_full = reshape_data(\n",
    "        X_test_long_df, FEATURES, N_TIMESTEPS\n",
    "    )\n",
    "\n",
    "    # --- Load and prepare y_train_full ---\n",
    "    y_train_full_df = labels_df.sort_values(by='sample_index')\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(LABEL_MAPPING.keys()))\n",
    "    y_train_full = le.transform(y_train_full_df['label'])\n",
    "    \n",
    "    print(f\"Loaded X_train_full (shape: {X_train_full.shape}) and y_train_full (shape: {y_train_full.shape})\")\n",
    "    print(f\"Loaded X_test_full (shape: {X_test_full.shape})\")\n",
    "\n",
    "    # --- 2. Engineer 'is_pirate' Feature (for Train) ---\n",
    "    print(\"\\n--- 2. Engineering 'is_pirate' Feature ---\")\n",
    "    static_cols = ['sample_index', 'n_legs', 'n_hands', 'n_eyes']\n",
    "    static_df = features_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    \n",
    "    pirate_filter = (\n",
    "        (static_df['n_legs'] == 'one+peg_leg') |\n",
    "        (static_df['n_hands'] == 'one+hook_hand') |\n",
    "        (static_df['n_eyes'] == 'one+eye_patch')\n",
    "    )\n",
    "    pirate_indices = static_df[pirate_filter].index\n",
    "    sample_indices_ordered = sorted(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())]['sample_index'].unique())\n",
    "    is_pirate_map = np.array([1 if idx in pirate_indices else 0 for idx in sample_indices_ordered])\n",
    "    pirate_feature_broadcast = np.tile(is_pirate_map.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    \n",
    "    # Concatenate with X_train_full\n",
    "    X_train_full_engineered = np.concatenate([X_train_full, pirate_feature_broadcast], axis=2)\n",
    "    \n",
    "    # --- 3. Engineer 'is_pirate' Feature (for Test) ---\n",
    "    static_df_test = X_test_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    pirate_filter_test = (\n",
    "        (static_df_test['n_legs'] == 'one+peg_leg') |\n",
    "        (static_df_test['n_hands'] == 'one+hook_hand') |\n",
    "        (static_df_test['n_eyes'] == 'one+eye_patch')\n",
    "    )\n",
    "    pirate_indices_test = static_df_test[pirate_filter_test].index\n",
    "    sample_indices_test_ordered = sorted(X_test_long_df['sample_index'].unique())\n",
    "    is_pirate_map_test = np.array([1 if idx in pirate_indices_test else 0 for idx in sample_indices_test_ordered])\n",
    "    pirate_feature_broadcast_test = np.tile(is_pirate_map_test.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    \n",
    "    # Concatenate with X_test_full\n",
    "    X_test_full_engineered = np.concatenate([X_test_full, pirate_feature_broadcast_test], axis=2)\n",
    "    \n",
    "    N_FEATURES_NEW = X_train_full_engineered.shape[2] # This will be 36\n",
    "    print(f\"Created X_train_full_engineered (shape: {X_train_full_engineered.shape})\")\n",
    "    print(f\"Created X_test_full_engineered (shape: {X_test_full_engineered.shape})\")\n",
    "    print(f\"N_FEATURES is now: {N_FEATURES_NEW}\")\n",
    "\n",
    "    # --- 4. Calculate Class Weights ---\n",
    "    print(\"\\n--- 3. Calculating Class Weights ---\")\n",
    "    class_counts_series = labels_df['label'].value_counts()\n",
    "    counts_ordered = class_counts_series.reindex(LABEL_MAPPING.keys()).values\n",
    "    class_weights_tensor = 1.0 / torch.tensor(counts_ordered, dtype=torch.float)\n",
    "    class_weights_tensor = class_weights_tensor / class_weights_tensor.sum() # Normalize weights\n",
    "    class_weights_tensor = class_weights_tensor.to(device)\n",
    "    \n",
    "    print(f\"Class counts (0, 1, 2): {counts_ordered}\")\n",
    "    print(f\"Calculated class weights: {class_weights_tensor}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find a required file. {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f8bc4",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a7c2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(X_3d, y=None, window_size=100, stride=20):\n",
    "    \"\"\"\n",
    "    Takes 3D data (n_samples, n_timesteps, n_features)\n",
    "    and creates overlapping windows.\n",
    "    \"\"\"\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    # This new array tracks which original sample each window came from.\n",
    "    window_indices = [] \n",
    "    \n",
    "    n_samples, n_timesteps, n_features = X_3d.shape\n",
    "    \n",
    "    # Iterate over each original sample\n",
    "    for i in range(n_samples):\n",
    "        sample = X_3d[i]\n",
    "        \n",
    "        # Slide a window over this sample\n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            window = sample[idx : idx + window_size]\n",
    "            new_X.append(window)\n",
    "            window_indices.append(i) # Track the original sample index (0, 1, 2...)\n",
    "            \n",
    "            if y is not None:\n",
    "                new_y.append(y[i]) # The label is the same for all windows\n",
    "                \n",
    "            idx += stride\n",
    "            \n",
    "    if y is not None:\n",
    "        # Return new X, new y, and the index mapping\n",
    "        return np.array(new_X), np.array(new_y), np.array(window_indices)\n",
    "    else:\n",
    "        # Return new X and the index mapping\n",
    "        return np.array(new_X), np.array(window_indices)\n",
    "    \n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    \"\"\"Creates a PyTorch DataLoader with optimized settings.\"\"\"\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=int(batch_size), # Ensure batch_size is an int\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=None,\n",
    "    )\n",
    "\n",
    "def recurrent_summary(model, input_size):\n",
    "    \"\"\"Custom summary function that correctly counts parameters for RNN/GRU/LSTM layers.\"\"\"\n",
    "    output_shapes = {}\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1  # Replace batch dimension with -1\n",
    "\n",
    "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
    "                    shape2 = list(output[1][0].shape)\n",
    "                else:  # RNN/GRU case: h_n only\n",
    "                    shape2 = list(output[1].shape)\n",
    "                shape2[1] = -1\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "        return hook\n",
    "\n",
    "    try:\n",
    "        device_summary = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device_summary = torch.device(\"cpu\")\n",
    "\n",
    "    dummy_input = torch.randn(1, *input_size).to(device_summary)\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(dummy_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faed930",
   "metadata": {},
   "source": [
    "## üß† 4. Model & Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3cadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic RNN classifier (RNN, LSTM, GRU).\n",
    "    Uses the last hidden state for classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "\n",
    "        # Dropout is only applied between layers (if num_layers > 1)\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        if self.bidirectional:\n",
    "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
    "        else:\n",
    "            classifier_input_size = hidden_size\n",
    "\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" x shape: (batch_size, seq_length, input_size) \"\"\"\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0] # Use only the hidden state, not the cell state\n",
    "\n",
    "        # Get the last layer's hidden state\n",
    "        if self.bidirectional:\n",
    "            # Reshape to (num_layers, num_directions, batch, hidden_size)\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "            # Concat the last fwd and bwd hidden states\n",
    "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
    "        else:\n",
    "            # Just take the last layer's hidden state\n",
    "            hidden_to_classify = hidden[-1]\n",
    "\n",
    "        logits = self.classifier(hidden_to_classify)\n",
    "        return logits\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            \n",
    "            # Add L1/L2 regularization if provided\n",
    "            if l1_lambda > 0 or l2_lambda > 0:\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "                loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Unscale gradients before clipping\n",
    "        scaler.unscale_(optimizer) \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5) \n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Warning: NaN loss detected in batch {batch_idx}. Skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    if not all_targets:\n",
    "        return 0.0, 0.0 # Return 0 if all batches were nan\n",
    "\n",
    "    epoch_loss = running_loss / len(np.concatenate(all_targets))\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset.tensors[1])\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
    "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "\n",
    "def objective_function_cv(config, X_full_engineered, y_full, class_weights_tensor):\n",
    "    \"\"\"\n",
    "    Robust Objective: Runs 3-Fold CV for EVERY trial.\n",
    "    Reports the AVERAGE validation F1 across folds to Ray Tune.\n",
    "    \"\"\"\n",
    "    # Define CV strategy inside the trial (e.g., 3-Fold is usually enough for HPO)\n",
    "    N_HPO_FOLDS = 3 \n",
    "    skf = StratifiedKFold(n_splits=N_HPO_FOLDS, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # Prepare data loaders for all folds UP FRONT to save time in the loop\n",
    "    fold_loaders = []\n",
    "    \n",
    "    # We need to scale inside the folds to avoid leakage, just like in the final loop\n",
    "    for train_idx, val_idx in skf.split(X_full_engineered, y_full):\n",
    "        \n",
    "        # 1. Split\n",
    "        X_train_fold = X_full_engineered[train_idx]\n",
    "        y_train_fold = y_full[train_idx]\n",
    "        X_val_fold = X_full_engineered[val_idx]\n",
    "        y_val_fold = y_full[val_idx]\n",
    "        \n",
    "        # 2. Selective Scale (Fit on Train, Transform Train & Val)\n",
    "        scaler_fold = StandardScaler()\n",
    "        \n",
    "        # Separate continuous vs binary\n",
    "        X_train_cont = X_train_fold[:, :, :35]\n",
    "        X_train_bin = X_train_fold[:, :, 35:]\n",
    "        X_val_cont = X_val_fold[:, :, :35]\n",
    "        X_val_bin = X_val_fold[:, :, 35:]\n",
    "        \n",
    "        # Fit/Transform\n",
    "        ns, ts, f_cont = X_train_cont.shape\n",
    "        X_train_cont_2d = X_train_cont.reshape(ns * ts, f_cont)\n",
    "        scaler_fold.fit(X_train_cont_2d)\n",
    "        \n",
    "        X_train_scaled_cont = scaler_fold.transform(X_train_cont_2d).reshape(ns, ts, f_cont)\n",
    "        X_val_scaled_cont = scaler_fold.transform(X_val_cont.reshape(-1, f_cont)).reshape(-1, ts, f_cont)\n",
    "        \n",
    "        # Re-concat\n",
    "        X_train_final = np.concatenate([X_train_scaled_cont, X_train_bin], axis=2)\n",
    "        X_val_final = np.concatenate([X_val_scaled_cont, X_val_bin], axis=2)\n",
    "        \n",
    "        # 3. Windowing\n",
    "        X_train_w, y_train_w, _ = create_sliding_windows(\n",
    "            X_train_final, y_train_fold, config[\"window_size\"], config[\"stride\"]\n",
    "        )\n",
    "        X_val_w, y_val_w, _ = create_sliding_windows(\n",
    "            X_val_final, y_val_fold, config[\"window_size\"], config[\"stride\"]\n",
    "        )\n",
    "        \n",
    "        # 4. Loaders\n",
    "        train_ds = TensorDataset(torch.from_numpy(X_train_w).float(), torch.from_numpy(y_train_w).long())\n",
    "        val_ds = TensorDataset(torch.from_numpy(X_val_w).float(), torch.from_numpy(y_val_w).long())\n",
    "        \n",
    "        t_loader = make_loader(train_ds, config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "        v_loader = make_loader(val_ds, config[\"batch_size\"], shuffle=False, drop_last=False)\n",
    "        \n",
    "        fold_loaders.append((t_loader, v_loader))\n",
    "\n",
    "    # Initialize K models and K optimizers\n",
    "    models = []\n",
    "    optimizers = []\n",
    "    scalers = []\n",
    "    \n",
    "    for _ in range(N_HPO_FOLDS):\n",
    "        model = RecurrentClassifier(\n",
    "            input_size=36, # Hardcoded for engineered features\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            num_classes=3,\n",
    "            dropout_rate=config[\"dropout_rate\"],\n",
    "            bidirectional=config[\"bidirectional\"],\n",
    "            rnn_type=config[\"rnn_type\"]\n",
    "        ).to(device)\n",
    "        \n",
    "        if torch.__version__[0] >= \"2\": model = torch.compile(model)\n",
    "        \n",
    "        optim = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "        scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "        \n",
    "        models.append(model)\n",
    "        optimizers.append(optim)\n",
    "        scalers.append(scaler)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    EPOCHS = 100 # Reduce epochs slightly since we are doing 3x the work\n",
    "    \n",
    "    # --- The Parallel Training Loop ---\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        \n",
    "        fold_val_f1s = []\n",
    "        fold_train_losses = []\n",
    "        \n",
    "        # Train each fold for 1 epoch\n",
    "        for i in range(N_HPO_FOLDS):\n",
    "            train_loader, val_loader = fold_loaders[i]\n",
    "            model = models[i]\n",
    "            optimizer = optimizers[i]\n",
    "            scaler = scalers[i]\n",
    "            \n",
    "            t_loss, _ = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "            _, v_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "            \n",
    "            fold_train_losses.append(t_loss)\n",
    "            fold_val_f1s.append(v_f1)\n",
    "        \n",
    "        # Calculate AVERAGE metrics across the 3 folds\n",
    "        avg_val_f1 = np.mean(fold_val_f1s)\n",
    "        avg_train_loss = np.mean(fold_train_losses)\n",
    "        \n",
    "        # Report the AVERAGE to Ray Tune\n",
    "        # If the config is bad on *any* fold, the average drops, and ASHA kills it.\n",
    "        tune.report({\n",
    "            \"val_f1\": avg_val_f1,\n",
    "            \"train_loss\": avg_train_loss\n",
    "        })\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Full training loop with early stopping, model checkpointing, and logging.\n",
    "    \"\"\"\n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    print(f\"Will train for {epochs} epochs with patience={patience} monitoring {evaluation_metric}\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
    "            )\n",
    "\n",
    "        if verbose > 0 and (epoch % verbose == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                  f\"Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n",
    "                  f\"Val: Loss={val_loss:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    if restore_best_weights and patience > 0:\n",
    "        print(f\"Restoring best model from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    if patience == 0:\n",
    "        print(\"Training complete. Saving final model.\")\n",
    "        torch.save(model.state_dict(), model_path.replace(\"_best_model.pt\", \"_final_model.pt\"))\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    \n",
    "    print(f\"--- Finished Training: {experiment_name} ---\")\n",
    "    return model, training_history, best_epoch if 'best_epoch' in locals() else epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f911c",
   "metadata": {},
   "source": [
    "## üß™ 5. Phase 1: Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa548a9",
   "metadata": {},
   "source": [
    "### 5.1. Preprocessing for HPO (Selective Scaling)\n",
    "\n",
    "We create a single 80/20 split and apply our selective scaling: scale features 0-34, but not feature 35 (`is_pirate`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09a835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Splitting NON-WINDOWED data for HPO ---\n",
      "  X_train_split_full: (528, 160, 36)\n",
      "  X_val_split_full:   (133, 160, 36)\n",
      "\n",
      "--- Applying Selective Scaling for HPO ---\n",
      "Fitted scaler on continuous training data shape: (84480, 35)\n",
      "  X_train_full_scaled (final): (528, 160, 36)\n",
      "  X_val_full_scaled (final):   (133, 160, 36)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Split Data (NON-WINDOWED) ---\n",
    "print(\"--- Splitting NON-WINDOWED data for HPO ---\")\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "\n",
    "for train_idx, val_idx in sss.split(X_train_full_engineered, y_train_full):\n",
    "    X_train_split_full = X_train_full_engineered[train_idx]\n",
    "    y_train_split_full = y_train_full[train_idx]\n",
    "    X_val_split_full = X_train_full_engineered[val_idx]\n",
    "    y_val_split_full = y_train_full[val_idx]\n",
    "\n",
    "print(f\"  X_train_split_full: {X_train_split_full.shape}\")\n",
    "print(f\"  X_val_split_full:   {X_val_split_full.shape}\")\n",
    "\n",
    "# --- 2. Scale Features (SELECTIVELY) ---\n",
    "print(\"\\n--- Applying Selective Scaling for HPO ---\")\n",
    "scaler_hpo = StandardScaler()\n",
    "\n",
    "# 1. Separate continuous (features 0-34) and binary (feature 35)\n",
    "X_train_cont = X_train_split_full[:, :, :35]\n",
    "X_train_bin = X_train_split_full[:, :, 35:] # The 'is_pirate' feature\n",
    "X_val_cont = X_val_split_full[:, :, :35]\n",
    "X_val_bin = X_val_split_full[:, :, 35:]\n",
    "\n",
    "# 2. Fit scaler ONLY on 2D-reshaped CONTINUOUS training data\n",
    "ns, ts, f_cont = X_train_cont.shape\n",
    "X_train_cont_2d = X_train_cont.reshape(ns * ts, f_cont)\n",
    "scaler_hpo.fit(X_train_cont_2d)\n",
    "print(f\"Fitted scaler on continuous training data shape: {X_train_cont_2d.shape}\")\n",
    "\n",
    "# 3. Transform continuous parts of both train and val\n",
    "X_train_scaled_2d = scaler_hpo.transform(X_train_cont_2d)\n",
    "X_train_scaled_cont = X_train_scaled_2d.reshape(ns, ts, f_cont)\n",
    "\n",
    "ns_val, ts_val, f_val_cont = X_val_cont.shape\n",
    "X_val_cont_2d = X_val_cont.reshape(ns_val * ts_val, f_val_cont)\n",
    "X_val_scaled_2d = scaler_hpo.transform(X_val_cont_2d)\n",
    "X_val_scaled_cont = X_val_scaled_2d.reshape(ns_val, ts_val, f_val_cont)\n",
    "\n",
    "# 4. Re-concatenate with the UNTOUCHED binary feature\n",
    "X_train_full_scaled = np.concatenate([X_train_scaled_cont, X_train_bin], axis=2)\n",
    "X_val_full_scaled = np.concatenate([X_val_scaled_cont, X_val_bin], axis=2)\n",
    "\n",
    "print(f\"  X_train_full_scaled (final): {X_train_full_scaled.shape}\")\n",
    "print(f\"  X_val_full_scaled (final):   {X_val_full_scaled.shape}\")\n",
    "\n",
    "# Verify the binary feature is still 0/1\n",
    "# print(f\"Min/Max of pirate feature in scaled train: {X_train_full_scaled[:, :, 35].min()}, {X_train_full_scaled[:, :, 35].max()}\")\n",
    "\n",
    "# Clean up\n",
    "del X_train_cont, X_train_bin, X_val_cont, X_val_bin\n",
    "del X_train_cont_2d, X_train_scaled_2d, X_val_cont_2d, X_val_scaled_2d\n",
    "del X_train_scaled_cont, X_val_scaled_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526ea0b",
   "metadata": {},
   "source": [
    "### 5.2. HPO Search Execution (Ray Tune + Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42797c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-12 15:50:01</td></tr>\n",
       "<tr><td>Running for: </td><td>01:27:54.21        </td></tr>\n",
       "<tr><td>Memory:      </td><td>8.0/13.9 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=50<br>Bracket: Iter 80.000: 0.9148485162347626 | Iter 40.000: 0.9035349283408719 | Iter 20.000: 0.8840309951788454<br>Logical resource usage: 4.0/16 CPUs, 0.25/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th>bidirectional  </th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  l2_lambda</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th>rnn_type  </th><th style=\"text-align: right;\">  stride</th><th style=\"text-align: right;\">  window_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_f1</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_function_a81fccea</td><td>TERMINATED</td><td>127.0.0.1:26188</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.389684</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.000150741</td><td style=\"text-align: right;\">5.51364e-05</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         20.3301</td><td style=\"text-align: right;\"> 0.0584517  </td><td style=\"text-align: right;\">  0.871666</td><td style=\"text-align: right;\">  0.236291</td></tr>\n",
       "<tr><td>objective_function_a15f428d</td><td>TERMINATED</td><td>127.0.0.1:6444 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.599838</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000441689</td><td style=\"text-align: right;\">0.00101293 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        366.082 </td><td style=\"text-align: right;\"> 1.60095e-05</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.300293</td></tr>\n",
       "<tr><td>objective_function_4b1bc738</td><td>TERMINATED</td><td>127.0.0.1:31576</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.318428</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.52309e-07</td><td style=\"text-align: right;\">5.13627e-05</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        376.06  </td><td style=\"text-align: right;\"> 0.00438218 </td><td style=\"text-align: right;\">  0.998521</td><td style=\"text-align: right;\">  0.477483</td></tr>\n",
       "<tr><td>objective_function_38242623</td><td>TERMINATED</td><td>127.0.0.1:19052</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.312721</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.54554e-07</td><td style=\"text-align: right;\">0.000977984</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         66.4265</td><td style=\"text-align: right;\"> 0.00358984 </td><td style=\"text-align: right;\">  0.992023</td><td style=\"text-align: right;\">  0.299066</td></tr>\n",
       "<tr><td>objective_function_7dbe0c92</td><td>TERMINATED</td><td>127.0.0.1:12636</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.226078</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">2.62517e-05</td><td style=\"text-align: right;\">1.0298e-05 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         27.5393</td><td style=\"text-align: right;\"> 0.551215   </td><td style=\"text-align: right;\">  0.735775</td><td style=\"text-align: right;\">  0.565655</td></tr>\n",
       "<tr><td>objective_function_fff3a8cf</td><td>TERMINATED</td><td>127.0.0.1:8348 </td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.101754</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.24821e-07</td><td style=\"text-align: right;\">1.92944e-05</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         19.8883</td><td style=\"text-align: right;\"> 0.275991   </td><td style=\"text-align: right;\">  0.714153</td><td style=\"text-align: right;\">  0.309728</td></tr>\n",
       "<tr><td>objective_function_3d561f97</td><td>TERMINATED</td><td>127.0.0.1:6920 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.18108 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000130863</td><td style=\"text-align: right;\">0.0039916  </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        112.006 </td><td style=\"text-align: right;\"> 6.07378e-06</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.509429</td></tr>\n",
       "<tr><td>objective_function_a5220080</td><td>TERMINATED</td><td>127.0.0.1:15936</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.172465</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.61589e-07</td><td style=\"text-align: right;\">0.000111708</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         11.6405</td><td style=\"text-align: right;\"> 0.278291   </td><td style=\"text-align: right;\">  0.74776 </td><td style=\"text-align: right;\">  0.355103</td></tr>\n",
       "<tr><td>objective_function_f4adbcc3</td><td>TERMINATED</td><td>127.0.0.1:31896</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.325689</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">3.01678e-06</td><td style=\"text-align: right;\">1.42631e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        336.235 </td><td style=\"text-align: right;\"> 0.0595018  </td><td style=\"text-align: right;\">  0.922545</td><td style=\"text-align: right;\">  0.270003</td></tr>\n",
       "<tr><td>objective_function_4c27ad08</td><td>TERMINATED</td><td>127.0.0.1:32592</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.593208</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">2.64184e-06</td><td style=\"text-align: right;\">9.28073e-05</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         16.6816</td><td style=\"text-align: right;\"> 0.165329   </td><td style=\"text-align: right;\">  0.787004</td><td style=\"text-align: right;\">  0.223046</td></tr>\n",
       "<tr><td>objective_function_ca411669</td><td>TERMINATED</td><td>127.0.0.1:32416</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.326367</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.45248e-07</td><td style=\"text-align: right;\">0.00179384 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         61.6491</td><td style=\"text-align: right;\"> 0.0637583  </td><td style=\"text-align: right;\">  0.940142</td><td style=\"text-align: right;\">  0.23993 </td></tr>\n",
       "<tr><td>objective_function_6def39a6</td><td>TERMINATED</td><td>127.0.0.1:1304 </td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.403421</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">5.44866e-06</td><td style=\"text-align: right;\">0.00103532 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        236.679 </td><td style=\"text-align: right;\"> 7.7288e-06 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.481636</td></tr>\n",
       "<tr><td>objective_function_c30bdfa6</td><td>TERMINATED</td><td>127.0.0.1:4380 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.360144</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.00055839 </td><td style=\"text-align: right;\">0.0010622  </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        708.957 </td><td style=\"text-align: right;\"> 8.75902e-07</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.537969</td></tr>\n",
       "<tr><td>objective_function_fb8c4398</td><td>TERMINATED</td><td>127.0.0.1:27484</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.130899</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.8511e-06 </td><td style=\"text-align: right;\">0.000139358</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         44.0655</td><td style=\"text-align: right;\"> 0.0617977  </td><td style=\"text-align: right;\">  0.872056</td><td style=\"text-align: right;\">  0.227838</td></tr>\n",
       "<tr><td>objective_function_8bdb0fd8</td><td>TERMINATED</td><td>127.0.0.1:2128 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.538536</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000708753</td><td style=\"text-align: right;\">0.000481221</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       1085.53  </td><td style=\"text-align: right;\"> 5.95331e-07</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.388882</td></tr>\n",
       "<tr><td>objective_function_9c740747</td><td>TERMINATED</td><td>127.0.0.1:28860</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.593377</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.00082663 </td><td style=\"text-align: right;\">0.00062881 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        388.64  </td><td style=\"text-align: right;\"> 0.000921621</td><td style=\"text-align: right;\">  0.998853</td><td style=\"text-align: right;\">  0.383589</td></tr>\n",
       "<tr><td>objective_function_58a86180</td><td>TERMINATED</td><td>127.0.0.1:7012 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.599897</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.00091414 </td><td style=\"text-align: right;\">0.000595473</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        209.075 </td><td style=\"text-align: right;\"> 0.0183824  </td><td style=\"text-align: right;\">  0.985562</td><td style=\"text-align: right;\">  0.307178</td></tr>\n",
       "<tr><td>objective_function_a8cd9de7</td><td>TERMINATED</td><td>127.0.0.1:30120</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.589622</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000986832</td><td style=\"text-align: right;\">0.000433912</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        451.97  </td><td style=\"text-align: right;\"> 1.39423e-05</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.481053</td></tr>\n",
       "<tr><td>objective_function_67fb3216</td><td>TERMINATED</td><td>127.0.0.1:31780</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.490422</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">7.87068e-07</td><td style=\"text-align: right;\">0.000391319</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         72.2191</td><td style=\"text-align: right;\"> 0.017552   </td><td style=\"text-align: right;\">  0.981786</td><td style=\"text-align: right;\">  0.319838</td></tr>\n",
       "<tr><td>objective_function_b071e7f0</td><td>TERMINATED</td><td>127.0.0.1:24964</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.488713</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">5.79176e-07</td><td style=\"text-align: right;\">0.000338702</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         38.5159</td><td style=\"text-align: right;\"> 0.0397545  </td><td style=\"text-align: right;\">  0.940304</td><td style=\"text-align: right;\">  0.275829</td></tr>\n",
       "<tr><td>objective_function_3c895433</td><td>TERMINATED</td><td>127.0.0.1:30276</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.46704 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.02388e-05</td><td style=\"text-align: right;\">0.00232158 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         23.3048</td><td style=\"text-align: right;\"> 0.277522   </td><td style=\"text-align: right;\">  0.757   </td><td style=\"text-align: right;\">  0.329729</td></tr>\n",
       "<tr><td>objective_function_674128fb</td><td>TERMINATED</td><td>127.0.0.1:10980</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.259372</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">3.22225e-05</td><td style=\"text-align: right;\">0.00229794 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         48.4009</td><td style=\"text-align: right;\"> 0.0883737  </td><td style=\"text-align: right;\">  0.904604</td><td style=\"text-align: right;\">  0.296686</td></tr>\n",
       "<tr><td>objective_function_7d6db377</td><td>TERMINATED</td><td>127.0.0.1:32256</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.263806</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">7.95794e-05</td><td style=\"text-align: right;\">0.00195108 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        188.957 </td><td style=\"text-align: right;\"> 0.0167359  </td><td style=\"text-align: right;\">  0.981156</td><td style=\"text-align: right;\">  0.321643</td></tr>\n",
       "<tr><td>objective_function_27149a8b</td><td>TERMINATED</td><td>127.0.0.1:29116</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.258928</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">8.33378e-05</td><td style=\"text-align: right;\">0.0031386  </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        108.765 </td><td style=\"text-align: right;\"> 0.0136677  </td><td style=\"text-align: right;\">  0.983443</td><td style=\"text-align: right;\">  0.262747</td></tr>\n",
       "<tr><td>objective_function_769a1eea</td><td>TERMINATED</td><td>127.0.0.1:31552</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.268793</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">8.81849e-05</td><td style=\"text-align: right;\">0.00480648 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         15.2647</td><td style=\"text-align: right;\"> 0.0729701  </td><td style=\"text-align: right;\">  0.869975</td><td style=\"text-align: right;\">  0.198941</td></tr>\n",
       "<tr><td>objective_function_efa8b494</td><td>TERMINATED</td><td>127.0.0.1:32084</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.534803</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000457434</td><td style=\"text-align: right;\">0.00110048 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        355.787 </td><td style=\"text-align: right;\"> 0.0122785  </td><td style=\"text-align: right;\">  0.984423</td><td style=\"text-align: right;\">  0.239039</td></tr>\n",
       "<tr><td>objective_function_e0599cb6</td><td>TERMINATED</td><td>127.0.0.1:27404</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.538789</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000343089</td><td style=\"text-align: right;\">0.000923795</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        369.419 </td><td style=\"text-align: right;\"> 0.0108597  </td><td style=\"text-align: right;\">  0.984636</td><td style=\"text-align: right;\">  0.307889</td></tr>\n",
       "<tr><td>objective_function_05463bda</td><td>TERMINATED</td><td>127.0.0.1:16840</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.546977</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000345053</td><td style=\"text-align: right;\">0.000282545</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        216.747 </td><td style=\"text-align: right;\"> 0.00920793 </td><td style=\"text-align: right;\">  0.987034</td><td style=\"text-align: right;\">  0.461658</td></tr>\n",
       "<tr><td>objective_function_d55e3105</td><td>TERMINATED</td><td>127.0.0.1:30384</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.534494</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000366364</td><td style=\"text-align: right;\">0.000230424</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        377.898 </td><td style=\"text-align: right;\"> 0.00617188 </td><td style=\"text-align: right;\">  0.995009</td><td style=\"text-align: right;\">  0.455626</td></tr>\n",
       "<tr><td>objective_function_e62048cf</td><td>TERMINATED</td><td>127.0.0.1:4576 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.52645 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000277423</td><td style=\"text-align: right;\">0.000863977</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       1198.78  </td><td style=\"text-align: right;\"> 5.16867e-07</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.384089</td></tr>\n",
       "<tr><td>objective_function_f4104abd</td><td>TERMINATED</td><td>127.0.0.1:8164 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.532897</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000281686</td><td style=\"text-align: right;\">0.00118431 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       1202.49  </td><td style=\"text-align: right;\"> 4.38244e-07</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.437715</td></tr>\n",
       "<tr><td>objective_function_802c6ac1</td><td>TERMINATED</td><td>127.0.0.1:30120</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.438178</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000257218</td><td style=\"text-align: right;\">0.000170849</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       1206.34  </td><td style=\"text-align: right;\"> 1.81665e-06</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.567817</td></tr>\n",
       "<tr><td>objective_function_fb9f5be5</td><td>TERMINATED</td><td>127.0.0.1:12960</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.437235</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000213714</td><td style=\"text-align: right;\">0.000238962</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        648.737 </td><td style=\"text-align: right;\"> 8.22667e-06</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.452843</td></tr>\n",
       "<tr><td>objective_function_06bd8628</td><td>TERMINATED</td><td>127.0.0.1:32220</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.418825</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">9.17042e-06</td><td style=\"text-align: right;\">0.000589964</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         67.5983</td><td style=\"text-align: right;\"> 0.016597   </td><td style=\"text-align: right;\">  0.981126</td><td style=\"text-align: right;\">  0.510361</td></tr>\n",
       "<tr><td>objective_function_80f9a957</td><td>TERMINATED</td><td>127.0.0.1:16848</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.387136</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.46632e-05</td><td style=\"text-align: right;\">0.000595574</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         24.7038</td><td style=\"text-align: right;\"> 0.0174281  </td><td style=\"text-align: right;\">  0.972853</td><td style=\"text-align: right;\">  0.31894 </td></tr>\n",
       "<tr><td>objective_function_9976e743</td><td>TERMINATED</td><td>127.0.0.1:9276 </td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.421548</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000196049</td><td style=\"text-align: right;\">0.000176757</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        121.454 </td><td style=\"text-align: right;\"> 0.00783828 </td><td style=\"text-align: right;\">  0.988254</td><td style=\"text-align: right;\">  0.505792</td></tr>\n",
       "<tr><td>objective_function_5326416f</td><td>TERMINATED</td><td>127.0.0.1:29680</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.472589</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000200511</td><td style=\"text-align: right;\">0.000171626</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        231.654 </td><td style=\"text-align: right;\"> 0.00475629 </td><td style=\"text-align: right;\">  0.994007</td><td style=\"text-align: right;\">  0.427239</td></tr>\n",
       "<tr><td>objective_function_69e5ad98</td><td>TERMINATED</td><td>127.0.0.1:27812</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.563534</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000189387</td><td style=\"text-align: right;\">0.00140228 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       1230.8   </td><td style=\"text-align: right;\"> 6.89366e-07</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.343903</td></tr>\n",
       "<tr><td>objective_function_cb07a7fa</td><td>TERMINATED</td><td>127.0.0.1:29788</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.556139</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">4.19049e-05</td><td style=\"text-align: right;\">0.00136554 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       1237.03  </td><td style=\"text-align: right;\"> 5.82894e-07</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.377509</td></tr>\n",
       "<tr><td>objective_function_ac1fe972</td><td>TERMINATED</td><td>127.0.0.1:6360 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.563507</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">3.61364e-05</td><td style=\"text-align: right;\">0.00140265 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       1243.39  </td><td style=\"text-align: right;\"> 6.72355e-07</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.574781</td></tr>\n",
       "<tr><td>objective_function_97d11cba</td><td>TERMINATED</td><td>127.0.0.1:9380 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.562101</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">4.74144e-05</td><td style=\"text-align: right;\">0.00142037 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">       1061.26  </td><td style=\"text-align: right;\"> 1.42474e-06</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.53037 </td></tr>\n",
       "<tr><td>objective_function_4fa1f551</td><td>TERMINATED</td><td>127.0.0.1:31268</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.557572</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">5.05583e-05</td><td style=\"text-align: right;\">0.00127338 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        875.967 </td><td style=\"text-align: right;\"> 1.41456e-06</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.331173</td></tr>\n",
       "<tr><td>objective_function_eff52db8</td><td>TERMINATED</td><td>127.0.0.1:11056</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.509678</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.000132226</td><td style=\"text-align: right;\">3.35977e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         55.4991</td><td style=\"text-align: right;\"> 0.0659309  </td><td style=\"text-align: right;\">  0.885843</td><td style=\"text-align: right;\">  0.193006</td></tr>\n",
       "<tr><td>objective_function_addc223f</td><td>TERMINATED</td><td>127.0.0.1:32636</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.509813</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.000584542</td><td style=\"text-align: right;\">0.000768137</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        286.847 </td><td style=\"text-align: right;\"> 1.11879e-06</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.399586</td></tr>\n",
       "<tr><td>objective_function_afaa9a2e</td><td>TERMINATED</td><td>127.0.0.1:23868</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.569987</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.000146393</td><td style=\"text-align: right;\">4.25114e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         59.186 </td><td style=\"text-align: right;\"> 0.0656827  </td><td style=\"text-align: right;\">  0.906426</td><td style=\"text-align: right;\">  0.227731</td></tr>\n",
       "<tr><td>objective_function_d75b5c1b</td><td>TERMINATED</td><td>127.0.0.1:19128</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.519361</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.000135828</td><td style=\"text-align: right;\">0.000840616</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        301.287 </td><td style=\"text-align: right;\"> 1.25206e-06</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.399773</td></tr>\n",
       "<tr><td>objective_function_44039a8c</td><td>TERMINATED</td><td>127.0.0.1:4204 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.509045</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000582689</td><td style=\"text-align: right;\">0.000754083</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        953.365 </td><td style=\"text-align: right;\"> 5.36135e-07</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.420991</td></tr>\n",
       "<tr><td>objective_function_d2caf419</td><td>TERMINATED</td><td>127.0.0.1:26800</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.51414 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000623536</td><td style=\"text-align: right;\">0.000742611</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        740.403 </td><td style=\"text-align: right;\"> 2.69993e-06</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.346814</td></tr>\n",
       "<tr><td>objective_function_c4d7c26c</td><td>TERMINATED</td><td>127.0.0.1:6072 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.508022</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.000562885</td><td style=\"text-align: right;\">0.000756621</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        117.041 </td><td style=\"text-align: right;\"> 0.0338571  </td><td style=\"text-align: right;\">  0.980944</td><td style=\"text-align: right;\">  0.400566</td></tr>\n",
       "<tr><td>objective_function_f002eaa6</td><td>TERMINATED</td><td>127.0.0.1:32088</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.512659</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.000705064</td><td style=\"text-align: right;\">0.000760279</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        290.826 </td><td style=\"text-align: right;\"> 7.55823e-06</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.25374 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 15:50:01,581\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'c:/Users/Karim Negm/Documents/AN2DL/Challenge 1/ray_results/pirate_pain_optuna_search' in 0.0743s.\n",
      "2025-11-12 15:50:01,619\tINFO tune.py:1041 -- Total run time: 5274.29 seconds (5274.12 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Search Complete ---\n",
      "\n",
      "Getting best trial from analysis...\n",
      "Best validation F1 score: 0.9377\n",
      "Best hyperparameters found:\n",
      "{'window_size': 80, 'stride': 10, 'rnn_type': 'GRU', 'lr': 0.0004812212932637246, 'batch_size': 64, 'hidden_size': 384, 'num_layers': 3, 'dropout_rate': 0.5385357600632669, 'bidirectional': True, 'l2_lambda': 0.0007087527682059368}\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define the Search Space for Optuna --\n",
    "search_space = {\n",
    "    # Windowing params\n",
    "    \"window_size\": tune.choice([10, 20, 30]),\n",
    "    \"stride\": tune.choice([1, 2, 5]),\n",
    "    \n",
    "    # Model params\n",
    "    \"rnn_type\": tune.choice(['GRU']),\n",
    "    \"lr\": tune.loguniform(1e-5, 5e-3),\n",
    "    \"batch_size\": tune.choice([64, 128, 256]),  \n",
    "    \"hidden_size\": tune.choice([128, 256, 384]),\n",
    "    \"num_layers\": tune.choice([2, 3]),\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.6),\n",
    "    \"bidirectional\": tune.choice([True, False]),\n",
    "    \"l2_lambda\": tune.loguniform(1e-7, 1e-3) # This is weight_decay in AdamW\n",
    "}\n",
    "# --- 2. Define the Optimizer (Optuna) and Scheduler (ASHA) ---\n",
    "optuna_search = OptunaSearch(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\",\n",
    "    grace_period=20,  # Min epochs a trial must run\n",
    "    reduction_factor=2  # How aggressively to stop trials\n",
    ")\n",
    "\n",
    "# --- 3. Initialize Ray ---\n",
    "# Shutdown previous sessions if any (helps in notebooks)\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "ray_logs_path = os.path.abspath(\"./ray_results\")\n",
    "os.makedirs(ray_logs_path, exist_ok=True)\n",
    "os.environ[\"RAY_TEMP_DIR\"] = ray_logs_path\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=16, \n",
    "    num_gpus=1, \n",
    "    ignore_reinit_error=True,\n",
    "    log_to_driver=False # Suppress logs in notebook\n",
    ")\n",
    "\n",
    "def short_trial_name(trial):\n",
    "    \"\"\"Creates a short, unique name for each trial folder.\"\"\"\n",
    "    return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "\n",
    "# --- 4. Run the Tuner --\n",
    "print(\"Starting hyperparameter search...\")\n",
    "\n",
    "# Use tune.with_parameters to pass our NON-WINDOWED, SCALED numpy arrays\n",
    "# and the class weights to the objective function\n",
    "objective_with_data = tune.with_parameters(\n",
    "    objective_function_cv, # Use the new function\n",
    "    X_full_engineered=X_train_full_engineered, # The full (661, 160, 36) array\n",
    "    y_full=y_train_full,                       # The full labels\n",
    "    class_weights_tensor=class_weights_tensor\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    objective_with_data,\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 0.5}, # Increased GPU usage per trial slightly\n",
    "    config=search_space,\n",
    "    num_samples=20, # Reduce samples if time is an issue\n",
    "    search_alg=optuna_search,\n",
    "    scheduler=scheduler,\n",
    "    name=\"pirate_pain_robust_cv_search\",\n",
    "    trial_dirname_creator=short_trial_name,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n--- Search Complete ---\\n\")\n",
    "\n",
    "# --- 5. Get Best Results ---\n",
    "print(\"Getting best trial from analysis...\")\n",
    "best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "if best_trial:\n",
    "    FINAL_CONFIG = best_trial.config\n",
    "    FINAL_BEST_VAL_F1 = best_trial.last_result[\"val_f1\"]\n",
    "    \n",
    "    print(f\"Best validation F1 score: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(FINAL_CONFIG)\n",
    "else:\n",
    "    print(\"ERROR: No trials completed successfully. Using a default config.\")\n",
    "    # Fallback config in case HPO fails\n",
    "    FINAL_CONFIG = {\n",
    "        'window_size': 100, 'stride': 10, 'rnn_type': 'GRU', 'lr': 0.0005,\n",
    "        'batch_size': 64, 'hidden_size': 256, 'num_layers': 3,\n",
    "        'dropout_rate': 0.5, 'bidirectional': True, 'l2_lambda': 1e-06\n",
    "    }\n",
    "    FINAL_BEST_VAL_F1 = 0.0\n",
    "\n",
    "# Clean up HPO data\n",
    "del X_train_full_scaled, y_train_split_full, X_val_full_scaled, y_val_split_full, scaler_hpo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c0bb8",
   "metadata": {},
   "source": [
    "## üèÜ 6. Phase 2: K-Fold Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13308e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèÜ Final Configuration Set --- \n",
      "Best Val F1 from HPO search: 0.9377\n",
      "{'window_size': 80, 'stride': 10, 'rnn_type': 'GRU', 'lr': 0.0004812212932637246, 'batch_size': 64, 'hidden_size': 384, 'num_layers': 3, 'dropout_rate': 0.5385357600632669, 'bidirectional': True, 'l2_lambda': 0.0007087527682059368}\n",
      "Submission name will be: submission_GRU_H384_L3_BTrue_Optuna_KFold_Ensemble_w80_s10.csv\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# --- üèÜ FINAL MODEL CONFIGURATION üèÜ ---\n",
    "# ===================================================================\n",
    "print(\"--- üèÜ Final Configuration Set --- \")\n",
    "print(f\"Best Val F1 from HPO search: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "print(FINAL_CONFIG)\n",
    "\n",
    "# --- Set variables for the K-Fold & submission cells ---\n",
    "FINAL_MODEL_TYPE = FINAL_CONFIG[\"rnn_type\"]\n",
    "FINAL_HIDDEN_SIZE = FINAL_CONFIG[\"hidden_size\"]\n",
    "FINAL_HIDDEN_LAYERS = FINAL_CONFIG[\"num_layers\"]\n",
    "FINAL_BIDIRECTIONAL = FINAL_CONFIG[\"bidirectional\"]\n",
    "FINAL_DROPOUT_RATE = FINAL_CONFIG[\"dropout_rate\"]\n",
    "FINAL_LEARNING_RATE = FINAL_CONFIG[\"lr\"]\n",
    "FINAL_L2_LAMBDA = FINAL_CONFIG[\"l2_lambda\"]\n",
    "FINAL_BATCH_SIZE = FINAL_CONFIG[\"batch_size\"]\n",
    "FINAL_WINDOW_SIZE = FINAL_CONFIG[\"window_size\"]\n",
    "FINAL_STRIDE = FINAL_CONFIG[\"stride\"]\n",
    "N_SPLITS = 5 # Number of folds\n",
    "\n",
    "FINAL_EXPERIMENT_NAME = f\"{FINAL_MODEL_TYPE}_H{FINAL_HIDDEN_SIZE}_L{FINAL_HIDDEN_LAYERS}_B{FINAL_BIDIRECTIONAL}_Optuna_KFold_Ensemble\"\n",
    "submission_filename_base = f\"submission_{FINAL_EXPERIMENT_NAME}_w{FINAL_WINDOW_SIZE}_s{FINAL_STRIDE}.csv\"\n",
    "print(f\"Submission name will be: {submission_filename_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bda55dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting 5-Fold CV Training ---\n",
      "Splitting original engineered data: (661, 160, 36)\n",
      "Using Class Weights: [0.06426252 0.34934196 0.5863955 ]\n",
      "\n",
      "--- Fold 1/5 --- (kfold_fold_1) ---\n",
      "  Fold Train Windows: (4752, 80, 36), Fold Val Windows: (1197, 80, 36)\n",
      "--- Starting Training: kfold_fold_1 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.2969, F1=0.7717 | Val: Loss=0.3028, F1=0.8111\n",
      "Epoch  25/300 | Train: Loss=0.0045, F1=0.9968 | Val: Loss=0.2908, F1=0.9147\n",
      "\n",
      "Early stopping triggered after 48 epochs.\n",
      "Restoring best model from epoch 18 with val_f1 0.9205\n",
      "--- Finished Training: kfold_fold_1 ---\n",
      "Fold 1 Best Model Val F1: 0.9205\n",
      "\n",
      "--- Fold 2/5 --- (kfold_fold_2) ---\n",
      "  Fold Train Windows: (4761, 80, 36), Fold Val Windows: (1188, 80, 36)\n",
      "--- Starting Training: kfold_fold_2 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.3057, F1=0.7609 | Val: Loss=0.2484, F1=0.8271\n",
      "Epoch  25/300 | Train: Loss=0.0067, F1=0.9927 | Val: Loss=0.4726, F1=0.9050\n",
      "\n",
      "Early stopping triggered after 43 epochs.\n",
      "Restoring best model from epoch 13 with val_f1 0.9292\n",
      "--- Finished Training: kfold_fold_2 ---\n",
      "Fold 2 Best Model Val F1: 0.9292\n",
      "\n",
      "--- Fold 3/5 --- (kfold_fold_3) ---\n",
      "  Fold Train Windows: (4761, 80, 36), Fold Val Windows: (1188, 80, 36)\n",
      "--- Starting Training: kfold_fold_3 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.2984, F1=0.7680 | Val: Loss=0.2881, F1=0.8282\n",
      "Epoch  25/300 | Train: Loss=0.0005, F1=0.9989 | Val: Loss=0.4971, F1=0.9275\n",
      "Epoch  50/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.3810, F1=0.9386\n",
      "\n",
      "Early stopping triggered after 63 epochs.\n",
      "Restoring best model from epoch 33 with val_f1 0.9458\n",
      "--- Finished Training: kfold_fold_3 ---\n",
      "Fold 3 Best Model Val F1: 0.9458\n",
      "\n",
      "--- Fold 4/5 --- (kfold_fold_4) ---\n",
      "  Fold Train Windows: (4761, 80, 36), Fold Val Windows: (1188, 80, 36)\n",
      "--- Starting Training: kfold_fold_4 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.2964, F1=0.7811 | Val: Loss=0.2892, F1=0.8057\n",
      "Epoch  25/300 | Train: Loss=0.0029, F1=0.9944 | Val: Loss=0.3085, F1=0.8887\n",
      "Epoch  50/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.3413, F1=0.9112\n",
      "\n",
      "Early stopping triggered after 67 epochs.\n",
      "Restoring best model from epoch 37 with val_f1 0.9167\n",
      "--- Finished Training: kfold_fold_4 ---\n",
      "Fold 4 Best Model Val F1: 0.9167\n",
      "\n",
      "--- Fold 5/5 --- (kfold_fold_5) ---\n",
      "  Fold Train Windows: (4761, 80, 36), Fold Val Windows: (1188, 80, 36)\n",
      "--- Starting Training: kfold_fold_5 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.3055, F1=0.7638 | Val: Loss=0.1645, F1=0.8619\n",
      "Epoch  25/300 | Train: Loss=0.0047, F1=0.9934 | Val: Loss=0.3583, F1=0.9147\n",
      "Epoch  50/300 | Train: Loss=0.0025, F1=0.9970 | Val: Loss=0.2901, F1=0.9182\n",
      "Epoch  75/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.2173, F1=0.9320\n",
      "\n",
      "Early stopping triggered after 86 epochs.\n",
      "Restoring best model from epoch 56 with val_f1 0.9372\n",
      "--- Finished Training: kfold_fold_5 ---\n",
      "Fold 5 Best Model Val F1: 0.9372\n",
      "\n",
      "--- üèÜ K-Fold Training Complete ---\n",
      "Fold F1 scores: [0.9205, 0.9292, 0.9458, 0.9167, 0.9372]\n",
      "Average F1 across folds: 0.9299\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED) \n",
    "print(f\"--- Starting {N_SPLITS}-Fold CV Training ---\")\n",
    "print(f\"Splitting original engineered data: {X_train_full_engineered.shape}\")\n",
    "print(f\"Using Class Weights: {class_weights_tensor.cpu().numpy()}\")\n",
    "\n",
    "fold_val_f1_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_full_engineered, y_train_full)):\n",
    "    fold_name = f\"kfold_fold_{fold+1}\"\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} --- ({fold_name}) ---\")\n",
    "    \n",
    "    X_train_fold_full = X_train_full_engineered[train_idx]\n",
    "    y_train_fold_full = y_train_full[train_idx]\n",
    "    X_val_fold_full = X_train_full_engineered[val_idx]\n",
    "    y_val_fold_full = y_train_full[val_idx]\n",
    "\n",
    "    # --- Scale INSIDE the fold (SELECTIVELY) ---\n",
    "    scaler_fold = StandardScaler()\n",
    "    \n",
    "    # 1. Separate continuous (features 0-34) and binary (feature 35)\n",
    "    X_train_cont = X_train_fold_full[:, :, :35]\n",
    "    X_train_bin = X_train_fold_full[:, :, 35:] # The 'is_pirate' feature\n",
    "    X_val_cont = X_val_fold_full[:, :, :35]\n",
    "    X_val_bin = X_val_fold_full[:, :, 35:]\n",
    "\n",
    "    # 2. Fit scaler ONLY on 2D-reshaped CONTINUOUS training data\n",
    "    ns, ts, f_cont = X_train_cont.shape\n",
    "    X_train_cont_2d = X_train_cont.reshape(ns * ts, f_cont)\n",
    "    scaler_fold.fit(X_train_cont_2d)\n",
    "\n",
    "    # 3. Transform continuous parts of both train and val\n",
    "    X_train_scaled_2d = scaler_fold.transform(X_train_cont_2d)\n",
    "    X_train_fold_scaled_cont = X_train_scaled_2d.reshape(ns, ts, f_cont)\n",
    "\n",
    "    ns_val, ts_val, f_val_cont = X_val_cont.shape\n",
    "    X_val_cont_2d = X_val_cont.reshape(ns_val * ts_val, f_val_cont)\n",
    "    X_val_scaled_2d = scaler_fold.transform(X_val_cont_2d)\n",
    "    X_val_fold_scaled_cont = X_val_scaled_2d.reshape(ns_val, ts_val, f_val_cont)\n",
    "\n",
    "    # 4. Re-concatenate with the UNTOUCHED binary feature\n",
    "    X_train_fold_scaled = np.concatenate([X_train_fold_scaled_cont, X_train_bin], axis=2)\n",
    "    X_val_fold_scaled = np.concatenate([X_val_fold_scaled_cont, X_val_bin], axis=2)\n",
    "\n",
    "    # --- Create Sliding Windows (POST-SPLIT) ---\n",
    "    X_train_w, y_train_w, _ = create_sliding_windows(\n",
    "        X_train_fold_scaled, y_train_fold_full, \n",
    "        window_size=FINAL_WINDOW_SIZE, stride=FINAL_STRIDE\n",
    "    )\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(\n",
    "        X_val_fold_scaled, y_val_fold_full, \n",
    "        window_size=FINAL_WINDOW_SIZE, stride=FINAL_STRIDE\n",
    "    )\n",
    "    print(f\"  Fold Train Windows: {X_train_w.shape}, Fold Val Windows: {X_val_w.shape}\")\n",
    "\n",
    "    # --- Create Tensors, datasets and dataloaders --\n",
    "    X_train_fold = torch.from_numpy(X_train_w).float()\n",
    "    y_train_fold = torch.from_numpy(y_train_w).long()\n",
    "    X_val_fold = torch.from_numpy(X_val_w).float()\n",
    "    y_val_fold = torch.from_numpy(y_val_w).long()\n",
    "    train_ds_fold = TensorDataset(X_train_fold, y_train_fold)\n",
    "    val_ds_fold = TensorDataset(X_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader_fold = make_loader(train_ds_fold, batch_size=FINAL_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    val_loader_fold = make_loader(val_ds_fold, batch_size=FINAL_BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "    \n",
    "    # --- Create a fresh model (using FINAL_CONFIG) ---\n",
    "    model_fold = RecurrentClassifier(\n",
    "        input_size=N_FEATURES_NEW, # This is 36\n",
    "        hidden_size=FINAL_HIDDEN_SIZE, num_layers=FINAL_HIDDEN_LAYERS,\n",
    "        num_classes=N_CLASSES, dropout_rate=FINAL_DROPOUT_RATE,\n",
    "        bidirectional=FINAL_BIDIRECTIONAL, rnn_type=FINAL_MODEL_TYPE\n",
    "    ).to(device)\n",
    "    \n",
    "    if torch.__version__[0] >= \"2\": model_fold = torch.compile(model_fold)\n",
    "    optimizer_fold = torch.optim.AdamW(model_fold.parameters(), lr=FINAL_LEARNING_RATE, weight_decay=FINAL_L2_LAMBDA)\n",
    "    scaler_fold_amp = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion_fold = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    # --- Train this fold with early stopping ---\n",
    "    model_fold, _, _ = fit(\n",
    "        model=model_fold, train_loader=train_loader_fold,\n",
    "        val_loader=val_loader_fold, epochs=300,\n",
    "        criterion=criterion_fold, optimizer=optimizer_fold,\n",
    "        scaler=scaler_fold_amp, device=device,\n",
    "        writer=None, verbose=25,\n",
    "        experiment_name=fold_name, patience=30\n",
    "    )\n",
    "    \n",
    "    val_loss, val_f1 = validate_one_epoch(model_fold, val_loader_fold, criterion_fold, device)\n",
    "    fold_val_f1_list.append(val_f1)\n",
    "    print(f\"Fold {fold+1} Best Model Val F1: {val_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n--- üèÜ K-Fold Training Complete ---\")\n",
    "print(f\"Fold F1 scores: {[round(f, 4) for f in fold_val_f1_list]}\")\n",
    "print(f\"Average F1 across folds: {np.mean(fold_val_f1_list):.4f}\")\n",
    "\n",
    "# Clean up\n",
    "del X_train_fold, y_train_fold, X_val_fold, y_val_fold\n",
    "del X_train_w, y_train_w, X_val_w, y_val_w\n",
    "del X_train_fold_full, y_train_fold_full, X_val_fold_full, y_val_fold_full\n",
    "del X_train_cont, X_train_bin, X_val_cont, X_val_bin\n",
    "del X_train_cont_2d, X_train_scaled_2d, X_val_cont_2d, X_val_scaled_2d\n",
    "del X_train_fold_scaled_cont, X_val_fold_scaled_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9d380",
   "metadata": {},
   "source": [
    "## üì¨ 7. Phase 3: Ensemble Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b31871e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing full dataset for FINAL SCALER ---\n",
      "Fitted FINAL scaler on continuous training data shape: (105760, 35)\n",
      "\n",
      "--- Preparing Test Set (Selective Scaling) ---\n",
      "Created final scaled test set (shape: (1324, 160, 36))\n",
      "--- Applying sliding windows to final test set ---\n",
      "Test windowed shape: (11916, 80, 36)\n",
      "Final TestLoader created.\n",
      "\n",
      "--- Generating predictions from 5 fold models ---\n",
      "Loading model 1/5 from models/kfold_fold_1_best_model.pt...\n",
      "Loading model 2/5 from models/kfold_fold_2_best_model.pt...\n",
      "Loading model 3/5 from models/kfold_fold_3_best_model.pt...\n",
      "Loading model 4/5 from models/kfold_fold_4_best_model.pt...\n",
      "Loading model 5/5 from models/kfold_fold_5_best_model.pt...\n",
      "\n",
      "--- Averaging 5 sets of probabilities... ---\n",
      "Mean probability matrix shape: (11916, 3)\n",
      "Aggregating window probabilities to sample predictions (using MEAN)...\n",
      "Aggregated to 1324 final probability vectors.\n",
      "Loading sample submission file for correct formatting...\n",
      "Prediction count matches. Creating submission.\n",
      "\n",
      "Successfully saved to submissions\\submission_GRU_H384_L3_BTrue_Optuna_KFold_Ensemble_w80_s10.csv!\n",
      "This file is correctly formatted for Kaggle:\n",
      "  sample_index    label\n",
      "0          000  no_pain\n",
      "1          001  no_pain\n",
      "2          002  no_pain\n",
      "3          003  no_pain\n",
      "4          004  no_pain\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Preparing full dataset for FINAL SCALER ---\")\n",
    "\n",
    "# --- 1. Prepare Final Scaler (Fit on ALL training data) ---\n",
    "scaler_final = StandardScaler()\n",
    "\n",
    "# 1. Separate continuous (features 0-34) and binary (feature 35)\n",
    "X_train_full_cont = X_train_full_engineered[:, :, :35]\n",
    "X_train_full_bin = X_train_full_engineered[:, :, 35:] # The 'is_pirate' feature\n",
    "\n",
    "# 2. Fit scaler ONLY on 2D-reshaped CONTINUOUS training data\n",
    "ns, ts, f_cont = X_train_full_cont.shape\n",
    "X_train_full_cont_2d = X_train_full_cont.reshape(ns * ts, f_cont)\n",
    "scaler_final.fit(X_train_full_cont_2d)\n",
    "print(f\"Fitted FINAL scaler on continuous training data shape: {X_train_full_cont_2d.shape}\")\n",
    "\n",
    "# --- 2. Prepare, Scale (Selectively), and Window the TEST data ---\n",
    "print(\"\\n--- Preparing Test Set (Selective Scaling) ---\")\n",
    "\n",
    "# 1. Separate test data\n",
    "X_test_cont = X_test_full_engineered[:, :, :35]\n",
    "X_test_bin = X_test_full_engineered[:, :, 35:]\n",
    "\n",
    "# 2. Transform continuous part of test data\n",
    "ns_test, ts_test, f_test_cont = X_test_cont.shape\n",
    "X_test_cont_2d = X_test_cont.reshape(ns_test * ts_test, f_test_cont)\n",
    "X_test_scaled_2d = scaler_final.transform(X_test_cont_2d)\n",
    "X_test_scaled_cont = X_test_scaled_2d.reshape(ns_test, ts_test, f_test_cont)\n",
    "\n",
    "# 3. Re-concatenate with the UNTOUCHED binary feature\n",
    "X_test_final_scaled = np.concatenate([X_test_scaled_cont, X_test_bin], axis=2)\n",
    "print(f\"Created final scaled test set (shape: {X_test_final_scaled.shape})\")\n",
    "\n",
    "# --- 3. Apply Sliding Windows ---\n",
    "print(\"--- Applying sliding windows to final test set ---\")\n",
    "X_test_final_windowed, test_window_indices = create_sliding_windows(\n",
    "    X_test_final_scaled, y=None, \n",
    "    window_size=FINAL_WINDOW_SIZE, stride=FINAL_STRIDE\n",
    ")\n",
    "print(f\"Test windowed shape: {X_test_final_windowed.shape}\")\n",
    "\n",
    "# --- 4. Create Final TestLoader ---\n",
    "final_test_features = torch.from_numpy(X_test_final_windowed).float()\n",
    "final_test_ds = TensorDataset(final_test_features)\n",
    "test_loader = make_loader(final_test_ds, batch_size=FINAL_BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "print(\"Final TestLoader created.\")\n",
    "\n",
    "# --- 5. Get Predictions from all K-Fold Models ---\n",
    "all_fold_probabilities = []\n",
    "print(f\"\\n--- Generating predictions from {N_SPLITS} fold models ---\")\n",
    "\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_name = f\"kfold_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    print(f\"Loading model {fold+1}/{N_SPLITS} from {model_path}...\")\n",
    "\n",
    "    # Create a fresh model shell\n",
    "    model_fold = RecurrentClassifier(\n",
    "        input_size=N_FEATURES_NEW, # Use 36\n",
    "        hidden_size=FINAL_HIDDEN_SIZE, num_layers=FINAL_HIDDEN_LAYERS,\n",
    "        num_classes=N_CLASSES, dropout_rate=FINAL_DROPOUT_RATE,\n",
    "        bidirectional=FINAL_BIDIRECTIONAL, rnn_type=FINAL_MODEL_TYPE\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load the saved weights (with compile-fix)\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    # Remove the '_orig_mod.' prefix if model was compiled\n",
    "    new_state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "    model_fold.load_state_dict(new_state_dict)\n",
    "    model_fold.eval()\n",
    "\n",
    "    # Get Softmax probabilities\n",
    "    fold_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in test_loader: \n",
    "            inputs = inputs.to(device)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model_fold(inputs)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                fold_predictions.append(probs.cpu().numpy())\n",
    "    all_fold_probabilities.append(np.concatenate(fold_predictions))\n",
    "\n",
    "# --- 6. Average the Probabilities ---\n",
    "print(f\"\\n--- Averaging {len(all_fold_probabilities)} sets of probabilities... ---\")\n",
    "mean_probabilities = np.mean(all_fold_probabilities, axis=0)\n",
    "print(f\"Mean probability matrix shape: {mean_probabilities.shape}\")\n",
    "\n",
    "# --- 7. Aggregate Mean Probabilities (MEAN) ---\n",
    "print(\"Aggregating window probabilities to sample predictions (using MEAN)...\")\n",
    "prob_cols = [f\"prob_{i}\" for i in range(N_CLASSES)]\n",
    "df_probs = pd.DataFrame(mean_probabilities, columns=prob_cols)\n",
    "df_probs['original_index'] = test_window_indices \n",
    "agg_probs = df_probs.groupby('original_index')[prob_cols].mean().values\n",
    "print(f\"Aggregated to {len(agg_probs)} final probability vectors.\")\n",
    "\n",
    "# --- 8. Get Final Predictions and Save ---\n",
    "final_predictions_numeric = np.argmax(agg_probs, axis=1)\n",
    "predicted_labels = le.inverse_transform(final_predictions_numeric)\n",
    "\n",
    "print(\"Loading sample submission file for correct formatting...\")\n",
    "test_sample_indices = sorted(X_test_long_df['sample_index'].unique())\n",
    "\n",
    "if len(predicted_labels) != len(test_sample_indices):\n",
    "    print(f\"ERROR: Prediction count mismatch!\")\n",
    "else:\n",
    "    print(\"Prediction count matches. Creating submission.\")\n",
    "    final_submission_df = pd.DataFrame({\n",
    "        'sample_index': test_sample_indices,\n",
    "        'label': predicted_labels \n",
    "    })\n",
    "    final_submission_df['sample_index'] = final_submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "\n",
    "    submission_filepath = os.path.join(\"submissions\", submission_filename_base)\n",
    "    final_submission_df.to_csv(submission_filepath, index=False)\n",
    "\n",
    "    print(f\"\\nSuccessfully saved to {submission_filepath}!\")\n",
    "    print(\"This file is correctly formatted for Kaggle:\")\n",
    "    print(final_submission_df.head())\n",
    "\n",
    "# Clean up\n",
    "del all_fold_probabilities, final_test_features, final_test_ds, test_loader\n",
    "del X_test_full_engineered, X_test_final_scaled, X_test_final_windowed\n",
    "del X_train_full_cont, X_train_full_bin, X_train_full_cont_2d, scaler_final\n",
    "del X_test_cont, X_test_bin, X_test_cont_2d, X_test_scaled_2d, X_test_scaled_cont"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
