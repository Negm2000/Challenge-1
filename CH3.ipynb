{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5222aca",
   "metadata": {
    "id": "markdown-notebook-intro"
   },
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è (v2: K-Fold Ensemble)**\n",
    "\n",
    "This notebook implements a robust K-Fold Cross-Validation and Ensembling strategy to improve on the baseline model. \n",
    "\n",
    "**Strategy:**\n",
    "1.  **Hyperparameter Search:** Use Ray Tune & Optuna on a single 80/20 split to find a good set of hyperparameters (the `FINAL_CONFIG`).\n",
    "2.  **K-Fold Training:** Instead of training one model on 100% of the data, we train `K` (e.g., 5) models on `K` different 80/20 splits (\"folds\"). We use early stopping to find the best model for each fold and save it to disk.\n",
    "3.  **Ensemble Prediction:** To create the final submission, we load all `K` models. We get `K` different predictions for the test set, average their (softmax) probabilities, and then aggregate these averaged probabilities for the final submission. This is far more robust than training a single, \"blind\" model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e25d9",
   "metadata": {
    "id": "markdown-libraries"
   },
   "source": [
    "## ‚öôÔ∏è **1. Setup & Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f67f7de3",
   "metadata": {
    "id": "code-libraries"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GPU (RTX 3070, here we come!) ---\n",
      "PyTorch version: 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Sklearn Imports ---\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Ray[tune] & Optuna Imports ---\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from functools import partial\n",
    "\n",
    "# --- Setup Directories & Device ---\n",
    "logs_dir = \"tensorboard\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU (RTX 3070, here we come!) ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0184001",
   "metadata": {
    "id": "markdown-load-reshape"
   },
   "source": [
    "## üîÑ **2. Data Loading & Reshaping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c7115f",
   "metadata": {
    "id": "code-load-reshape"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 35 features: ['joint_00', 'joint_01', 'joint_02']... to ['pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Loading and reshaping training data...\n",
      "Loading and reshaping test data...\n",
      "X_train_full shape: (661, 160, 35)\n",
      "y_train_labels_str shape: (661,)\n",
      "X_test shape: (1324, 160, 35)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define File Paths and Features ---\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "# Define our time-series features\n",
    "# We'll ignore static features (n_legs, etc.) for our baseline model\n",
    "JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(31)]\n",
    "PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "FEATURES = JOINT_FEATURES + PAIN_FEATURES\n",
    "\n",
    "N_FEATURES = len(FEATURES)\n",
    "N_TIMESTEPS = 160 # Fixed from our earlier debugging\n",
    "\n",
    "print(f\"Using {N_FEATURES} features: {FEATURES[:3]}... to {FEATURES[-3:]}\")\n",
    "\n",
    "# --- 2. Create the Reshaping Function ---\n",
    "def reshape_data(df, features_list, n_timesteps):\n",
    "    \"\"\"\n",
    "    Pivots the long-format dataframe into a 3D NumPy array.\n",
    "    Shape: (n_samples, n_timesteps, n_features)\n",
    "    \"\"\"\n",
    "    df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "    data_2d = df_pivot.values\n",
    "    n_samples = data_2d.shape[0]\n",
    "    data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "    return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "def create_sliding_windows(X_3d, y=None, window_size=100, stride=20):\n",
    "    \"\"\"\n",
    "    Takes 3D data (n_samples, n_timesteps, n_features)\n",
    "    and creates overlapping windows.\n",
    "    \n",
    "    Returns:\n",
    "    - new_X: (n_windows, window_size, n_features)\n",
    "    - new_y (if y is provided): (n_windows,)\n",
    "    - window_indices: (n_windows,) array tracking which original sample\n",
    "                      (e.g., 0, 1, 2...) each window came from.\n",
    "    \"\"\"\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    # This new array tracks which original sample each window came from.\n",
    "    window_indices = [] \n",
    "    \n",
    "    n_samples, n_timesteps, n_features = X_3d.shape\n",
    "    \n",
    "    # Iterate over each original sample\n",
    "    for i in range(n_samples):\n",
    "        sample = X_3d[i] # Shape (160, 35)\n",
    "        \n",
    "        # Slide a window over this sample\n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            window = sample[idx : idx + window_size]\n",
    "            new_X.append(window)\n",
    "            window_indices.append(i) # Track the original sample index (0, 1, 2...)\n",
    "            \n",
    "            if y is not None:\n",
    "                new_y.append(y[i]) # The label is the same for all windows\n",
    "                \n",
    "            idx += stride\n",
    "            \n",
    "    if y is not None:\n",
    "        # Return new X, new y, and the index mapping\n",
    "        return np.array(new_X), np.array(new_y), np.array(window_indices)\n",
    "    else:\n",
    "        # Return new X and the index mapping\n",
    "        return np.array(new_X), np.array(window_indices)\n",
    "\n",
    "# --- 3. Load and Reshape Data ---\n",
    "print(\"Loading and reshaping training data...\")\n",
    "X_train_long = pd.read_csv(X_TRAIN_PATH)\n",
    "X_train_full = reshape_data(X_train_long[X_train_long['sample_index'].isin(X_train_long['sample_index'].unique())], FEATURES, N_TIMESTEPS)\n",
    "\n",
    "print(\"Loading and reshaping test data...\")\n",
    "X_test_long = pd.read_csv(X_TEST_PATH)\n",
    "X_test = reshape_data(X_test_long, FEATURES, N_TIMESTEPS)\n",
    "\n",
    "# Load labels\n",
    "y_train_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "y_train_full_df = y_train_df.sort_values(by='sample_index')\n",
    "y_train_labels_str = y_train_full_df['label'].values # Fixed from our debugging\n",
    "\n",
    "print(f\"X_train_full shape: {X_train_full.shape}\")\n",
    "print(f\"y_train_labels_str shape: {y_train_labels_str.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "del X_train_long, X_test_long, y_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d77fc",
   "metadata": {
    "id": "markdown-preprocessing"
   },
   "source": [
    "## üöß **3. Preprocessing: Split & Scale**\n",
    "\n",
    "1.  **Encode Labels:** Convert `no_pain`, `low_pain`, `high_pain` to `0`, `1`, `2`.\n",
    "2.  **Split Data:** Use `StratifiedShuffleSplit` to create a single 80/20 train/validation split **for the HPO phase**.\n",
    "3.  **Scale Features:** Use `StandardScaler`. We `fit` it *only* on the training data and `transform` all sets (train, val, and test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2355a626",
   "metadata": {
    "id": "code-preprocessing"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels encoded. 3 classes: {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
      "--- Applying sliding window augmentation ---\n",
      "Original X shape: (661, 160, 35)\n",
      "Windowed X shape: (3305, 80, 35)\n",
      "Original y shape: (661,)\n",
      "Windowed y shape: (3305,)\n",
      "\n",
      "--- Splitting windowed data ---\n",
      "  X_train_split: (2644, 80, 35)\n",
      "  y_train_split: (2644,)\n",
      "  X_val_split:   (661, 80, 35)\n",
      "  y_val_split:   (661,)\n",
      "Fitting Scaler on X_train_2d shape: (211520, 35)\n",
      "Scaling complete for HPO data.\n",
      "  X_train_scaled: (2644, 80, 35)\n",
      "  X_val_scaled:   (661, 80, 35)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Encode Labels ---\n",
    "LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "le = LabelEncoder()\n",
    "le.fit(list(LABEL_MAPPING.keys()))\n",
    "y_train_full = le.transform(y_train_labels_str)\n",
    "N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "print(f\"Labels encoded. {N_CLASSES} classes: {LABEL_MAPPING}\")\n",
    "\n",
    "# 1. DEFINE YOUR WINDOW PARAMETERS\n",
    "NEW_WINDOW_SIZE = 80  # Example: 80 timesteps\n",
    "NEW_STRIDE = 20       # Example: 20 timesteps\n",
    "\n",
    "print(\"--- Applying sliding window augmentation ---\")\n",
    "# 3. APPLY THE NEW WINDOWING FUNCTION\n",
    "X_train_windowed, y_train_windowed, _ = create_sliding_windows(\n",
    "    X_train_full, \n",
    "    y_train_full, \n",
    "    window_size=NEW_WINDOW_SIZE, \n",
    "    stride=NEW_STRIDE\n",
    ")\n",
    "\n",
    "print(f\"Original X shape: {X_train_full.shape}\")\n",
    "print(f\"Windowed X shape: {X_train_windowed.shape}\")\n",
    "print(f\"Original y shape: {y_train_full.shape}\")\n",
    "print(f\"Windowed y shape: {y_train_windowed.shape}\")\n",
    "\n",
    "# (You would also need to apply this to X_test for submission)\n",
    "# (But NOT to y_train_full for the split)\n",
    "\n",
    "# 4. USE THE *WINDOWED* DATA FOR YOUR SPLIT\n",
    "print(\"\\n--- Splitting windowed data ---\")\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# IMPORTANT: You split the *new*, *larger* X and y arrays\n",
    "for train_idx, val_idx in sss.split(X_train_windowed, y_train_windowed):\n",
    "    X_train_split = X_train_windowed[train_idx]\n",
    "    y_train_split = y_train_windowed[train_idx]\n",
    "    X_val_split = X_train_windowed[val_idx]\n",
    "    y_val_split = y_train_windowed[val_idx]\n",
    "\n",
    "print(f\"  X_train_split: {X_train_split.shape}\")\n",
    "print(f\"  y_train_split: {y_train_split.shape}\")\n",
    "print(f\"  X_val_split:   {X_val_split.shape}\")\n",
    "print(f\"  y_val_split:   {y_val_split.shape}\")\n",
    "# --- 3. Scale Features (The \"No-Cheating\" Rule) ---\n",
    "scaler = StandardScaler()\n",
    "ns, ts, f = X_train_split.shape\n",
    "X_train_2d = X_train_split.reshape(ns * ts, f)\n",
    "print(f\"Fitting Scaler on X_train_2d shape: {X_train_2d.shape}\")\n",
    "scaler.fit(X_train_2d)\n",
    "\n",
    "X_train_scaled_2d = scaler.transform(X_train_2d)\n",
    "X_train_scaled = X_train_scaled_2d.reshape(ns, ts, f)\n",
    "\n",
    "ns_val, ts_val, f_val = X_val_split.shape\n",
    "X_val_2d = X_val_split.reshape(ns_val * ts_val, f_val)\n",
    "X_val_scaled_2d = scaler.transform(X_val_2d)\n",
    "X_val_scaled = X_val_scaled_2d.reshape(ns_val, ts_val, f_val)\n",
    "\n",
    "# We will scale the X_test data LATER, during the submission phase\n",
    "# This ensures our K-Fold models use the correct scaler for their fold\n",
    "# For now, we only need the HPO data scaled.\n",
    "print(\"Scaling complete for HPO data.\")\n",
    "print(f\"  X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"  X_val_scaled:   {X_val_scaled.shape}\")\n",
    "\n",
    "del X_train_2d, X_val_2d, X_train_scaled_2d, X_val_scaled_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c883b33",
   "metadata": {
    "id": "markdown-dataloaders"
   },
   "source": [
    "## üöö **4. PyTorch DataLoaders (for HPO)**\n",
    "\n",
    "This section prepares the DataLoaders for the Ray Tune HPO phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8702f5a",
   "metadata": {
    "id": "code-dataloaders"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders will be created inside the tuning loop.\n",
      "Keeping HPO numpy arrays in memory for K-Fold training...\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Convert to Tensors ---\n",
    "train_features = torch.from_numpy(X_train_scaled).float()\n",
    "train_targets = torch.from_numpy(y_train_split).long()\n",
    "\n",
    "val_features = torch.from_numpy(X_val_scaled).float()\n",
    "val_targets = torch.from_numpy(y_val_split).long()\n",
    "\n",
    "# test_features = torch.from_numpy(X_test_scaled).float() # We do this later\n",
    "\n",
    "# --- 2. Create TensorDatasets ---\n",
    "train_ds = TensorDataset(train_features, train_targets)\n",
    "val_ds = TensorDataset(val_features, val_targets)\n",
    "# test_ds = TensorDataset(test_features) # We do this later\n",
    "\n",
    "# --- 3. Define make_loader function (from Lecture 4) ---\n",
    "BATCH_SIZE = 128 # This will be our default, but Optuna can tune it\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    # Set num_workers=0 for Windows-friendly loading (from our debugging)\n",
    "    num_workers = 0 \n",
    "    \n",
    "    # Create DataLoader with performance optimizations\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=int(batch_size), # Ensure batch_size is an int for the DataLoader\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=None,\n",
    "    )\n",
    "\n",
    "# --- 4. Create DataLoaders ---\n",
    "# We will create these *inside* the objective function now,\n",
    "# as the batch size is a hyperparameter we want to tune.\n",
    "print(\"DataLoaders will be created inside the tuning loop.\")\n",
    "\n",
    "# --- !! MODIFICATION !! ---\n",
    "# We are KEEPING the numpy arrays for the K-Fold step later\n",
    "print(\"Keeping HPO numpy arrays in memory for K-Fold training...\")\n",
    "# del X_train_scaled, X_val_scaled, test_features, train_features, val_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16263592",
   "metadata": {
    "id": "markdown-model-training"
   },
   "source": [
    "## üõ†Ô∏è **5. Model & Training Engine**\n",
    "\n",
    "These are the core components from Lecture 4, modified for Ray Tune and K-Fold.\n",
    "\n",
    "-   `RecurrentClassifier`: Our flexible model (RNN, LSTM, GRU).\n",
    "-   `train_one_epoch` / `validate_one_epoch`: Standard loops.\n",
    "-   `objective_function`: The wrapper for Ray Tune's HPO.\n",
    "-   `fit`: The **original** training loop from Lecture 4, **b-rought back** to handle our K-Fold training, complete with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06613e58",
   "metadata": {
    "id": "code-recurrent-summary"
   },
   "outputs": [],
   "source": [
    "def recurrent_summary(model, input_size):\n",
    "    \"\"\"\n",
    "    Custom summary function that emulates torchinfo's output while correctly\n",
    "    counting parameters for RNN/GRU/LSTM layers.\n",
    "    \"\"\"\n",
    "\n",
    "    output_shapes = {}\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1  # Replace batch dimension with -1\n",
    "\n",
    "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
    "                    shape2 = list(output[1][0].shape)\n",
    "                else:  # RNN/GRU case: h_n only\n",
    "                    shape2 = list(output[1].shape)\n",
    "                shape2[1] = -1\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "        return hook\n",
    "\n",
    "    try:\n",
    "        device_summary = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device_summary = torch.device(\"cpu\")\n",
    "\n",
    "    dummy_input = torch.randn(1, *input_size).to(device_summary)\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(dummy_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ac4fc09",
   "metadata": {
    "id": "code-recurrent-classifier"
   },
   "outputs": [],
   "source": [
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic RNN classifier (RNN, LSTM, GRU) from Lecture 4.\n",
    "    Uses the last hidden state for classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',        # 'RNN', 'LSTM', or 'GRU'\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "\n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "\n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "\n",
    "        # Dropout is only applied between layers (if num_layers > 1)\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        if self.bidirectional:\n",
    "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
    "        else:\n",
    "            classifier_input_size = hidden_size\n",
    "\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_length, input_size)\n",
    "        \"\"\"\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0]\n",
    "\n",
    "        if self.bidirectional:\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
    "        else:\n",
    "            hidden_to_classify = hidden[-1]\n",
    "\n",
    "        logits = self.classifier(hidden_to_classify)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17b67a46",
   "metadata": {
    "id": "code-training-loop-MODIFIED"
   },
   "outputs": [],
   "source": [
    "# This cell contains all our training functions\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
    "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "\n",
    "# This is the objective_function for Ray Tune HPO\n",
    "def objective_function(config, train_ds, val_ds):\n",
    "    \"\"\"\n",
    "    This is the main function that Ray Tune will call for each trial.\n",
    "    'config' is a dictionary of hyperparameters from Optuna.\n",
    "    'train_ds' and 'val_ds' are our TensorDatasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Create DataLoaders with the tuned batch size ---\n",
    "    train_loader = make_loader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "    val_loader = make_loader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=False)\n",
    "    \n",
    "    # --- 2. Create Model --- \n",
    "    model = RecurrentClassifier(\n",
    "        input_size=N_FEATURES,\n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        num_classes=N_CLASSES,\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        bidirectional=config[\"bidirectional\"],\n",
    "        rnn_type=config[\"rnn_type\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    if torch.__version__[0] >= \"2\":\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    # --- 3. Create Optimizer, Loss, Scaler ---\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "    # --- 4. The Training Loop (adapted from fit) ---\n",
    "    # We loop for a fixed number of epochs (e.g., 200) and let Ray's\n",
    "    # ASHA scheduler handle early stopping of bad trials.\n",
    "    EPOCHS = 200 \n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, 0, config[\"l2_lambda\"]\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # --- Send Results to Ray Tune --- \n",
    "        tune.report({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_f1\": val_f1\n",
    "        })\n",
    "\n",
    "# --- !! MODIFICATION !! ---\n",
    "# This is the original 'fit' function from Lecture 4, brought back.\n",
    "# We will use THIS function for our K-Fold training, as it has early stopping.\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    print(f\"Will train for {epochs} epochs with patience={patience} monitoring {evaluation_metric}\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
    "            )\n",
    "\n",
    "        if verbose > 0 and (epoch % verbose == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                  f\"Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n",
    "                  f\"Val: Loss={val_loss:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    if restore_best_weights and patience > 0:\n",
    "        print(f\"Restoring best model from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    if patience == 0:\n",
    "        print(\"Training complete. Saving final model.\")\n",
    "        torch.save(model.state_dict(), model_path.replace(\"_best_model.pt\", \"_final_model.pt\"))\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    \n",
    "    print(f\"--- Finished Training: {experiment_name} ---\")\n",
    "    return model, training_history, best_epoch if 'best_epoch' in locals() else epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a903e3",
   "metadata": {
    "id": "markdown-optuna-search"
   },
   "source": [
    "## üß™ **6. Phase 1: Hyperparameter Search**\n",
    "\n",
    "This cell is identical to before. We run HPO on our single 80/20 split to find a good `FINAL_CONFIG` to use for our K-Fold training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41d8c96b",
   "metadata": {
    "id": "code-optuna-search"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-08 22:43:58</td></tr>\n",
       "<tr><td>Running for: </td><td>00:10:28.79        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.8/13.9 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=20<br>Bracket: Iter 80.000: 0.9832878472699766 | Iter 40.000: 0.978597344733804 | Iter 20.000: 0.956015437064059<br>Logical resource usage: 4.0/16 CPUs, 0.25/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th>bidirectional  </th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  l2_lambda</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th>rnn_type  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_f1</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_function_dd0b15d7</td><td>TERMINATED</td><td>127.0.0.1:6696 </td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.37759 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">3.51052e-05</td><td style=\"text-align: right;\">2.57749e-05</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       116.595  </td><td style=\"text-align: right;\">  0.229354  </td><td style=\"text-align: right;\">  0.938132</td><td style=\"text-align: right;\"> 0.170618 </td></tr>\n",
       "<tr><td>objective_function_405329c7</td><td>TERMINATED</td><td>127.0.0.1:46992</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.500568</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.87899e-05</td><td style=\"text-align: right;\">0.00395307 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       248.193  </td><td style=\"text-align: right;\">  0.798879  </td><td style=\"text-align: right;\">  0.944063</td><td style=\"text-align: right;\"> 0.193177 </td></tr>\n",
       "<tr><td>objective_function_e8dbb73e</td><td>TERMINATED</td><td>127.0.0.1:52396</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.54854 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.57871e-07</td><td style=\"text-align: right;\">0.00349848 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">       143.817  </td><td style=\"text-align: right;\">  0.205152  </td><td style=\"text-align: right;\">  0.938113</td><td style=\"text-align: right;\"> 0.223358 </td></tr>\n",
       "<tr><td>objective_function_142bf406</td><td>TERMINATED</td><td>127.0.0.1:27616</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.490094</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.14063e-05</td><td style=\"text-align: right;\">0.000228306</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       397.04   </td><td style=\"text-align: right;\">  0.0276794 </td><td style=\"text-align: right;\">  0.998476</td><td style=\"text-align: right;\"> 0.0618401</td></tr>\n",
       "<tr><td>objective_function_eaae3216</td><td>TERMINATED</td><td>127.0.0.1:51840</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.241778</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">4.51583e-06</td><td style=\"text-align: right;\">0.00122211 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       134.977  </td><td style=\"text-align: right;\">  0.0356843 </td><td style=\"text-align: right;\">  0.994916</td><td style=\"text-align: right;\"> 0.0666771</td></tr>\n",
       "<tr><td>objective_function_74ce3293</td><td>TERMINATED</td><td>127.0.0.1:6708 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.141629</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000728551</td><td style=\"text-align: right;\">0.00161677 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       163.886  </td><td style=\"text-align: right;\">  0.119777  </td><td style=\"text-align: right;\">  0.980728</td><td style=\"text-align: right;\"> 0.0809032</td></tr>\n",
       "<tr><td>objective_function_4feb27ba</td><td>TERMINATED</td><td>127.0.0.1:8436 </td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.258338</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">3.72887e-06</td><td style=\"text-align: right;\">0.00429286 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        36.2272 </td><td style=\"text-align: right;\">  0.00352432</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.09291  </td></tr>\n",
       "<tr><td>objective_function_954ea5f5</td><td>TERMINATED</td><td>127.0.0.1:40944</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.528355</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000319292</td><td style=\"text-align: right;\">0.000765373</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">       168.726  </td><td style=\"text-align: right;\">  0.123499  </td><td style=\"text-align: right;\">  0.983529</td><td style=\"text-align: right;\"> 0.109776 </td></tr>\n",
       "<tr><td>objective_function_935aae6a</td><td>TERMINATED</td><td>127.0.0.1:45928</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.593239</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">3.2435e-07 </td><td style=\"text-align: right;\">0.000844423</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        31.0759 </td><td style=\"text-align: right;\">  0.21506   </td><td style=\"text-align: right;\">  0.93011 </td><td style=\"text-align: right;\"> 0.23864  </td></tr>\n",
       "<tr><td>objective_function_50fdc555</td><td>TERMINATED</td><td>127.0.0.1:20616</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.406444</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000438249</td><td style=\"text-align: right;\">0.0066905  </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">       118.452  </td><td style=\"text-align: right;\"> 10.362     </td><td style=\"text-align: right;\">  0.698501</td><td style=\"text-align: right;\"> 0.840256 </td></tr>\n",
       "<tr><td>objective_function_93bc5cde</td><td>TERMINATED</td><td>127.0.0.1:13020</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.28817 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000527422</td><td style=\"text-align: right;\">1.61635e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        25.6367 </td><td style=\"text-align: right;\">  1.17866   </td><td style=\"text-align: right;\">  0.740753</td><td style=\"text-align: right;\"> 0.55688  </td></tr>\n",
       "<tr><td>objective_function_45c50030</td><td>TERMINATED</td><td>127.0.0.1:10904</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.413167</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">7.33637e-07</td><td style=\"text-align: right;\">0.000226868</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        46.2177 </td><td style=\"text-align: right;\">  0.0861917 </td><td style=\"text-align: right;\">  0.968819</td><td style=\"text-align: right;\"> 0.147508 </td></tr>\n",
       "<tr><td>objective_function_49b7a378</td><td>TERMINATED</td><td>127.0.0.1:50488</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.321758</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000117068</td><td style=\"text-align: right;\">0.00296898 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        59.9628 </td><td style=\"text-align: right;\">  0.0457976 </td><td style=\"text-align: right;\">  0.998828</td><td style=\"text-align: right;\"> 0.0543289</td></tr>\n",
       "<tr><td>objective_function_ddbe6556</td><td>TERMINATED</td><td>127.0.0.1:51724</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.451234</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000484859</td><td style=\"text-align: right;\">0.000398484</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        83.4602 </td><td style=\"text-align: right;\">  0.065432  </td><td style=\"text-align: right;\">  0.997256</td><td style=\"text-align: right;\"> 0.0462141</td></tr>\n",
       "<tr><td>objective_function_05403885</td><td>TERMINATED</td><td>127.0.0.1:38392</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.227571</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">2.27287e-06</td><td style=\"text-align: right;\">0.00825405 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        37.2356 </td><td style=\"text-align: right;\">  0.0948701 </td><td style=\"text-align: right;\">  0.986278</td><td style=\"text-align: right;\"> 0.131597 </td></tr>\n",
       "<tr><td>objective_function_48014cc6</td><td>TERMINATED</td><td>127.0.0.1:11040</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.237431</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">2.34086e-06</td><td style=\"text-align: right;\">0.00960453 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        13.5724 </td><td style=\"text-align: right;\">  0.136179  </td><td style=\"text-align: right;\">  0.958021</td><td style=\"text-align: right;\"> 0.11024  </td></tr>\n",
       "<tr><td>objective_function_563fc87f</td><td>TERMINATED</td><td>127.0.0.1:41400</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.21796 </td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">2.31259e-06</td><td style=\"text-align: right;\">0.00847569 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        27.4885 </td><td style=\"text-align: right;\">  0.00392567</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.0941545</td></tr>\n",
       "<tr><td>objective_function_ce329a8c</td><td>TERMINATED</td><td>127.0.0.1:29140</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.120943</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">8.76483e-05</td><td style=\"text-align: right;\">6.96363e-05</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         9.478  </td><td style=\"text-align: right;\">  0.546473  </td><td style=\"text-align: right;\">  0.774201</td><td style=\"text-align: right;\"> 0.491521 </td></tr>\n",
       "<tr><td>objective_function_1e07d962</td><td>TERMINATED</td><td>127.0.0.1:11488</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.111139</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">7.77895e-05</td><td style=\"text-align: right;\">7.44406e-05</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         7.39498</td><td style=\"text-align: right;\">  0.54024   </td><td style=\"text-align: right;\">  0.7746  </td><td style=\"text-align: right;\"> 0.488158 </td></tr>\n",
       "<tr><td>objective_function_0bd16f98</td><td>TERMINATED</td><td>127.0.0.1:34612</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.329739</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000156612</td><td style=\"text-align: right;\">7.50275e-05</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        21.5961 </td><td style=\"text-align: right;\">  0.460057  </td><td style=\"text-align: right;\">  0.923266</td><td style=\"text-align: right;\"> 0.194213 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-08 22:33:48,102 E 40516 6752] (gcs_server.exe) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-08 22:33:52,235 E 49856 11732] (raylet.exe) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "2025-11-08 22:43:58,622\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'c:/Users/Karim Negm/Documents/AN2DL/Challenge 1/ray_results/pirate_pain_optuna_search' in 0.0404s.\n",
      "2025-11-08 22:43:58,645\tINFO tune.py:1041 -- Total run time: 628.86 seconds (628.74 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Search Complete ---\n",
      "Getting best trial from analysis...\n",
      "Best validation F1 score: 0.9789\n",
      "Best hyperparameters found:\n",
      "{'rnn_type': 'GRU', 'lr': 0.00022830551615609333, 'batch_size': 64, 'hidden_size': 384, 'num_layers': 2, 'dropout_rate': 0.4900942505159511, 'bidirectional': True, 'l2_lambda': 1.1406282234266435e-05}\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define the Search Space for Optuna ---\n",
    "search_space = {\n",
    "    \"rnn_type\": tune.choice(['GRU', 'LSTM']),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),           # Widen the learning rate\n",
    "    \"batch_size\": tune.choice([64, 128, 256]),  \n",
    "    \"hidden_size\": tune.choice([128, 256, 384]),# Let's try bigger models\n",
    "    \"num_layers\": tune.choice([2, 3]),       # Let's try deeper models\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.6),     # Widen the dropout range\n",
    "    \"bidirectional\": tune.choice([True, False]),\n",
    "    \"l2_lambda\": tune.loguniform(1e-7, 1e-3)      # Widen the L2 range\n",
    "}\n",
    "\n",
    "# --- 2. Define the Optimizer (Optuna) and Scheduler (ASHA) ---\n",
    "optuna_search = OptunaSearch(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\",\n",
    "    grace_period=20,  # Min epochs a trial must run\n",
    "    reduction_factor=2  # How aggressively to stop trials\n",
    ")\n",
    "\n",
    "# --- 3. Initialize Ray (WITH THE BIG HAMMER FIX) ---\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "ray_logs_path = os.path.abspath(\"./ray_results\")\n",
    "os.makedirs(ray_logs_path, exist_ok=True)\n",
    "os.environ[\"RAY_TEMP_DIR\"] = ray_logs_path\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=16, \n",
    "    num_gpus=1, \n",
    "    ignore_reinit_error=True\n",
    ")\n",
    "\n",
    "def short_trial_name(trial):\n",
    "    \"\"\"Creates a short, unique name for each trial folder.\"\"\"\n",
    "    return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "\n",
    "# --- 4. Run the Tuner ---\n",
    "print(\"Starting hyperparameter search (1 trial at a time)...\")\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(objective_function, train_ds=train_ds, val_ds=val_ds),\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 0.25}, \n",
    "    config=search_space,\n",
    "    num_samples=20, # Number of different HPO trials to run\n",
    "    search_alg=optuna_search,\n",
    "    scheduler=scheduler,\n",
    "    name=\"pirate_pain_optuna_search\",\n",
    "    storage_path=ray_logs_path,\n",
    "    trial_dirname_creator=short_trial_name,\n",
    "    log_to_file=True,\n",
    "    verbose=1 # 0 = quiet, 1 = table, 2 = detailed\n",
    ")\n",
    "\n",
    "print(\"\\n--- Search Complete ---\")\n",
    "\n",
    "# --- 5. Get Best Results (FIX 3) ---\n",
    "print(\"Getting best trial from analysis...\")\n",
    "best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "if best_trial:\n",
    "    best_config = best_trial.config\n",
    "    best_val_f1 = best_trial.last_result[\"val_f1\"]\n",
    "    \n",
    "    print(f\"Best validation F1 score: {best_val_f1:.4f}\")\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(best_config)\n",
    "else:\n",
    "    print(\"ERROR: No trials completed successfully. Check the 'ray_results' folder for logs.\")\n",
    "    best_config = None # Handle the case where all trials failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b564a8e",
   "metadata": {
    "id": "markdown-submission-config"
   },
   "source": [
    "## üèÜ **7. Final Model Configuration**\n",
    "\n",
    "This cell now holds our winning configuration, ready for the K-Fold training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab5802d6",
   "metadata": {
    "id": "code-submission-config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèÜ Final Configuration Set --- \n",
      "Best Val F1 from HPO search: 0.9789\n",
      "{'rnn_type': 'GRU', 'lr': 0.00022830551615609333, 'batch_size': 64, 'hidden_size': 384, 'num_layers': 2, 'dropout_rate': 0.4900942505159511, 'bidirectional': True, 'l2_lambda': 1.1406282234266435e-05}\n",
      "Submission name will be: submission_GRU_H384_L2_BTrue_Optuna_KFold_Ensemble.csv\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# --- üèÜ FINAL MODEL CONFIGURATION üèÜ ---\n",
    "# ===================================================================\n",
    "FINAL_CONFIG = best_config\n",
    "FINAL_BEST_VAL_F1 = best_val_f1\n",
    "\n",
    "print(\"--- üèÜ Final Configuration Set --- \")\n",
    "print(f\"Best Val F1 from HPO search: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "print(FINAL_CONFIG)\n",
    "\n",
    "# --- Set variables for the K-Fold & submission cells ---\n",
    "FINAL_MODEL_TYPE = FINAL_CONFIG[\"rnn_type\"]\n",
    "FINAL_HIDDEN_SIZE = FINAL_CONFIG[\"hidden_size\"]\n",
    "FINAL_HIDDEN_LAYERS = FINAL_CONFIG[\"num_layers\"]\n",
    "FINAL_BIDIRECTIONAL = FINAL_CONFIG[\"bidirectional\"]\n",
    "FINAL_DROPOUT_RATE = FINAL_CONFIG[\"dropout_rate\"]\n",
    "FINAL_LEARNING_RATE = FINAL_CONFIG[\"lr\"]\n",
    "FINAL_L2_LAMBDA = FINAL_CONFIG[\"l2_lambda\"]\n",
    "FINAL_BATCH_SIZE = FINAL_CONFIG[\"batch_size\"]\n",
    "\n",
    "FINAL_EXPERIMENT_NAME = f\"{FINAL_MODEL_TYPE}_H{FINAL_HIDDEN_SIZE}_L{FINAL_HIDDEN_LAYERS}_B{FINAL_BIDIRECTIONAL}_Optuna_KFold_Ensemble\"\n",
    "\n",
    "print(f\"Submission name will be: submission_{FINAL_EXPERIMENT_NAME}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03626778",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è **8. Phase 2: K-Fold Ensemble Training**\n",
    "\n",
    "This is the **NEW** robust training step.\n",
    "\n",
    "Instead of finding one \"best epoch,\" we will train 5 separate models on 5 different splits (folds) of the data. We use the `FINAL_CONFIG` from our HPO search as the configuration for all 5 models.\n",
    "\n",
    "We use the `fit` function's early stopping to find the best version of each model and save its weights (e.g., `kfold_fold_1_best_model.pt`, `kfold_fold_2_best_model.pt`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cfc58f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting 5-Fold CV Training ---\n",
      "Full windowed training data shape: (3305, 80, 35)\n",
      "\n",
      "--- Fold 1/5 --- (kfold_fold_1) ---\n",
      "--- Starting Training: kfold_fold_1 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.6546, F1=0.7273 | Val: Loss=0.5451, F1=0.7644\n",
      "Epoch  25/300 | Train: Loss=0.0194, F1=0.9950 | Val: Loss=0.1105, F1=0.9645\n",
      "Epoch  50/300 | Train: Loss=0.0131, F1=0.9973 | Val: Loss=0.0917, F1=0.9631\n",
      "Epoch  75/300 | Train: Loss=0.0020, F1=1.0000 | Val: Loss=0.0956, F1=0.9725\n",
      "Epoch 100/300 | Train: Loss=0.0002, F1=1.0000 | Val: Loss=0.0992, F1=0.9787\n",
      "\n",
      "Early stopping triggered after 100 epochs.\n",
      "Restoring best model from epoch 70 with val_f1 0.9804\n",
      "--- Finished Training: kfold_fold_1 ---\n",
      "Fold 1 Best Model Val F1: 0.9804\n",
      "\n",
      "--- Fold 2/5 --- (kfold_fold_2) ---\n",
      "--- Starting Training: kfold_fold_2 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.6439, F1=0.7524 | Val: Loss=0.5239, F1=0.7601\n",
      "Epoch  25/300 | Train: Loss=0.0452, F1=0.9854 | Val: Loss=0.0983, F1=0.9726\n",
      "Epoch  50/300 | Train: Loss=0.0054, F1=0.9992 | Val: Loss=0.0339, F1=0.9893\n",
      "Epoch  75/300 | Train: Loss=0.0001, F1=1.0000 | Val: Loss=0.0324, F1=0.9924\n",
      "\n",
      "Early stopping triggered after 84 epochs.\n",
      "Restoring best model from epoch 54 with val_f1 0.9924\n",
      "--- Finished Training: kfold_fold_2 ---\n",
      "Fold 2 Best Model Val F1: 0.9924\n",
      "\n",
      "--- Fold 3/5 --- (kfold_fold_3) ---\n",
      "--- Starting Training: kfold_fold_3 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.6374, F1=0.7411 | Val: Loss=0.5141, F1=0.7682\n",
      "Epoch  25/300 | Train: Loss=0.0265, F1=0.9893 | Val: Loss=0.1086, F1=0.9798\n",
      "Epoch  50/300 | Train: Loss=0.0005, F1=1.0000 | Val: Loss=0.0773, F1=0.9876\n",
      "Epoch  75/300 | Train: Loss=0.0001, F1=1.0000 | Val: Loss=0.0805, F1=0.9876\n",
      "Epoch 100/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.0868, F1=0.9876\n",
      "\n",
      "Early stopping triggered after 108 epochs.\n",
      "Restoring best model from epoch 78 with val_f1 0.9892\n",
      "--- Finished Training: kfold_fold_3 ---\n",
      "Fold 3 Best Model Val F1: 0.9892\n",
      "\n",
      "--- Fold 4/5 --- (kfold_fold_4) ---\n",
      "--- Starting Training: kfold_fold_4 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.6382, F1=0.7443 | Val: Loss=0.5149, F1=0.7796\n",
      "Epoch  25/300 | Train: Loss=0.0491, F1=0.9851 | Val: Loss=0.1893, F1=0.9501\n",
      "Epoch  50/300 | Train: Loss=0.0014, F1=1.0000 | Val: Loss=0.0838, F1=0.9819\n",
      "Epoch  75/300 | Train: Loss=0.0001, F1=1.0000 | Val: Loss=0.0996, F1=0.9819\n",
      "\n",
      "Early stopping triggered after 86 epochs.\n",
      "Restoring best model from epoch 56 with val_f1 0.9834\n",
      "--- Finished Training: kfold_fold_4 ---\n",
      "Fold 4 Best Model Val F1: 0.9834\n",
      "\n",
      "--- Fold 5/5 --- (kfold_fold_5) ---\n",
      "--- Starting Training: kfold_fold_5 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.6301, F1=0.7474 | Val: Loss=0.5136, F1=0.7783\n",
      "Epoch  25/300 | Train: Loss=0.0183, F1=0.9947 | Val: Loss=0.0944, F1=0.9713\n",
      "Epoch  50/300 | Train: Loss=0.0051, F1=0.9989 | Val: Loss=0.0568, F1=0.9802\n",
      "Epoch  75/300 | Train: Loss=0.0001, F1=1.0000 | Val: Loss=0.0977, F1=0.9773\n",
      "\n",
      "Early stopping triggered after 81 epochs.\n",
      "Restoring best model from epoch 51 with val_f1 0.9848\n",
      "--- Finished Training: kfold_fold_5 ---\n",
      "Fold 5 Best Model Val F1: 0.9848\n",
      "\n",
      "--- üèÜ K-Fold Training Complete ---\n",
      "Fold F1 scores: [0.9804, 0.9924, 0.9892, 0.9834, 0.9848]\n",
      "Average F1 across folds: 0.9860\n"
     ]
    }
   ],
   "source": [
    "# --- !! NEW CELL: K-FOLD TRAINING !! ---\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "N_SPLITS = 5  # 5-fold CV is a standard\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# --- 1. Reconstruct the full (windowed) training set ---\n",
    "# We combine the HPO train/val splits to get our full dataset back\n",
    "X_full_windowed = np.concatenate((X_train_scaled, X_val_scaled), axis=0)\n",
    "y_full_windowed = np.concatenate((y_train_split, y_val_split), axis=0)\n",
    "\n",
    "print(f\"--- Starting {N_SPLITS}-Fold CV Training ---\")\n",
    "print(f\"Full windowed training data shape: {X_full_windowed.shape}\")\n",
    "\n",
    "fold_val_f1_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_full_windowed, y_full_windowed)):\n",
    "    fold_name = f\"kfold_fold_{fold+1}\"\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} --- ({fold_name}) ---\")\n",
    "    \n",
    "    # --- 2. Create datasets for this fold ---\n",
    "    X_train_fold = torch.from_numpy(X_full_windowed[train_idx]).float()\n",
    "    y_train_fold = torch.from_numpy(y_full_windowed[train_idx]).long()\n",
    "    X_val_fold = torch.from_numpy(X_full_windowed[val_idx]).float()\n",
    "    y_val_fold = torch.from_numpy(y_full_windowed[val_idx]).long()\n",
    "\n",
    "    train_ds_fold = TensorDataset(X_train_fold, y_train_fold)\n",
    "    val_ds_fold = TensorDataset(X_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader_fold = make_loader(train_ds_fold, batch_size=FINAL_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    val_loader_fold = make_loader(val_ds_fold, batch_size=FINAL_BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "    \n",
    "    # --- 3. Create a fresh model (using FINAL_CONFIG) ---\n",
    "    model_fold = RecurrentClassifier(\n",
    "        input_size=N_FEATURES,\n",
    "        hidden_size=FINAL_HIDDEN_SIZE,\n",
    "        num_layers=FINAL_HIDDEN_LAYERS,\n",
    "        num_classes=N_CLASSES,\n",
    "        dropout_rate=FINAL_DROPOUT_RATE,\n",
    "        bidirectional=FINAL_BIDIRECTIONAL,\n",
    "        rnn_type=FINAL_MODEL_TYPE\n",
    "    ).to(device)\n",
    "    \n",
    "    if torch.__version__[0] >= \"2\": \n",
    "        model_fold = torch.compile(model_fold)\n",
    "\n",
    "    optimizer_fold = torch.optim.AdamW(model_fold.parameters(), lr=FINAL_LEARNING_RATE, weight_decay=FINAL_L2_LAMBDA)\n",
    "    scaler_fold = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion_fold = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # --- 4. Train this fold with early stopping ---\n",
    "    # The 'fit' function will find the best epoch and save the model\n",
    "    # to 'models/kfold_fold_{fold+1}_best_model.pt'\n",
    "    model_fold, _, _ = fit(\n",
    "        model=model_fold,\n",
    "        train_loader=train_loader_fold,\n",
    "        val_loader=val_loader_fold,\n",
    "        epochs=300, # High epoch count, patience will stop it\n",
    "        criterion=criterion_fold,\n",
    "        optimizer=optimizer_fold,\n",
    "        scaler=scaler_fold,\n",
    "        device=device,\n",
    "        writer=None, # No need to log to tensorboard\n",
    "        verbose=25,\n",
    "        experiment_name=fold_name, # This saves the model with a unique name\n",
    "        patience=30 # 30-epoch patience\n",
    "    )\n",
    "    \n",
    "    # --- 5. (Optional) Check this fold's final F1 score ---\n",
    "    val_loss, val_f1 = validate_one_epoch(model_fold, val_loader_fold, criterion_fold, device)\n",
    "    fold_val_f1_list.append(val_f1)\n",
    "    print(f\"Fold {fold+1} Best Model Val F1: {val_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n--- üèÜ K-Fold Training Complete ---\")\n",
    "print(f\"Fold F1 scores: {[round(f, 4) for f in fold_val_f1_list]}\")\n",
    "print(f\"Average F1 across folds: {np.mean(fold_val_f1_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f19754",
   "metadata": {
    "id": "markdown-submission"
   },
   "source": [
    "## üì¨ **9. Phase 3: K-Fold Ensemble Submission**\n",
    "\n",
    "This is the **NEW** robust submission step.\n",
    "\n",
    "1.  Prepare the `X_test` data (scaling and windowing) exactly as before.\n",
    "2.  Create the `test_loader`.\n",
    "3.  Load each of our 5 saved fold-models.\n",
    "4.  Get 5 sets of (softmax) probability predictions for the test set.\n",
    "5.  Average these 5 probability sets into a single, robust probability matrix.\n",
    "6.  Aggregate these mean probabilities (using mean) from windows to full samples.\n",
    "7.  Take the `argmax` of the final aggregated probabilities to get the submission class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acc486f2",
   "metadata": {
    "id": "code-submission"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing full dataset for FINAL SCALER ---\n",
      "Fitting FINAL Scaler on X_train_full_2d shape: (105760, 35)\n",
      "Final scaling of test set complete.\n",
      "--- Applying sliding windows to final test set ---\n",
      "Test windowed shape: (6620, 80, 35)\n",
      "Test window indices shape: (6620,)\n",
      "Final TestLoader created.\n",
      "\n",
      "--- Generating predictions from 5 fold models ---\n",
      "Loading model 1/5 from models/kfold_fold_1_best_model.pt...\n",
      "Loading model 2/5 from models/kfold_fold_2_best_model.pt...\n",
      "Loading model 3/5 from models/kfold_fold_3_best_model.pt...\n",
      "Loading model 4/5 from models/kfold_fold_4_best_model.pt...\n",
      "Loading model 5/5 from models/kfold_fold_5_best_model.pt...\n",
      "\n",
      "Averaging 5 sets of probabilities...\n",
      "Mean probability matrix shape: (6620, 3)\n",
      "Aggregating window probabilities to sample predictions...\n",
      "Aggregated to 1324 final probability vectors.\n",
      "Loading sample submission file for correct formatting...\n",
      "Prediction count matches. Creating submission.\n",
      "\n",
      "Successfully saved to submissions\\submission_GRU_H384_L2_BTrue_Optuna_KFold_Ensemble_w80_s20.csv!\n",
      "This file is correctly formatted for Kaggle:\n",
      "  sample_index    label\n",
      "0          000  no_pain\n",
      "1          001  no_pain\n",
      "2          002  no_pain\n",
      "3          003  no_pain\n",
      "4          004  no_pain\n"
     ]
    }
   ],
   "source": [
    "# --- !! NEW CELL: K-FOLD ENSEMBLE SUBMISSION !! ---\n",
    "from scipy.stats import mode \n",
    "\n",
    "print(\"\\n--- Preparing full dataset for FINAL SCALER ---\")\n",
    "\n",
    "# --- 1. Prepare Full Training Set for Final Scaler ---\n",
    "# We fit the scaler on ALL available training data (non-windowed)\n",
    "scaler_final = StandardScaler()\n",
    "ns, ts, f = X_train_full.shape\n",
    "X_train_full_2d = X_train_full.reshape(ns * ts, f)\n",
    "\n",
    "print(f\"Fitting FINAL Scaler on X_train_full_2d shape: {X_train_full_2d.shape}\")\n",
    "scaler_final.fit(X_train_full_2d)\n",
    "\n",
    "# --- 2. Scale and Window the TEST data ---\n",
    "# Scale X_test using the scaler_final\n",
    "ns_test, ts_test, f_test = X_test.shape\n",
    "X_test_2d = X_test.reshape(ns_test * ts_test, f_test)\n",
    "X_test_final_scaled_2d = scaler_final.transform(X_test_2d)\n",
    "X_test_final_scaled = X_test_final_scaled_2d.reshape(ns_test, ts_test, f_test)\n",
    "\n",
    "print(\"Final scaling of test set complete.\")\n",
    "print(\"--- Applying sliding windows to final test set ---\")\n",
    "\n",
    "X_test_final_windowed, test_window_indices = create_sliding_windows(\n",
    "    X_test_final_scaled,\n",
    "    y=None, # No labels for the test set\n",
    "    window_size=NEW_WINDOW_SIZE,\n",
    "    stride=NEW_STRIDE\n",
    ")\n",
    "print(f\"Test windowed shape: {X_test_final_windowed.shape}\")\n",
    "print(f\"Test window indices shape: {test_window_indices.shape}\")\n",
    "\n",
    "# --- 3. Create Final TestLoader ---\n",
    "final_test_features = torch.from_numpy(X_test_final_windowed).float()\n",
    "final_test_ds = TensorDataset(final_test_features) # No labels\n",
    "\n",
    "def make_final_loader(ds, batch_size, shuffle, drop_last):\n",
    "    return DataLoader(\n",
    "        ds, batch_size=int(batch_size), shuffle=shuffle, drop_last=drop_last,\n",
    "        num_workers=0, pin_memory=True, pin_memory_device=\"cuda\", prefetch_factor=None\n",
    "    )\n",
    "\n",
    "test_loader = make_final_loader(final_test_ds, batch_size=FINAL_BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "print(\"Final TestLoader created.\")\n",
    "\n",
    "# --- 4. Get Predictions from all K-Fold Models ---\n",
    "all_fold_probabilities = []\n",
    "print(f\"\\n--- Generating predictions from {N_SPLITS} fold models ---\")\n",
    "\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_name = f\"kfold_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    print(f\"Loading model {fold+1}/{N_SPLITS} from {model_path}...\")\n",
    "\n",
    "    # 1. Create a fresh model shell\n",
    "    model_fold = RecurrentClassifier(\n",
    "        input_size=N_FEATURES,\n",
    "        hidden_size=FINAL_HIDDEN_SIZE,\n",
    "        num_layers=FINAL_HIDDEN_LAYERS,\n",
    "        num_classes=N_CLASSES,\n",
    "        dropout_rate=FINAL_DROPOUT_RATE,\n",
    "        bidirectional=FINAL_BIDIRECTIONAL,\n",
    "        rnn_type=FINAL_MODEL_TYPE\n",
    "    ).to(device)\n",
    "    \n",
    "    # 2. Load the saved weights (no compilation needed for eval)\n",
    "    # --- FIX: Load state_dict and clean keys ---\n",
    "    # The saved model was compiled, so keys have a '_orig_mod.' prefix.\n",
    "    # We must remove this prefix to load into a non-compiled model.\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('_orig_mod.'):\n",
    "            new_state_dict[k[len('_orig_mod.'):]] = v # Remove the prefix\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    \n",
    "    model_fold.load_state_dict(new_state_dict) # Load the cleaned state_dict\n",
    "    # --- END FIX ---\n",
    "    model_fold.eval()\n",
    "\n",
    "    fold_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in test_loader: \n",
    "            inputs = inputs.to(device)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model_fold(inputs)\n",
    "                # --- Get SOFTMAX probabilities, not argmax ---\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                fold_predictions.append(probs.cpu().numpy())\n",
    "    \n",
    "    all_fold_probabilities.append(np.concatenate(fold_predictions))\n",
    "\n",
    "# --- 5. Average the Probabilities ---\n",
    "# 'all_fold_probabilities' is a list of 5 arrays, each of shape (n_windows, n_classes)\n",
    "print(f\"\\nAveraging {len(all_fold_probabilities)} sets of probabilities...\")\n",
    "# Stack and average across the first dimension (the folds)\n",
    "mean_probabilities = np.mean(all_fold_probabilities, axis=0)\n",
    "print(f\"Mean probability matrix shape: {mean_probabilities.shape}\") # (n_windows, 3)\n",
    "\n",
    "# --- 6. Aggregate Mean Probabilities (Mean) ---\n",
    "print(\"Aggregating window probabilities to sample predictions...\")\n",
    "\n",
    "# Use pandas for easy aggregation\n",
    "prob_cols = [f\"prob_{i}\" for i in range(N_CLASSES)]\n",
    "df_probs = pd.DataFrame(mean_probabilities, columns=prob_cols)\n",
    "df_probs['original_index'] = test_window_indices # Map from window to original sample\n",
    "\n",
    "# Group by the original sample index and find the MEAN probability\n",
    "agg_probs = df_probs.groupby('original_index')[prob_cols].mean().values\n",
    "\n",
    "# 'agg_probs' now has one probability vector per original sample (shape 1324, 3)\n",
    "print(f\"Aggregated to {len(agg_probs)} final probability vectors.\")\n",
    "\n",
    "# --- NOW get the final class by taking argmax of the mean probabilities ---\n",
    "final_predictions_numeric = np.argmax(agg_probs, axis=1)\n",
    "\n",
    "# Inverse transform these aggregated predictions to labels\n",
    "predicted_labels = le.inverse_transform(final_predictions_numeric)\n",
    "\n",
    "# --- 7. Save Submission File ---\n",
    "print(\"Loading sample submission file for correct formatting...\")\n",
    "X_test_long = pd.read_csv(X_TEST_PATH)\n",
    "test_sample_indices = sorted(X_test_long['sample_index'].unique())\n",
    "\n",
    "if len(predicted_labels) != len(test_sample_indices):\n",
    "    print(f\"ERROR: Prediction count mismatch! Predictions: {len(predicted_labels)}, Test Indices: {len(test_sample_indices)}\")\n",
    "else:\n",
    "    print(\"Prediction count matches. Creating submission.\")\n",
    "    \n",
    "    final_submission_df = pd.DataFrame({\n",
    "        'sample_index': test_sample_indices,\n",
    "        'label': predicted_labels \n",
    "    })\n",
    "    \n",
    "    final_submission_df['sample_index'] = final_submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "\n",
    "    SUBMISSIONS_DIR = \"submissions\"\n",
    "    os.makedirs(SUBMISSIONS_DIR, exist_ok=True)\n",
    "    \n",
    "    submission_filename = f\"submission_{FINAL_EXPERIMENT_NAME}_w{NEW_WINDOW_SIZE}_s{NEW_STRIDE}.csv\"\n",
    "    submission_filepath = os.path.join(SUBMISSIONS_DIR, submission_filename)\n",
    "    \n",
    "    final_submission_df.to_csv(submission_filepath, index=False)\n",
    "\n",
    "    print(f\"\\nSuccessfully saved to {submission_filepath}!\")\n",
    "    print(\"This file is correctly formatted for Kaggle:\")\n",
    "    print(final_submission_df.head())\n",
    "\n",
    "del all_fold_probabilities, final_test_features, final_test_ds, test_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
