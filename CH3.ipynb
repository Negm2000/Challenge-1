{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d12f5a0",
   "metadata": {},
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è (v13: Final Strategy)**\n",
    "\n",
    "This notebook contains a final, consolidated strategy designed to address the critical class imbalance issue, particularly the low recall for the `high_pain` class. It incorporates several major enhancements based on the data analysis.\n",
    "\n",
    "**üî• Final Strategy Updates:**\n",
    "1.  **Feature Cleaning (`joint_30` Removed):** The zero-variance feature `joint_30` has been removed. All column indexing and model layers have been updated to reflect the new feature count.\n",
    "2.  **WeightedRandomSampler:** To combat imbalance at the batch level, a `WeightedRandomSampler` is now used for the training `DataLoader`. This oversamples minority classes (`low_pain`, `high_pain`) to ensure the model sees a more balanced set of examples in every batch.\n",
    "3.  **Noise Augmentation:** To improve generalization for minority classes, simple Gaussian noise is added to the training samples for `low_pain` and `high_pain`. This creates new, synthetic training data and forces the model to learn more robust features.\n",
    "4.  **Top-K Averaging for Submission:** The submission logic has been upgraded. Instead of averaging predictions from all sliding windows (which dilutes the signal), it now identifies the **Top 5 windows** most indicative of `high_pain` and averages their predictions for the final classification. This focuses the decision on the most critical moments in the time series.\n",
    "5.  **Compiled Model & OneCycleLR:** We retain the use of `torch.compile()` for speed and the `OneCycleLR` scheduler for efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03291d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Setup & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba22088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GPU ---\n",
      "PyTorch version: 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 1234\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# --- Sklearn Imports ---\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Ray[tune] & Optuna Imports ---\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from functools import partial\n",
    "\n",
    "# --- Setup Directories & Device ---\n",
    "logs_dir = \"tensorboard\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8582ef",
   "metadata": {},
   "source": [
    "## üîÑ 2. Data Loading & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44203b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Data ---\n",
      "Removed zero-variance feature: 'joint_30'\n",
      "Loaded X_train_full (shape: (661, 160, 35)) and y_train_full (shape: (661,))\n",
      "Loaded X_test_full (shape: (1324, 160, 35))\n",
      "\n",
      "--- 2. Engineering 'is_pirate' Feature ---\n",
      "Created X_train_full_engineered (shape: (661, 160, 36))\n",
      "Created X_test_full_engineered (shape: (1324, 160, 36))\n",
      "N_FEATURES is now: 36\n",
      "\n",
      "--- 3. Calculating Alpha Weights for Focal Loss ---\n",
      "Class counts (0, 1, 2): [511  94  56]\n",
      "Calculated alpha weights: tensor([0.0643, 0.3493, 0.5864], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Loading Data ---\")\n",
    "\n",
    "# --- Define File Paths and Features ---\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "try:\n",
    "    features_long_df = pd.read_csv(X_TRAIN_PATH)\n",
    "    labels_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "    X_test_long_df = pd.read_csv(X_TEST_PATH)\n",
    "    \n",
    "    # --- MODIFICATION: Remove joint_30 (zero variance) ---\n",
    "    if 'joint_30' in features_long_df.columns:\n",
    "        features_long_df = features_long_df.drop(columns=['joint_30'])\n",
    "        X_test_long_df = X_test_long_df.drop(columns=['joint_30'])\n",
    "        print(\"Removed zero-variance feature: 'joint_30'\")\n",
    "    \n",
    "    N_TIMESTEPS = 160\n",
    "    # --- MODIFICATION: JOINT_FEATURES now goes up to 29, not 30 ---\n",
    "    JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(30)]\n",
    "    PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "    TIME_FEATURE = ['time']\n",
    "    FEATURES = JOINT_FEATURES + PAIN_FEATURES + TIME_FEATURE\n",
    "    LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "    N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "    def reshape_data(df, features_list, n_timesteps):\n",
    "        df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "        data_2d = df_pivot.values\n",
    "        n_samples = data_2d.shape[0]\n",
    "        data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "        return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "    X_train_full = reshape_data(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())], FEATURES, N_TIMESTEPS)\n",
    "    X_test_full = reshape_data(X_test_long_df, FEATURES, N_TIMESTEPS)\n",
    "    y_train_full_df = labels_df.sort_values(by='sample_index')\n",
    "    le = LabelEncoder().fit(list(LABEL_MAPPING.keys()))\n",
    "    y_train_full = le.transform(y_train_full_df['label'])\n",
    "    print(f\"Loaded X_train_full (shape: {X_train_full.shape}) and y_train_full (shape: {y_train_full.shape})\")\n",
    "    print(f\"Loaded X_test_full (shape: {X_test_full.shape})\")\n",
    "\n",
    "    print(\"\\n--- 2. Engineering 'is_pirate' Feature ---\")\n",
    "    static_cols = ['sample_index', 'n_legs', 'n_hands', 'n_eyes']\n",
    "    static_df = features_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    pirate_filter = (static_df['n_legs'] == 'one+peg_leg') | (static_df['n_hands'] == 'one+hook_hand') | (static_df['n_eyes'] == 'one+eye_patch')\n",
    "    pirate_indices = static_df[pirate_filter].index\n",
    "    sample_indices_ordered = sorted(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())]['sample_index'].unique())\n",
    "    is_pirate_map = np.array([1 if idx in pirate_indices else 0 for idx in sample_indices_ordered])\n",
    "    pirate_feature_broadcast = np.tile(is_pirate_map.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    X_train_full_engineered = np.concatenate([X_train_full, pirate_feature_broadcast], axis=2)\n",
    "\n",
    "    static_df_test = X_test_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    pirate_filter_test = (static_df_test['n_legs'] == 'one+peg_leg') | (static_df_test['n_hands'] == 'one+hook_hand') | (static_df_test['n_eyes'] == 'one+eye_patch')\n",
    "    pirate_indices_test = static_df_test[pirate_filter_test].index\n",
    "    sample_indices_test_ordered = sorted(X_test_long_df['sample_index'].unique())\n",
    "    is_pirate_map_test = np.array([1 if idx in pirate_indices_test else 0 for idx in sample_indices_test_ordered])\n",
    "    pirate_feature_broadcast_test = np.tile(is_pirate_map_test.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    X_test_full_engineered = np.concatenate([X_test_full, pirate_feature_broadcast_test], axis=2)\n",
    "    \n",
    "    N_FEATURES_NEW = X_train_full_engineered.shape[2]\n",
    "    print(f\"Created X_train_full_engineered (shape: {X_train_full_engineered.shape})\")\n",
    "    print(f\"Created X_test_full_engineered (shape: {X_test_full_engineered.shape})\")\n",
    "    print(f\"N_FEATURES is now: {N_FEATURES_NEW}\")\n",
    "\n",
    "    print(\"\\n--- 3. Calculating Alpha Weights for Focal Loss ---\")\n",
    "    class_counts_series = labels_df['label'].value_counts()\n",
    "    counts_ordered = class_counts_series.reindex(LABEL_MAPPING.keys()).values\n",
    "    class_weights_tensor = 1.0 / torch.tensor(counts_ordered, dtype=torch.float)\n",
    "    alpha_tensor = (class_weights_tensor / class_weights_tensor.sum()).to(device)\n",
    "    print(f\"Class counts (0, 1, 2): {counts_ordered}\")\n",
    "    print(f\"Calculated alpha weights: {alpha_tensor}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "early-sanity-check-cell",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================================================================\n",
      "--- EARLY SANITY CHECK: Verifying Feature Indexing Logic ---\n",
      "======================================================================\n",
      "\n",
      "Detected a total of 36 features in the engineered data (X_train_full_engineered).\n",
      "Defined 31 continuous indices and 5 categorical indices.\n",
      "\n",
      "--- CONTINUOUS features being grabbed:\n",
      "First 5: ['joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04']\n",
      "Last 5: ['joint_26', 'joint_27', 'joint_28', 'joint_29', 'time']\n",
      "  -> OK if this list contains all 'joint_XX' features and the 'time' feature.\n",
      "\n",
      "--- CATEGORICAL features being grabbed:\n",
      "['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'is_pirate']\n",
      "  -> OK if this list contains all 'pain_survey_X' features and 'is_pirate'.\n",
      "\n",
      "--- AUTOMATED CHECKS ---\n",
      "  -> OK: Sum of indices matches total feature count.\n",
      "  -> OK: No overlap between continuous and categorical indices.\n",
      "  -> OK: 'time' feature correctly identified as continuous.\n",
      "\n",
      "======================================================================\n",
      "--- SANITY CHECK PASSED: Feature indexing logic is correct. ---\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n======================================================================\")\n",
    "print(\"--- EARLY SANITY CHECK: Verifying Feature Indexing Logic ---\")\n",
    "print(\"======================================================================\\n\")\n",
    "\n",
    "# This check ensures that after removing 'joint_30', our manual indexing correctly\\n\",\n",
    "# separates the remaining continuous and categorical features.\n",
    "\n",
    "# --- 1. Recreate the list of engineered feature names in order ---\n",
    "engineered_feature_names = JOINT_FEATURES + PAIN_FEATURES + TIME_FEATURE + ['is_pirate']\n",
    "total_features = len(engineered_feature_names)\n",
    "print(f\"Detected a total of {total_features} features in the engineered data (X_train_full_engineered).\")\n",
    "\n",
    "# --- 2. Define the exact index lists that will be used later in the pipeline ---\n",
    "# This logic MUST match the logic used in the HPO and K-Fold cells.\n",
    "continuous_indices_orig = list(range(30)) + [34]  # 30 joints (0-29) + time (34)\n",
    "categorical_indices_orig = list(range(30, 34)) + [35] # 4 pain surveys (30-33) + is_pirate (35)\n",
    "\n",
    "print(f\"Defined {len(continuous_indices_orig)} continuous indices and {len(categorical_indices_orig)} categorical indices.\\n\")\n",
    "\n",
    "# --- 3. Map indices to names to verify what we are grabbing ---\n",
    "feature_array = np.array(engineered_feature_names)\n",
    "grabbed_continuous_features = feature_array[continuous_indices_orig].tolist()\n",
    "grabbed_categorical_features = feature_array[categorical_indices_orig].tolist()\n",
    "\n",
    "print(\"--- CONTINUOUS features being grabbed:\")\n",
    "print(f\"First 5: {grabbed_continuous_features[:5]}\")\n",
    "print(f\"Last 5: {grabbed_continuous_features[-5:]}\")\n",
    "print(\"  -> OK if this list contains all 'joint_XX' features and the 'time' feature.\\n\")\n",
    "\n",
    "print(\"--- CATEGORICAL features being grabbed:\")\n",
    "print(grabbed_categorical_features)\n",
    "print(\"  -> OK if this list contains all 'pain_survey_X' features and 'is_pirate'.\\n\")\n",
    "\n",
    "# --- 4. Perform automated checks for correctness ---\n",
    "print(\"--- AUTOMATED CHECKS ---\")\n",
    "errors_found = False\n",
    "\n",
    "# Check for completeness\n",
    "if len(continuous_indices_orig) + len(categorical_indices_orig) != total_features:\n",
    "    print(f\"  -> !!! ERROR: Index count mismatch! Sum of indices ({len(continuous_indices_orig) + len(categorical_indices_orig)}) != Total Features ({total_features})\")\n",
    "    errors_found = True\n",
    "else:\n",
    "    print(\"  -> OK: Sum of indices matches total feature count.\")\n",
    "\n",
    "# Check for overlap\n",
    "overlap = set(continuous_indices_orig).intersection(set(categorical_indices_orig))\n",
    "if overlap:\n",
    "    print(f\"  -> !!! ERROR: Overlap detected between continuous and categorical indices: {overlap}\")\n",
    "    errors_found = True\n",
    "else:\n",
    "    print(\"  -> OK: No overlap between continuous and categorical indices.\")\n",
    "\n",
    "# Check if 'time' was correctly identified as continuous\n",
    "if 'time' not in grabbed_continuous_features:\n",
    "    print(\"  -> !!! ERROR: 'time' feature was not correctly assigned to the continuous group.\")\n",
    "    errors_found = True\n",
    "else:\n",
    "    print(\"  -> OK: 'time' feature correctly identified as continuous.\")\n",
    "\n",
    "print(\"\\n======================================================================\")\n",
    "if errors_found:\n",
    "    print(\"--- SANITY CHECK FAILED: Review the index definitions before proceeding! ---\")\n",
    "    # This will stop the notebook execution if an error is found\n",
    "    raise ValueError(\"Sanity check for feature indexing failed. Please review the output.\")\n",
    "else:\n",
    "    print(\"--- SANITY CHECK PASSED: Feature indexing logic is correct. ---\")\n",
    "print(\"======================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f8bc4",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 3. Helper Functions & Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7c2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Implements Focal Loss for cost-sensitive learning.\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets].to(focal_loss.device)\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def create_sliding_windows(X_3d, y=None, window_size=100, stride=20):\n",
    "    new_X, new_y, window_indices = [], [], []\n",
    "    n_samples, n_timesteps, _ = X_3d.shape\n",
    "    for i in range(n_samples):\n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            new_X.append(X_3d[i, idx:idx+window_size, :])\n",
    "            window_indices.append(i)\n",
    "            if y is not None: new_y.append(y[i])\n",
    "            idx += stride\n",
    "    if y is not None:\n",
    "        return np.array(new_X), np.array(new_y), np.array(window_indices)\n",
    "    return np.array(new_X), np.array(window_indices)\n",
    "\n",
    "# --- MODIFICATION: make_loader now accepts a sampler for weighted sampling ---\n",
    "def make_loader(ds, batch_size, shuffle, drop_last, sampler=None):\n",
    "    # Sampler and shuffle are mutually exclusive in PyTorch's DataLoader\n",
    "    use_shuffle = shuffle if sampler is None else False\n",
    "    return DataLoader(ds, batch_size=int(batch_size), shuffle=use_shuffle, drop_last=drop_last, \n",
    "                      num_workers=2, pin_memory=True, persistent_workers=True, sampler=sampler)\n",
    "\n",
    "# --- MODIFICATION: New function for data augmentation ---\n",
    "def augment_minority_classes(X_w, y_w, continuous_feature_count, noise_level=0.01, aug_factor=2):\n",
    "    \"\"\"Augments minority classes (1 and 2) with noise injection.\"\"\"\n",
    "    print(f\"Augmenting minority classes... Original sample count: {len(X_w)}\")\n",
    "    X_aug_list, y_aug_list = [X_w], [y_w]\n",
    "    \n",
    "    # Find indices for low_pain (1) and high_pain (2)\n",
    "    minority_indices = np.where((y_w == 1) | (y_w == 2))[0]\n",
    "    \n",
    "    if len(minority_indices) == 0:\n",
    "        print(\"No minority class samples found to augment.\")\n",
    "        return X_w, y_w\n",
    "        \n",
    "    for i in range(aug_factor):\n",
    "        X_to_augment = X_w[minority_indices]\n",
    "        \n",
    "        # Create Gaussian noise with the same shape as the data to be augmented\n",
    "        noise = np.random.normal(0, noise_level, X_to_augment.shape)\n",
    "        \n",
    "        # Create a zero-filled copy to store the augmented data\n",
    "        X_augmented = X_to_augment.copy()\n",
    "        \n",
    "        # Add noise ONLY to the continuous features, leaving categoricals untouched\n",
    "        X_augmented[:, :, :continuous_feature_count] += noise[:, :, :continuous_feature_count]\n",
    "        \n",
    "        X_aug_list.append(X_augmented)\n",
    "        y_aug_list.append(y_w[minority_indices])\n",
    "        \n",
    "    X_final_aug = np.concatenate(X_aug_list, axis=0)\n",
    "    y_final_aug = np.concatenate(y_aug_list, axis=0)\n",
    "    print(f\"Augmentation complete. New sample count: {len(X_final_aug)}\")\n",
    "    \n",
    "    return X_final_aug, y_final_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faed930",
   "metadata": {},
   "source": [
    "## üß† 4. Model & Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3cadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, rnn_outputs):\n",
    "        energy = torch.tanh(self.attn(rnn_outputs))\n",
    "        attn_scores = self.v(energy).squeeze(2)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(1), rnn_outputs).squeeze(1)\n",
    "        return context_vector\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes,\n",
    "                 conv_out_channels, conv_kernel_size, bidirectional,\n",
    "                 dropout_rate, feature_dropout_rate, rnn_type='GRU'):\n",
    "        super().__init__()\n",
    "        self.rnn_type, self.num_layers, self.hidden_size, self.bidirectional = \\\n",
    "            rnn_type, num_layers, hidden_size, bidirectional\n",
    "        \n",
    "        rnn_hidden_dim = hidden_size * 2 if bidirectional else hidden_size\n",
    "\n",
    "        self.pain_embed_dim, self.pirate_embed_dim = 4, 4\n",
    "        self.pain_embeddings = nn.ModuleList([nn.Embedding(3, self.pain_embed_dim) for _ in range(4)])\n",
    "        self.pirate_embedding = nn.Embedding(2, self.pirate_embed_dim)\n",
    "        \n",
    "        # --- MODIFICATION: Continuous features count is now 31 (30 joints + 1 time), not 32 ---\n",
    "        num_continuous_features = 31\n",
    "        total_embedding_dim = (4 * self.pain_embed_dim) + self.pirate_embed_dim\n",
    "        conv_input_size = num_continuous_features + total_embedding_dim\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=conv_input_size, out_channels=conv_out_channels,\n",
    "                                kernel_size=conv_kernel_size, padding='same')\n",
    "        self.conv_activation = nn.ReLU()\n",
    "        self.feature_dropout = nn.Dropout(feature_dropout_rate)\n",
    "\n",
    "        if rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=conv_out_channels, hidden_size=hidden_size,\n",
    "                num_layers=num_layers, batch_first=True, bidirectional=bidirectional,\n",
    "                dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=conv_out_channels, hidden_size=hidden_size,\n",
    "                num_layers=num_layers, batch_first=True, bidirectional=bidirectional,\n",
    "                dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        self.attention = Attention(rnn_hidden_dim)\n",
    "        self.classifier = nn.Linear(rnn_hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- MODIFICATION: Slicing adjusted for 31 continuous features ---\n",
    "        x_continuous = x[:, :, :31]\n",
    "        x_categorical = x[:, :, 31:].long()\n",
    "        \n",
    "        embedded_cats = [self.pain_embeddings[i](x_categorical[:, :, i]) for i in range(4)] \\\n",
    "                      + [self.pirate_embedding(x_categorical[:, :, 4])]\n",
    "        x_combined = torch.cat([x_continuous] + embedded_cats, dim=2)\n",
    "        x_permuted = x_combined.permute(0, 2, 1)\n",
    "        x_conv = self.conv_activation(self.conv1d(x_permuted))\n",
    "        x_conv_permuted = x_conv.permute(0, 2, 1)\n",
    "        x_dropped = self.feature_dropout(x_conv_permuted)\n",
    "        rnn_outputs, _ = self.rnn(x_dropped)\n",
    "        context_vector = self.attention(rnn_outputs)\n",
    "        return self.classifier(context_vector)\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step() # <-- OneCycleLR is stepped each batch\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / len(loader.sampler if loader.sampler else loader.dataset), f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset.tensors[0]), f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def objective_function(config, X_train_w, y_train_w, X_val_w, y_val_w, alpha_tensor):\n",
    "    EPOCHS = 150\n",
    "    train_loader = make_loader(TensorDataset(X_train_w, y_train_w), config[\"batch_size\"], True, True)\n",
    "    val_loader = make_loader(TensorDataset(X_val_w, y_val_w), config[\"batch_size\"], False, False)\n",
    "\n",
    "    model_config = {k: v for k, v in config.items() if k not in ['lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "    model = RecurrentClassifier(**model_config, num_classes=N_CLASSES).to(device)\n",
    "    model = torch.compile(model, backend=\"eager\") # <-- SPEEDUP\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=config[\"lr\"], epochs=EPOCHS, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = FocalLoss(alpha=alpha_tensor, gamma=config['focal_loss_gamma'])\n",
    "\n",
    "    best_val_f1 = -1.0; patience_counter = 0; hpo_patience = 30\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, _ = train_one_epoch(model, train_loader, criterion, optimizer, scaler, scheduler, device)\n",
    "        _, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        tune.report({\"val_f1\": val_f1, \"train_loss\": train_loss})\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1; patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= hpo_patience: break\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scheduler, scaler, device, patience, experiment_name):\n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "    best_f1 = -1; patience_counter = 0\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, scaler, scheduler, device)\n",
    "        val_loss, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1, patience_counter = val_f1, 0\n",
    "            torch.save(model._orig_mod.state_dict() if hasattr(model, '_orig_mod') else model.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience: print(f\"Early stopping at epoch {epoch}. Best F1: {best_f1:.4f}\"); break\n",
    "        if epoch % 3 == 0: print(f\"Epoch {epoch:3d}/{epochs} | Best Val F1: {best_f1:.4f} | Val F1: {val_f1:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(f\"--- Finished Training --- Best F1: {best_f1:.4f}\")\n",
    "    uncompiled_model = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "    uncompiled_model.load_state_dict(torch.load(model_path))\n",
    "    return uncompiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f911c",
   "metadata": {},
   "source": [
    "## üß™ 5. Phase 1: Hyperparameter Search\n",
    "\n",
    "We will run the HPO search as before to find the best set of hyperparameters. The core model and training logic inside the search remains the same, but the final K-Fold training will incorporate our new balancing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09a835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Splitting data for HPO ---\n",
      "--- Pre-scaling data for HPO efficiency ---\n",
      "--- Creating fixed sliding windows for HPO ---\n",
      "Created training windows of shape: torch.Size([40128, 10, 36])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define fixed windowing parameters\n",
    "WINDOW_SIZE = 10\n",
    "STRIDE = 2\n",
    "\n",
    "# --- MODIFICATION: Column indices updated to reflect removal of joint_30 ---\n",
    "# Original features: 30 joints (0-29), 4 pain (30-33), 1 time (34), 1 pirate (35)\n",
    "continuous_indices_orig = list(range(30)) + [34] # 30 joints + time\n",
    "categorical_indices_orig = list(range(30, 34)) + [35] # 4 pain surveys + is_pirate\n",
    "\n",
    "X_train_full_reordered = np.concatenate([\n",
    "    X_train_full_engineered[:, :, continuous_indices_orig],\n",
    "    X_train_full_engineered[:, :, categorical_indices_orig]\n",
    "], axis=2)\n",
    "\n",
    "print(\"--- Splitting data for HPO ---\")\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "for train_idx, val_idx in sss.split(X_train_full_reordered, y_train_full):\n",
    "    X_train_split, y_train_split = X_train_full_reordered[train_idx], y_train_full[train_idx]\n",
    "    X_val_split, y_val_split = X_train_full_reordered[val_idx], y_train_full[val_idx]\n",
    "\n",
    "print(\"--- Pre-scaling data for HPO efficiency ---\")\n",
    "# --- MODIFICATION: After reordering, there are 31 continuous features ---\n",
    "continuous_indices_reordered = list(range(31)) \n",
    "preprocessor_hpo = ColumnTransformer([('scaler', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "ns, ts, f = X_train_split.shape\n",
    "X_train_split_scaled = preprocessor_hpo.fit_transform(X_train_split.reshape(ns*ts, f)).reshape(ns, ts, f)\n",
    "ns_val, ts_val, f_val = X_val_split.shape\n",
    "X_val_split_scaled = preprocessor_hpo.transform(X_val_split.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "\n",
    "print(\"--- Creating fixed sliding windows for HPO ---\")\n",
    "X_train_w, y_train_w, _ = create_sliding_windows(X_train_split_scaled, y_train_split, WINDOW_SIZE, STRIDE)\n",
    "X_val_w, y_val_w, _ = create_sliding_windows(X_val_split_scaled, y_val_split, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "# Convert to tensors once before HPO\n",
    "X_train_w_torch = torch.from_numpy(X_train_w).float()\n",
    "y_train_w_torch = torch.from_numpy(y_train_w).long()\n",
    "X_val_w_torch = torch.from_numpy(X_val_w).float()\n",
    "y_val_w_torch = torch.from_numpy(y_val_w).long()\n",
    "\n",
    "print(f\"Created training windows of shape: {X_train_w_torch.shape}\")\n",
    "\n",
    "# Clean up memory\n",
    "del X_train_split_scaled, X_val_split_scaled, X_train_w, y_train_w, X_val_w, y_val_w\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b42797c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-17 09:49:23</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:02.70        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.2/13.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 100.000: None | Iter 50.000: None | Iter 25.000: None<br>Logical resource usage: 16.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th>bidirectional  </th><th style=\"text-align: right;\">  conv_kernel_size</th><th style=\"text-align: right;\">  conv_out_channels</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  feature_dropout_rate</th><th style=\"text-align: right;\">  focal_loss_gamma</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  l2_lambda</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th>rnn_type  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_f1</th><th style=\"text-align: right;\">  train_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_function_d3f059ea</td><td>RUNNING   </td><td>127.0.0.1:33552</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.471509 </td><td style=\"text-align: right;\">             0.362726 </td><td style=\"text-align: right;\">           1.45168</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">6.58493e-08</td><td style=\"text-align: right;\">0.00330047 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         37.6862</td><td style=\"text-align: right;\">0.859859</td><td style=\"text-align: right;\"> 0.0446891  </td></tr>\n",
       "<tr><td>objective_function_7dde1ec4</td><td>RUNNING   </td><td>127.0.0.1:33584</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.4558   </td><td style=\"text-align: right;\">             0.422645 </td><td style=\"text-align: right;\">           1.82882</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.13501e-07</td><td style=\"text-align: right;\">0.00309611 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         37.6992</td><td style=\"text-align: right;\">0.878423</td><td style=\"text-align: right;\"> 0.037524   </td></tr>\n",
       "<tr><td>objective_function_2a625bf7</td><td>RUNNING   </td><td>127.0.0.1:33616</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.468796 </td><td style=\"text-align: right;\">             0.370299 </td><td style=\"text-align: right;\">           1.50751</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">5.85567e-08</td><td style=\"text-align: right;\">0.00368539 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         37.6937</td><td style=\"text-align: right;\">0.866599</td><td style=\"text-align: right;\"> 0.0442824  </td></tr>\n",
       "<tr><td>objective_function_ab2aa931</td><td>RUNNING   </td><td>127.0.0.1:33688</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.345147 </td><td style=\"text-align: right;\">             0.410382 </td><td style=\"text-align: right;\">           1.4584 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.29261e-07</td><td style=\"text-align: right;\">0.00239552 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         40.9352</td><td style=\"text-align: right;\">0.874057</td><td style=\"text-align: right;\"> 0.0447165  </td></tr>\n",
       "<tr><td>objective_function_cec0c729</td><td>PENDING   </td><td>               </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.234849 </td><td style=\"text-align: right;\">             0.419    </td><td style=\"text-align: right;\">           1.82654</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.493e-07  </td><td style=\"text-align: right;\">0.00402668 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>objective_function_a861e309</td><td>TERMINATED</td><td>127.0.0.1:21932</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.383319 </td><td style=\"text-align: right;\">             0.0104024</td><td style=\"text-align: right;\">           1.27821</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">6.00886e-07</td><td style=\"text-align: right;\">0.000232323</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        378.408 </td><td style=\"text-align: right;\">0.928738</td><td style=\"text-align: right;\"> 0.00276643 </td></tr>\n",
       "<tr><td>objective_function_2df3d805</td><td>TERMINATED</td><td>127.0.0.1:29536</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.47144  </td><td style=\"text-align: right;\">             0.365539 </td><td style=\"text-align: right;\">           1.2182 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">3.39011e-07</td><td style=\"text-align: right;\">0.00153987 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        376.554 </td><td style=\"text-align: right;\">0.932257</td><td style=\"text-align: right;\"> 0.00226642 </td></tr>\n",
       "<tr><td>objective_function_69b1b00d</td><td>TERMINATED</td><td>127.0.0.1:20052</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.393727 </td><td style=\"text-align: right;\">             0.374004 </td><td style=\"text-align: right;\">           1.29281</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">4.30389e-07</td><td style=\"text-align: right;\">0.00140723 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">       1312.17  </td><td style=\"text-align: right;\">0.947306</td><td style=\"text-align: right;\"> 0.000238619</td></tr>\n",
       "<tr><td>objective_function_ee121a90</td><td>TERMINATED</td><td>127.0.0.1:29312</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.376464 </td><td style=\"text-align: right;\">             0.0509729</td><td style=\"text-align: right;\">           1.33862</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">4.56294e-07</td><td style=\"text-align: right;\">0.00163282 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        769.492 </td><td style=\"text-align: right;\">0.935686</td><td style=\"text-align: right;\"> 0.000829346</td></tr>\n",
       "<tr><td>objective_function_ad3c91f6</td><td>TERMINATED</td><td>127.0.0.1:19116</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.393205 </td><td style=\"text-align: right;\">             0.371099 </td><td style=\"text-align: right;\">           1.34248</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">4.65911e-07</td><td style=\"text-align: right;\">0.00147239 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">       1263.5   </td><td style=\"text-align: right;\">0.959049</td><td style=\"text-align: right;\"> 0.000316566</td></tr>\n",
       "<tr><td>objective_function_49b4a029</td><td>TERMINATED</td><td>127.0.0.1:24444</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.388644 </td><td style=\"text-align: right;\">             0.0658427</td><td style=\"text-align: right;\">           1.25935</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">5.64419e-07</td><td style=\"text-align: right;\">0.000384921</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        418.823 </td><td style=\"text-align: right;\">0.930808</td><td style=\"text-align: right;\"> 0.0020059  </td></tr>\n",
       "<tr><td>objective_function_d69f9070</td><td>TERMINATED</td><td>127.0.0.1:20320</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.324293 </td><td style=\"text-align: right;\">             0.332663 </td><td style=\"text-align: right;\">           1.96974</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.18182e-07</td><td style=\"text-align: right;\">0.000806365</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">        756.445 </td><td style=\"text-align: right;\">0.944721</td><td style=\"text-align: right;\"> 7.94059e-05</td></tr>\n",
       "<tr><td>objective_function_5a99338d</td><td>TERMINATED</td><td>127.0.0.1:26344</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.389308 </td><td style=\"text-align: right;\">             0.388779 </td><td style=\"text-align: right;\">           1.3396 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">4.29248e-07</td><td style=\"text-align: right;\">0.00165712 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        440.977 </td><td style=\"text-align: right;\">0.923043</td><td style=\"text-align: right;\"> 0.00208724 </td></tr>\n",
       "<tr><td>objective_function_2771abdb</td><td>TERMINATED</td><td>127.0.0.1:8356 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.31968  </td><td style=\"text-align: right;\">             0.441787 </td><td style=\"text-align: right;\">           2.03788</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">2.25743e-06</td><td style=\"text-align: right;\">0.000871964</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        815.544 </td><td style=\"text-align: right;\">0.951299</td><td style=\"text-align: right;\"> 0.000124352</td></tr>\n",
       "<tr><td>objective_function_f15fe7a6</td><td>TERMINATED</td><td>127.0.0.1:27836</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.427947 </td><td style=\"text-align: right;\">             0.477011 </td><td style=\"text-align: right;\">           2.78309</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.23215e-06</td><td style=\"text-align: right;\">0.000170753</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">        773.205 </td><td style=\"text-align: right;\">0.950379</td><td style=\"text-align: right;\"> 0.000126571</td></tr>\n",
       "<tr><td>objective_function_d31abf75</td><td>TERMINATED</td><td>127.0.0.1:2620 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.311742 </td><td style=\"text-align: right;\">             0.332916 </td><td style=\"text-align: right;\">           1.99456</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.97713e-07</td><td style=\"text-align: right;\">0.000933189</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        232.488 </td><td style=\"text-align: right;\">0.926437</td><td style=\"text-align: right;\"> 0.00143657 </td></tr>\n",
       "<tr><td>objective_function_2e8d41c6</td><td>TERMINATED</td><td>127.0.0.1:26836</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.309642 </td><td style=\"text-align: right;\">             0.320351 </td><td style=\"text-align: right;\">           2.14068</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.32125e-06</td><td style=\"text-align: right;\">0.00100219 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        220.2   </td><td style=\"text-align: right;\">0.928698</td><td style=\"text-align: right;\"> 0.00145754 </td></tr>\n",
       "<tr><td>objective_function_de1a0579</td><td>TERMINATED</td><td>127.0.0.1:24412</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.430605 </td><td style=\"text-align: right;\">             0.496014 </td><td style=\"text-align: right;\">           1.67788</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.29595e-07</td><td style=\"text-align: right;\">0.000169897</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">        719.045 </td><td style=\"text-align: right;\">0.940219</td><td style=\"text-align: right;\"> 0.000455028</td></tr>\n",
       "<tr><td>objective_function_922b5eea</td><td>TERMINATED</td><td>127.0.0.1:27832</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.424844 </td><td style=\"text-align: right;\">             0.467765 </td><td style=\"text-align: right;\">           2.14778</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.48723e-06</td><td style=\"text-align: right;\">0.000397779</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        213.301 </td><td style=\"text-align: right;\">0.930729</td><td style=\"text-align: right;\"> 0.0017993  </td></tr>\n",
       "<tr><td>objective_function_e11b18ae</td><td>TERMINATED</td><td>127.0.0.1:26132</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.425979 </td><td style=\"text-align: right;\">             0.475378 </td><td style=\"text-align: right;\">           2.86911</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.10283e-07</td><td style=\"text-align: right;\">0.000164729</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        200.393 </td><td style=\"text-align: right;\">0.925804</td><td style=\"text-align: right;\"> 0.00236189 </td></tr>\n",
       "<tr><td>objective_function_3e03c42a</td><td>TERMINATED</td><td>127.0.0.1:28792</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.351178 </td><td style=\"text-align: right;\">             0.359125 </td><td style=\"text-align: right;\">           1.54222</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">2.94364e-07</td><td style=\"text-align: right;\">0.000802496</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">       1465.08  </td><td style=\"text-align: right;\">0.952783</td><td style=\"text-align: right;\"> 0.000269433</td></tr>\n",
       "<tr><td>objective_function_7c44e3ef</td><td>TERMINATED</td><td>127.0.0.1:15908</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.436392 </td><td style=\"text-align: right;\">             0.488976 </td><td style=\"text-align: right;\">           2.98355</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.94929e-07</td><td style=\"text-align: right;\">0.000156944</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        211.209 </td><td style=\"text-align: right;\">0.924873</td><td style=\"text-align: right;\"> 0.00215738 </td></tr>\n",
       "<tr><td>objective_function_b0dc6195</td><td>TERMINATED</td><td>127.0.0.1:22648</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.349255 </td><td style=\"text-align: right;\">             0.386836 </td><td style=\"text-align: right;\">           1.73518</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.02661e-07</td><td style=\"text-align: right;\">0.000638639</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        204.35  </td><td style=\"text-align: right;\">0.931324</td><td style=\"text-align: right;\"> 0.00193685 </td></tr>\n",
       "<tr><td>objective_function_203085d4</td><td>TERMINATED</td><td>127.0.0.1:8680 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.492483 </td><td style=\"text-align: right;\">             0.237479 </td><td style=\"text-align: right;\">           1.01775</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">6.18085e-07</td><td style=\"text-align: right;\">0.000473913</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">       3269.87  </td><td style=\"text-align: right;\">0.941581</td><td style=\"text-align: right;\"> 0.000269942</td></tr>\n",
       "<tr><td>objective_function_fb082283</td><td>TERMINATED</td><td>127.0.0.1:9324 </td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.362094 </td><td style=\"text-align: right;\">             0.339118 </td><td style=\"text-align: right;\">           1.66919</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.26238e-08</td><td style=\"text-align: right;\">0.000572558</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">        911.715 </td><td style=\"text-align: right;\">0.930032</td><td style=\"text-align: right;\"> 0.000751081</td></tr>\n",
       "<tr><td>objective_function_f4edc62b</td><td>TERMINATED</td><td>127.0.0.1:23672</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.339893 </td><td style=\"text-align: right;\">             0.353181 </td><td style=\"text-align: right;\">           2.34128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">4.71801e-08</td><td style=\"text-align: right;\">0.000659515</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        237.316 </td><td style=\"text-align: right;\">0.930748</td><td style=\"text-align: right;\"> 0.00132515 </td></tr>\n",
       "<tr><td>objective_function_160b4b36</td><td>TERMINATED</td><td>127.0.0.1:23456</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.36726  </td><td style=\"text-align: right;\">             0.251497 </td><td style=\"text-align: right;\">           1.51104</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.20351e-06</td><td style=\"text-align: right;\">0.000653791</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">       2493.71  </td><td style=\"text-align: right;\">0.942069</td><td style=\"text-align: right;\"> 0.000632236</td></tr>\n",
       "<tr><td>objective_function_2ab1ff81</td><td>TERMINATED</td><td>127.0.0.1:26004</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.375122 </td><td style=\"text-align: right;\">             0.362071 </td><td style=\"text-align: right;\">           1.61298</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.80658e-07</td><td style=\"text-align: right;\">0.000646653</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        907.429 </td><td style=\"text-align: right;\">0.953774</td><td style=\"text-align: right;\"> 1.56561e-05</td></tr>\n",
       "<tr><td>objective_function_9cfd0617</td><td>TERMINATED</td><td>127.0.0.1:11116</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.380197 </td><td style=\"text-align: right;\">             0.229444 </td><td style=\"text-align: right;\">           1.59706</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">2.87691e-07</td><td style=\"text-align: right;\">0.00060541 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        900.559 </td><td style=\"text-align: right;\">0.954221</td><td style=\"text-align: right;\"> 0.000130043</td></tr>\n",
       "<tr><td>objective_function_e242a76e</td><td>TERMINATED</td><td>127.0.0.1:7952 </td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.39475  </td><td style=\"text-align: right;\">             0.282369 </td><td style=\"text-align: right;\">           1.57528</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.14206e-06</td><td style=\"text-align: right;\">0.000624274</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        882.461 </td><td style=\"text-align: right;\">0.926108</td><td style=\"text-align: right;\"> 0.0011323  </td></tr>\n",
       "<tr><td>objective_function_f0b00525</td><td>TERMINATED</td><td>127.0.0.1:11076</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.458666 </td><td style=\"text-align: right;\">             0.21921  </td><td style=\"text-align: right;\">           1.04699</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">9.92065e-07</td><td style=\"text-align: right;\">0.000458954</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        976.17  </td><td style=\"text-align: right;\">0.929044</td><td style=\"text-align: right;\"> 0.00209751 </td></tr>\n",
       "<tr><td>objective_function_461b10b6</td><td>TERMINATED</td><td>127.0.0.1:6224 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.4978   </td><td style=\"text-align: right;\">             0.285447 </td><td style=\"text-align: right;\">           2.6621 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.28289e-06</td><td style=\"text-align: right;\">0.000274645</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        833.003 </td><td style=\"text-align: right;\">0.928509</td><td style=\"text-align: right;\"> 0.00107957 </td></tr>\n",
       "<tr><td>objective_function_dcc3dca9</td><td>TERMINATED</td><td>127.0.0.1:13348</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.282907 </td><td style=\"text-align: right;\">             0.169512 </td><td style=\"text-align: right;\">           1.05714</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">9.81878e-08</td><td style=\"text-align: right;\">0.00487091 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">       1115     </td><td style=\"text-align: right;\">0.934884</td><td style=\"text-align: right;\"> 0.00110554 </td></tr>\n",
       "<tr><td>objective_function_f0494029</td><td>TERMINATED</td><td>127.0.0.1:20660</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.49931  </td><td style=\"text-align: right;\">             0.212475 </td><td style=\"text-align: right;\">           1.01711</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">7.38478e-07</td><td style=\"text-align: right;\">0.000324765</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        443.54  </td><td style=\"text-align: right;\">0.929199</td><td style=\"text-align: right;\"> 0.00327315 </td></tr>\n",
       "<tr><td>objective_function_6ed0ce5c</td><td>TERMINATED</td><td>127.0.0.1:1312 </td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.122957 </td><td style=\"text-align: right;\">             0.150598 </td><td style=\"text-align: right;\">           3.92413</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">2.61313e-07</td><td style=\"text-align: right;\">0.000192364</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        443.762 </td><td style=\"text-align: right;\">0.925285</td><td style=\"text-align: right;\"> 0.00082257 </td></tr>\n",
       "<tr><td>objective_function_1745c642</td><td>TERMINATED</td><td>127.0.0.1:23940</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.261309 </td><td style=\"text-align: right;\">             0.141585 </td><td style=\"text-align: right;\">           2.11257</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">1.21137e-08</td><td style=\"text-align: right;\">0.0002897  </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        200.874 </td><td style=\"text-align: right;\">0.9262  </td><td style=\"text-align: right;\"> 0.00169739 </td></tr>\n",
       "<tr><td>objective_function_a7600fad</td><td>TERMINATED</td><td>127.0.0.1:28952</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.301396 </td><td style=\"text-align: right;\">             0.297509 </td><td style=\"text-align: right;\">           3.49714</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">5.93586e-06</td><td style=\"text-align: right;\">0.00714252 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        530.818 </td><td style=\"text-align: right;\">0.934149</td><td style=\"text-align: right;\"> 0.000511316</td></tr>\n",
       "<tr><td>objective_function_a25855ce</td><td>TERMINATED</td><td>127.0.0.1:13684</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.208002 </td><td style=\"text-align: right;\">             0.0950245</td><td style=\"text-align: right;\">           2.45711</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">8.12654e-08</td><td style=\"text-align: right;\">0.0028719  </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        592.309 </td><td style=\"text-align: right;\">0.918379</td><td style=\"text-align: right;\"> 0.00136631 </td></tr>\n",
       "<tr><td>objective_function_42261600</td><td>TERMINATED</td><td>127.0.0.1:28404</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.113821 </td><td style=\"text-align: right;\">             0.0250154</td><td style=\"text-align: right;\">           1.81091</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">8.81355e-06</td><td style=\"text-align: right;\">0.00134642 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        441.942 </td><td style=\"text-align: right;\">0.931023</td><td style=\"text-align: right;\"> 0.00135972 </td></tr>\n",
       "<tr><td>objective_function_14d0e766</td><td>TERMINATED</td><td>127.0.0.1:8108 </td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.0667215</td><td style=\"text-align: right;\">             0.432151 </td><td style=\"text-align: right;\">           3.2266 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.52171e-08</td><td style=\"text-align: right;\">0.00231892 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        271.158 </td><td style=\"text-align: right;\">0.929136</td><td style=\"text-align: right;\"> 0.000984339</td></tr>\n",
       "<tr><td>objective_function_fd60dfc9</td><td>TERMINATED</td><td>127.0.0.1:28340</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.411054 </td><td style=\"text-align: right;\">             0.101429 </td><td style=\"text-align: right;\">           1.01521</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">4.84469e-07</td><td style=\"text-align: right;\">0.000290947</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">       1991.65  </td><td style=\"text-align: right;\">0.952139</td><td style=\"text-align: right;\"> 3.3134e-06 </td></tr>\n",
       "<tr><td>objective_function_4c035a9a</td><td>TERMINATED</td><td>127.0.0.1:28336</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.240339 </td><td style=\"text-align: right;\">             0.108127 </td><td style=\"text-align: right;\">           1.2266 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">2.4634e-06 </td><td style=\"text-align: right;\">0.00120171 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">        428.877 </td><td style=\"text-align: right;\">0.927985</td><td style=\"text-align: right;\"> 0.000703247</td></tr>\n",
       "<tr><td>objective_function_c3d12d9c</td><td>TERMINATED</td><td>127.0.0.1:28880</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.11005  </td><td style=\"text-align: right;\">             0.0868838</td><td style=\"text-align: right;\">           3.8793 </td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">4.65424e-07</td><td style=\"text-align: right;\">0.00197052 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">       1916.24  </td><td style=\"text-align: right;\">0.939972</td><td style=\"text-align: right;\"> 5.59302e-06</td></tr>\n",
       "<tr><td>objective_function_cfd60f5f</td><td>TERMINATED</td><td>127.0.0.1:13244</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.262787 </td><td style=\"text-align: right;\">             0.395984 </td><td style=\"text-align: right;\">           1.95412</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">9.60256e-08</td><td style=\"text-align: right;\">0.000105068</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">        498.596 </td><td style=\"text-align: right;\">0.921401</td><td style=\"text-align: right;\"> 0.00394363 </td></tr>\n",
       "<tr><td>objective_function_e07fce51</td><td>TERMINATED</td><td>127.0.0.1:26844</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.39683  </td><td style=\"text-align: right;\">             0.106028 </td><td style=\"text-align: right;\">           3.77222</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">6.69838e-06</td><td style=\"text-align: right;\">0.00528106 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">       1516.84  </td><td style=\"text-align: right;\">0.932318</td><td style=\"text-align: right;\"> 0.00062097 </td></tr>\n",
       "<tr><td>objective_function_ac443a83</td><td>TERMINATED</td><td>127.0.0.1:25136</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.294282 </td><td style=\"text-align: right;\">             0.173271 </td><td style=\"text-align: right;\">           1.42869</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">2.98509e-07</td><td style=\"text-align: right;\">0.00105039 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        855.611 </td><td style=\"text-align: right;\">0.944905</td><td style=\"text-align: right;\"> 0.000197104</td></tr>\n",
       "<tr><td>objective_function_977eb3e1</td><td>TERMINATED</td><td>127.0.0.1:6348 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">                128</td><td style=\"text-align: right;\">     0.203937 </td><td style=\"text-align: right;\">             0.0316637</td><td style=\"text-align: right;\">           1.30461</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.46183e-07</td><td style=\"text-align: right;\">0.00682176 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">        452.584 </td><td style=\"text-align: right;\">0.932009</td><td style=\"text-align: right;\"> 0.0022714  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 09:48:21,034\tINFO tune_controller.py:444 -- Restoring the run from the latest experiment state file: experiment_state-2025-11-17_09-07-44.json\n",
      "2025-11-17 09:49:23,934\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-11-17 09:49:23,981\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/Karim Negm/ray_results/pirate_pain_focalloss_search_v13_final' in 0.0368s.\n",
      "2025-11-17 09:49:30,843\tINFO tune.py:1041 -- Total run time: 69.87 seconds (62.67 seconds for the tuning loop).\n",
      "2025-11-17 09:49:30,845\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2025-11-17 09:49:31,307\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- objective_function_cec0c729: FileNotFoundError('Could not fetch metrics for objective_function_cec0c729: both result.json and progress.csv were not found at C:/Users/Karim Negm/ray_results/pirate_pain_focalloss_search_v13_final/objective_function_cec0c729')\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"rnn_type\": tune.choice(['GRU', 'LSTM']),\n",
    "    \"focal_loss_gamma\": tune.uniform(1.0, 4.0),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"batch_size\": tune.choice([64, 128]),\n",
    "    \"hidden_size\": tune.choice([256, 384, 512]), \n",
    "    \"num_layers\": tune.choice([2, 3]),\n",
    "    \"dropout_rate\": tune.uniform(0, 0.5), \n",
    "    \"feature_dropout_rate\": tune.uniform(0, 0.5),\n",
    "    \"bidirectional\": tune.choice([True, False]), \n",
    "    \"l2_lambda\": tune.loguniform(1e-8, 1e-5),\n",
    "    \"conv_out_channels\": tune.choice([128]), \n",
    "    \"conv_kernel_size\": tune.choice([5])\n",
    "}\n",
    "\n",
    "def short_trial_name(trial): return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "if ray.is_initialized(): ray.shutdown()\n",
    "ray.init(num_cpus=os.cpu_count(), num_gpus=1, ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "print(\"--- Starting HPO with Focal Loss and RNN-Type search ---\")\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(objective_function, \n",
    "                         X_train_w=X_train_w_torch, y_train_w=y_train_w_torch,\n",
    "                         X_val_w=X_val_w_torch, y_val_w=y_val_w_torch,\n",
    "                         alpha_tensor=alpha_tensor),\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 0.25},\n",
    "    config=search_space, \n",
    "    num_samples=0, \n",
    "    search_alg=OptunaSearch(metric=\"val_f1\", mode=\"max\"),\n",
    "    scheduler=ASHAScheduler(metric=\"val_f1\", mode=\"max\", grace_period=25, reduction_factor=2),\n",
    "    name=\"pirate_pain_focalloss_search_v13_final\", \n",
    "    verbose=1,\n",
    "    trial_dirname_creator=short_trial_name,\n",
    "    resume=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0bbf60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 09:49:44,863\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- objective_function_cec0c729: FileNotFoundError('Could not fetch metrics for objective_function_cec0c729: both result.json and progress.csv were not found at C:/Users/Karim Negm/ray_results/pirate_pain_focalloss_search_v13_final/objective_function_cec0c729')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading HPO Search Results ---\n",
      "Loading analysis from: C:\\Users\\Karim Negm/ray_results/pirate_pain_focalloss_search_v13_final\n",
      "Best validation F1 score from completed trials: 0.9421\n",
      "Best hyperparameters found:\n",
      "{'rnn_type': 'GRU', 'focal_loss_gamma': 1.51103656149311, 'lr': 0.00065379125036077, 'batch_size': 64, 'hidden_size': 384, 'num_layers': 3, 'dropout_rate': 0.36726022005612846, 'feature_dropout_rate': 0.2514972946679169, 'bidirectional': True, 'l2_lambda': 1.2035123100344062e-06, 'conv_out_channels': 128, 'conv_kernel_size': 5}\n",
      "\n",
      "--- Ready to proceed to K-Fold Training ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Loading HPO Search Results ---\")\n",
    "\n",
    "experiment_path = os.path.expanduser(\"~/ray_results/pirate_pain_focalloss_search_v13_final\")\n",
    "print(f\"Loading analysis from: {experiment_path}\")\n",
    "try:\n",
    "    # Note: If you re-run this notebook, the experiment name might increment (e.g., _v13_final_2).\n",
    "    # Make sure the path matches the one output by the tune.run() command.\n",
    "    analysis = tune.ExperimentAnalysis(experiment_path)\n",
    "    best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "    \n",
    "    if best_trial:\n",
    "        FINAL_CONFIG = best_trial.config\n",
    "        FINAL_BEST_VAL_F1 = best_trial.last_result.get(\"val_f1\", 0.0) \n",
    "        print(f\"Best validation F1 score from completed trials: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "        print(\"Best hyperparameters found:\")\n",
    "        print(FINAL_CONFIG)\n",
    "    else:\n",
    "        print(\"ERROR: No trials completed successfully. Using a default config.\")\n",
    "        raise ValueError(\"HPO run failed to produce a valid trial.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nWARNING: Could not load analysis from {experiment_path}. The error was: {e}\")\n",
    "    print(\"\\n--- USING FALLBACK CONFIGURATION ---\")\n",
    "    # This is a strong configuration from a previous run, adjusted for final strategy.\n",
    "    FINAL_CONFIG = {'rnn_type': 'GRU', 'focal_loss_gamma': 2.8, 'lr': 0.0015, 'batch_size': 128, 'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.35, 'feature_dropout_rate': 0.33, 'bidirectional': False, 'l2_lambda': 4.2e-06, 'conv_out_channels': 128, 'conv_kernel_size': 5}\n",
    "    FINAL_BEST_VAL_F1 = 0.95 # Placeholder value\n",
    "    print(\"Best hyperparameters (fallback):\")\n",
    "    print(FINAL_CONFIG)\n",
    "\n",
    "\n",
    "# Add the fixed windowing params to the final config for the next steps\n",
    "FINAL_CONFIG['window_size'] = WINDOW_SIZE\n",
    "FINAL_CONFIG['stride'] = STRIDE\n",
    "\n",
    "# Clean up HPO data to save memory\n",
    "try:\n",
    "    del X_train_w_torch, y_train_w_torch, X_val_w_torch, y_val_w_torch\n",
    "    del X_train_split, y_train_split, X_val_split, y_val_split\n",
    "except NameError:\n",
    "    print(\"Data already cleaned up or not in memory.\")\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n--- Ready to proceed to K-Fold Training ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c0bb8",
   "metadata": {},
   "source": [
    "## üèÜ 6. Phase 2: K-Fold Ensemble Training with Final Strategy\n",
    "\n",
    "This is the main training block. It incorporates the best hyperparameters from the HPO search along with our new strategies for handling class imbalance:\n",
    "1.  **WeightedRandomSampler**: A sampler is created for each fold's training data to ensure balanced batches.\n",
    "2.  **Noise Augmentation**: The training data for each fold is augmented by adding noisy copies of the minority class samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13308e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèÜ Final Configuration Set --- \n",
      "Best Val F1 from HPO search: 0.9421\n",
      "{'rnn_type': 'GRU', 'focal_loss_gamma': 1.51103656149311, 'lr': 0.00065379125036077, 'batch_size': 64, 'hidden_size': 384, 'num_layers': 3, 'dropout_rate': 0.36726022005612846, 'feature_dropout_rate': 0.2514972946679169, 'bidirectional': True, 'l2_lambda': 1.2035123100344062e-06, 'conv_out_channels': 128, 'conv_kernel_size': 5, 'window_size': 10, 'stride': 2}\n",
      "Submission name will be: submission_FinalStrategy-GRU_H384_L3_C128_K5_v13.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"--- üèÜ Final Configuration Set --- \")\n",
    "print(f\"Best Val F1 from HPO search: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "print(FINAL_CONFIG)\n",
    "\n",
    "N_SPLITS = 5\n",
    "FINAL_EXPERIMENT_NAME = f\"FinalStrategy-{FINAL_CONFIG['rnn_type']}_H{FINAL_CONFIG['hidden_size']}_L{FINAL_CONFIG['num_layers']}_\"\\\n",
    "                      f\"C{FINAL_CONFIG['conv_out_channels']}_K{FINAL_CONFIG['conv_kernel_size']}_v13\"\n",
    "submission_filename_base = f\"submission_{FINAL_EXPERIMENT_NAME}.csv\"\n",
    "print(f\"Submission name will be: {submission_filename_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bda55dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/5 --- (FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_1) ---\n",
      "Augmenting minority classes... Original sample count: 40128\n",
      "Augmentation complete. New sample count: 113696\n",
      "Creating WeightedRandomSampler for the training loader...\n",
      "--- Starting Training: FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_1 ---\n",
      "Epoch   3/150 | Best Val F1: 0.9391 | Val F1: 0.9391 | LR: 0.000033\n",
      "Epoch   6/150 | Best Val F1: 0.9391 | Val F1: 0.9381 | LR: 0.000053\n",
      "Epoch   9/150 | Best Val F1: 0.9391 | Val F1: 0.9373 | LR: 0.000086\n",
      "Epoch  12/150 | Best Val F1: 0.9412 | Val F1: 0.9310 | LR: 0.000130\n",
      "Epoch  15/150 | Best Val F1: 0.9427 | Val F1: 0.9354 | LR: 0.000183\n",
      "Epoch  18/150 | Best Val F1: 0.9427 | Val F1: 0.9379 | LR: 0.000243\n",
      "Epoch  21/150 | Best Val F1: 0.9427 | Val F1: 0.9240 | LR: 0.000307\n",
      "Epoch  24/150 | Best Val F1: 0.9427 | Val F1: 0.9334 | LR: 0.000373\n",
      "Epoch  27/150 | Best Val F1: 0.9427 | Val F1: 0.9302 | LR: 0.000437\n",
      "Epoch  30/150 | Best Val F1: 0.9427 | Val F1: 0.9386 | LR: 0.000497\n",
      "Epoch  33/150 | Best Val F1: 0.9427 | Val F1: 0.9341 | LR: 0.000550\n",
      "Epoch  36/150 | Best Val F1: 0.9427 | Val F1: 0.9425 | LR: 0.000594\n",
      "Epoch  39/150 | Best Val F1: 0.9427 | Val F1: 0.9399 | LR: 0.000627\n",
      "Epoch  42/150 | Best Val F1: 0.9427 | Val F1: 0.9339 | LR: 0.000647\n",
      "Epoch  45/150 | Best Val F1: 0.9446 | Val F1: 0.9385 | LR: 0.000654\n",
      "Epoch  48/150 | Best Val F1: 0.9446 | Val F1: 0.9351 | LR: 0.000652\n",
      "Epoch  51/150 | Best Val F1: 0.9446 | Val F1: 0.9352 | LR: 0.000649\n",
      "Epoch  54/150 | Best Val F1: 0.9446 | Val F1: 0.9385 | LR: 0.000642\n",
      "Epoch  57/150 | Best Val F1: 0.9446 | Val F1: 0.9397 | LR: 0.000633\n",
      "Epoch  60/150 | Best Val F1: 0.9446 | Val F1: 0.9348 | LR: 0.000621\n",
      "Epoch  63/150 | Best Val F1: 0.9446 | Val F1: 0.9401 | LR: 0.000608\n",
      "Epoch  66/150 | Best Val F1: 0.9446 | Val F1: 0.9314 | LR: 0.000591\n",
      "Epoch  69/150 | Best Val F1: 0.9446 | Val F1: 0.9313 | LR: 0.000573\n",
      "Epoch  72/150 | Best Val F1: 0.9446 | Val F1: 0.9335 | LR: 0.000553\n",
      "Epoch  75/150 | Best Val F1: 0.9446 | Val F1: 0.9342 | LR: 0.000531\n",
      "Epoch  78/150 | Best Val F1: 0.9446 | Val F1: 0.9319 | LR: 0.000507\n",
      "Epoch  81/150 | Best Val F1: 0.9446 | Val F1: 0.9310 | LR: 0.000482\n",
      "Epoch  84/150 | Best Val F1: 0.9446 | Val F1: 0.9314 | LR: 0.000455\n",
      "Epoch  87/150 | Best Val F1: 0.9446 | Val F1: 0.9300 | LR: 0.000428\n",
      "Epoch  90/150 | Best Val F1: 0.9446 | Val F1: 0.9317 | LR: 0.000400\n",
      "Epoch  93/150 | Best Val F1: 0.9446 | Val F1: 0.9324 | LR: 0.000371\n",
      "Epoch  96/150 | Best Val F1: 0.9446 | Val F1: 0.9373 | LR: 0.000342\n",
      "Epoch  99/150 | Best Val F1: 0.9446 | Val F1: 0.9355 | LR: 0.000312\n",
      "Epoch 102/150 | Best Val F1: 0.9446 | Val F1: 0.9341 | LR: 0.000283\n",
      "Epoch 105/150 | Best Val F1: 0.9446 | Val F1: 0.9309 | LR: 0.000254\n",
      "Epoch 108/150 | Best Val F1: 0.9446 | Val F1: 0.9309 | LR: 0.000226\n",
      "Epoch 111/150 | Best Val F1: 0.9446 | Val F1: 0.9324 | LR: 0.000198\n",
      "Early stopping at epoch 114. Best F1: 0.9446\n",
      "--- Finished Training --- Best F1: 0.9446\n",
      "Fold 1 Final Val F1: 0.9446\n",
      "\n",
      "--- Fold 2/5 --- (FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_2) ---\n",
      "Augmenting minority classes... Original sample count: 40204\n",
      "Augmentation complete. New sample count: 113772\n",
      "Creating WeightedRandomSampler for the training loader...\n",
      "--- Starting Training: FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_2 ---\n",
      "Epoch   3/150 | Best Val F1: 0.9118 | Val F1: 0.9118 | LR: 0.000033\n",
      "Epoch   6/150 | Best Val F1: 0.9188 | Val F1: 0.9188 | LR: 0.000053\n",
      "Epoch   9/150 | Best Val F1: 0.9212 | Val F1: 0.9160 | LR: 0.000086\n",
      "Epoch  12/150 | Best Val F1: 0.9225 | Val F1: 0.9188 | LR: 0.000130\n",
      "Epoch  15/150 | Best Val F1: 0.9302 | Val F1: 0.9302 | LR: 0.000183\n",
      "Epoch  18/150 | Best Val F1: 0.9302 | Val F1: 0.9178 | LR: 0.000243\n",
      "Epoch  21/150 | Best Val F1: 0.9302 | Val F1: 0.9288 | LR: 0.000307\n",
      "Epoch  24/150 | Best Val F1: 0.9302 | Val F1: 0.9266 | LR: 0.000373\n",
      "Epoch  27/150 | Best Val F1: 0.9302 | Val F1: 0.9257 | LR: 0.000437\n",
      "Epoch  30/150 | Best Val F1: 0.9302 | Val F1: 0.9276 | LR: 0.000497\n",
      "Epoch  33/150 | Best Val F1: 0.9302 | Val F1: 0.9281 | LR: 0.000550\n",
      "Epoch  36/150 | Best Val F1: 0.9302 | Val F1: 0.9283 | LR: 0.000594\n",
      "Epoch  39/150 | Best Val F1: 0.9302 | Val F1: 0.9232 | LR: 0.000627\n",
      "Epoch  42/150 | Best Val F1: 0.9302 | Val F1: 0.9267 | LR: 0.000647\n",
      "Epoch  45/150 | Best Val F1: 0.9302 | Val F1: 0.9197 | LR: 0.000654\n",
      "Epoch  48/150 | Best Val F1: 0.9302 | Val F1: 0.9240 | LR: 0.000652\n",
      "Epoch  51/150 | Best Val F1: 0.9302 | Val F1: 0.9179 | LR: 0.000649\n",
      "Epoch  54/150 | Best Val F1: 0.9302 | Val F1: 0.9238 | LR: 0.000642\n",
      "Epoch  57/150 | Best Val F1: 0.9302 | Val F1: 0.9236 | LR: 0.000633\n",
      "Epoch  60/150 | Best Val F1: 0.9302 | Val F1: 0.9212 | LR: 0.000621\n",
      "Epoch  63/150 | Best Val F1: 0.9302 | Val F1: 0.9233 | LR: 0.000608\n",
      "Epoch  66/150 | Best Val F1: 0.9302 | Val F1: 0.9113 | LR: 0.000591\n",
      "Epoch  69/150 | Best Val F1: 0.9302 | Val F1: 0.9198 | LR: 0.000573\n",
      "Epoch  72/150 | Best Val F1: 0.9302 | Val F1: 0.9229 | LR: 0.000553\n",
      "Epoch  75/150 | Best Val F1: 0.9302 | Val F1: 0.9182 | LR: 0.000531\n",
      "Epoch  78/150 | Best Val F1: 0.9302 | Val F1: 0.9232 | LR: 0.000507\n",
      "Epoch  81/150 | Best Val F1: 0.9302 | Val F1: 0.9170 | LR: 0.000482\n",
      "Epoch  84/150 | Best Val F1: 0.9302 | Val F1: 0.9207 | LR: 0.000455\n",
      "Early stopping at epoch 85. Best F1: 0.9302\n",
      "--- Finished Training --- Best F1: 0.9302\n",
      "Fold 2 Final Val F1: 0.9302\n",
      "\n",
      "--- Fold 3/5 --- (FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_3) ---\n",
      "Augmenting minority classes... Original sample count: 40204\n",
      "Augmentation complete. New sample count: 113772\n",
      "Creating WeightedRandomSampler for the training loader...\n",
      "--- Starting Training: FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_3 ---\n",
      "Epoch   3/150 | Best Val F1: 0.9080 | Val F1: 0.9080 | LR: 0.000033\n",
      "Epoch   6/150 | Best Val F1: 0.9131 | Val F1: 0.9131 | LR: 0.000053\n",
      "Epoch   9/150 | Best Val F1: 0.9225 | Val F1: 0.9225 | LR: 0.000086\n",
      "Epoch  12/150 | Best Val F1: 0.9225 | Val F1: 0.9175 | LR: 0.000130\n",
      "Epoch  15/150 | Best Val F1: 0.9299 | Val F1: 0.9273 | LR: 0.000183\n",
      "Epoch  18/150 | Best Val F1: 0.9299 | Val F1: 0.9225 | LR: 0.000243\n",
      "Epoch  21/150 | Best Val F1: 0.9299 | Val F1: 0.9208 | LR: 0.000307\n",
      "Epoch  24/150 | Best Val F1: 0.9299 | Val F1: 0.9158 | LR: 0.000373\n",
      "Epoch  27/150 | Best Val F1: 0.9306 | Val F1: 0.9159 | LR: 0.000437\n",
      "Epoch  30/150 | Best Val F1: 0.9345 | Val F1: 0.9345 | LR: 0.000497\n",
      "Epoch  33/150 | Best Val F1: 0.9345 | Val F1: 0.9299 | LR: 0.000550\n",
      "Epoch  36/150 | Best Val F1: 0.9393 | Val F1: 0.9393 | LR: 0.000594\n",
      "Epoch  39/150 | Best Val F1: 0.9393 | Val F1: 0.9245 | LR: 0.000627\n",
      "Epoch  42/150 | Best Val F1: 0.9393 | Val F1: 0.9328 | LR: 0.000647\n",
      "Epoch  45/150 | Best Val F1: 0.9393 | Val F1: 0.9264 | LR: 0.000654\n",
      "Epoch  48/150 | Best Val F1: 0.9393 | Val F1: 0.9381 | LR: 0.000652\n",
      "Epoch  51/150 | Best Val F1: 0.9393 | Val F1: 0.9361 | LR: 0.000649\n",
      "Epoch  54/150 | Best Val F1: 0.9393 | Val F1: 0.9298 | LR: 0.000642\n",
      "Epoch  57/150 | Best Val F1: 0.9393 | Val F1: 0.9270 | LR: 0.000633\n",
      "Epoch  60/150 | Best Val F1: 0.9393 | Val F1: 0.9265 | LR: 0.000621\n",
      "Epoch  63/150 | Best Val F1: 0.9393 | Val F1: 0.9286 | LR: 0.000608\n",
      "Epoch  66/150 | Best Val F1: 0.9393 | Val F1: 0.9262 | LR: 0.000591\n",
      "Epoch  69/150 | Best Val F1: 0.9435 | Val F1: 0.9435 | LR: 0.000573\n",
      "Epoch  72/150 | Best Val F1: 0.9435 | Val F1: 0.9316 | LR: 0.000553\n",
      "Epoch  75/150 | Best Val F1: 0.9435 | Val F1: 0.9251 | LR: 0.000531\n",
      "Epoch  78/150 | Best Val F1: 0.9435 | Val F1: 0.9282 | LR: 0.000507\n",
      "Epoch  81/150 | Best Val F1: 0.9435 | Val F1: 0.9381 | LR: 0.000482\n",
      "Epoch  84/150 | Best Val F1: 0.9435 | Val F1: 0.9274 | LR: 0.000455\n",
      "Epoch  87/150 | Best Val F1: 0.9435 | Val F1: 0.9211 | LR: 0.000428\n",
      "Epoch  90/150 | Best Val F1: 0.9435 | Val F1: 0.9270 | LR: 0.000400\n",
      "Epoch  93/150 | Best Val F1: 0.9435 | Val F1: 0.9277 | LR: 0.000371\n",
      "Epoch  96/150 | Best Val F1: 0.9435 | Val F1: 0.9254 | LR: 0.000342\n",
      "Epoch  99/150 | Best Val F1: 0.9435 | Val F1: 0.9214 | LR: 0.000312\n",
      "Epoch 102/150 | Best Val F1: 0.9435 | Val F1: 0.9234 | LR: 0.000283\n",
      "Epoch 105/150 | Best Val F1: 0.9435 | Val F1: 0.9279 | LR: 0.000254\n",
      "Epoch 108/150 | Best Val F1: 0.9435 | Val F1: 0.9267 | LR: 0.000226\n",
      "Epoch 111/150 | Best Val F1: 0.9435 | Val F1: 0.9231 | LR: 0.000198\n",
      "Epoch 114/150 | Best Val F1: 0.9435 | Val F1: 0.9273 | LR: 0.000172\n",
      "Epoch 117/150 | Best Val F1: 0.9435 | Val F1: 0.9228 | LR: 0.000147\n",
      "Epoch 120/150 | Best Val F1: 0.9435 | Val F1: 0.9239 | LR: 0.000123\n",
      "Epoch 123/150 | Best Val F1: 0.9435 | Val F1: 0.9259 | LR: 0.000101\n",
      "Epoch 126/150 | Best Val F1: 0.9435 | Val F1: 0.9256 | LR: 0.000081\n",
      "Epoch 129/150 | Best Val F1: 0.9435 | Val F1: 0.9243 | LR: 0.000062\n",
      "Epoch 132/150 | Best Val F1: 0.9435 | Val F1: 0.9236 | LR: 0.000046\n",
      "Epoch 135/150 | Best Val F1: 0.9435 | Val F1: 0.9238 | LR: 0.000032\n",
      "Epoch 138/150 | Best Val F1: 0.9435 | Val F1: 0.9249 | LR: 0.000021\n",
      "Early stopping at epoch 139. Best F1: 0.9435\n",
      "--- Finished Training --- Best F1: 0.9435\n",
      "Fold 3 Final Val F1: 0.9435\n",
      "\n",
      "--- Fold 4/5 --- (FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_4) ---\n",
      "Augmenting minority classes... Original sample count: 40204\n",
      "Augmentation complete. New sample count: 113772\n",
      "Creating WeightedRandomSampler for the training loader...\n",
      "--- Starting Training: FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_4 ---\n",
      "Epoch   3/150 | Best Val F1: 0.9308 | Val F1: 0.9308 | LR: 0.000033\n",
      "Epoch   6/150 | Best Val F1: 0.9308 | Val F1: 0.9206 | LR: 0.000053\n",
      "Epoch   9/150 | Best Val F1: 0.9308 | Val F1: 0.9152 | LR: 0.000086\n",
      "Epoch  12/150 | Best Val F1: 0.9342 | Val F1: 0.9279 | LR: 0.000130\n",
      "Epoch  15/150 | Best Val F1: 0.9342 | Val F1: 0.9179 | LR: 0.000183\n",
      "Epoch  18/150 | Best Val F1: 0.9342 | Val F1: 0.9293 | LR: 0.000243\n",
      "Epoch  21/150 | Best Val F1: 0.9342 | Val F1: 0.9197 | LR: 0.000307\n",
      "Epoch  24/150 | Best Val F1: 0.9344 | Val F1: 0.9325 | LR: 0.000373\n",
      "Epoch  27/150 | Best Val F1: 0.9344 | Val F1: 0.9263 | LR: 0.000437\n",
      "Epoch  30/150 | Best Val F1: 0.9344 | Val F1: 0.9266 | LR: 0.000497\n",
      "Epoch  33/150 | Best Val F1: 0.9359 | Val F1: 0.9359 | LR: 0.000550\n",
      "Epoch  36/150 | Best Val F1: 0.9359 | Val F1: 0.9305 | LR: 0.000594\n",
      "Epoch  39/150 | Best Val F1: 0.9359 | Val F1: 0.9267 | LR: 0.000627\n",
      "Epoch  42/150 | Best Val F1: 0.9359 | Val F1: 0.9281 | LR: 0.000647\n",
      "Epoch  45/150 | Best Val F1: 0.9359 | Val F1: 0.9256 | LR: 0.000654\n",
      "Epoch  48/150 | Best Val F1: 0.9359 | Val F1: 0.9186 | LR: 0.000652\n",
      "Epoch  51/150 | Best Val F1: 0.9359 | Val F1: 0.9213 | LR: 0.000649\n",
      "Epoch  54/150 | Best Val F1: 0.9359 | Val F1: 0.9295 | LR: 0.000642\n",
      "Epoch  57/150 | Best Val F1: 0.9359 | Val F1: 0.9284 | LR: 0.000633\n",
      "Epoch  60/150 | Best Val F1: 0.9359 | Val F1: 0.9323 | LR: 0.000621\n",
      "Epoch  63/150 | Best Val F1: 0.9359 | Val F1: 0.9326 | LR: 0.000608\n",
      "Epoch  66/150 | Best Val F1: 0.9359 | Val F1: 0.9267 | LR: 0.000591\n",
      "Epoch  69/150 | Best Val F1: 0.9359 | Val F1: 0.9266 | LR: 0.000573\n",
      "Epoch  72/150 | Best Val F1: 0.9359 | Val F1: 0.9238 | LR: 0.000553\n",
      "Epoch  75/150 | Best Val F1: 0.9359 | Val F1: 0.9278 | LR: 0.000531\n",
      "Epoch  78/150 | Best Val F1: 0.9359 | Val F1: 0.9252 | LR: 0.000507\n",
      "Epoch  81/150 | Best Val F1: 0.9359 | Val F1: 0.9325 | LR: 0.000482\n",
      "Epoch  84/150 | Best Val F1: 0.9359 | Val F1: 0.9250 | LR: 0.000455\n",
      "Epoch  87/150 | Best Val F1: 0.9359 | Val F1: 0.9268 | LR: 0.000428\n",
      "Epoch  90/150 | Best Val F1: 0.9359 | Val F1: 0.9293 | LR: 0.000400\n",
      "Epoch  93/150 | Best Val F1: 0.9359 | Val F1: 0.9299 | LR: 0.000371\n",
      "Epoch  96/150 | Best Val F1: 0.9359 | Val F1: 0.9348 | LR: 0.000342\n",
      "Epoch  99/150 | Best Val F1: 0.9359 | Val F1: 0.9352 | LR: 0.000312\n",
      "Epoch 102/150 | Best Val F1: 0.9359 | Val F1: 0.9316 | LR: 0.000283\n",
      "Early stopping at epoch 103. Best F1: 0.9359\n",
      "--- Finished Training --- Best F1: 0.9359\n",
      "Fold 4 Final Val F1: 0.9359\n",
      "\n",
      "--- Fold 5/5 --- (FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_5) ---\n",
      "Augmenting minority classes... Original sample count: 40204\n",
      "Augmentation complete. New sample count: 113772\n",
      "Creating WeightedRandomSampler for the training loader...\n",
      "--- Starting Training: FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_5 ---\n",
      "Epoch   3/150 | Best Val F1: 0.8933 | Val F1: 0.8835 | LR: 0.000033\n",
      "Epoch   6/150 | Best Val F1: 0.9065 | Val F1: 0.9065 | LR: 0.000053\n",
      "Epoch   9/150 | Best Val F1: 0.9065 | Val F1: 0.8962 | LR: 0.000086\n",
      "Epoch  12/150 | Best Val F1: 0.9065 | Val F1: 0.8979 | LR: 0.000130\n",
      "Epoch  15/150 | Best Val F1: 0.9065 | Val F1: 0.9033 | LR: 0.000183\n",
      "Epoch  18/150 | Best Val F1: 0.9086 | Val F1: 0.9061 | LR: 0.000243\n",
      "Epoch  21/150 | Best Val F1: 0.9086 | Val F1: 0.8864 | LR: 0.000307\n",
      "Epoch  24/150 | Best Val F1: 0.9086 | Val F1: 0.8979 | LR: 0.000373\n",
      "Epoch  27/150 | Best Val F1: 0.9086 | Val F1: 0.9008 | LR: 0.000437\n",
      "Epoch  30/150 | Best Val F1: 0.9113 | Val F1: 0.8974 | LR: 0.000497\n",
      "Epoch  33/150 | Best Val F1: 0.9113 | Val F1: 0.9049 | LR: 0.000550\n",
      "Epoch  36/150 | Best Val F1: 0.9113 | Val F1: 0.9045 | LR: 0.000594\n",
      "Epoch  39/150 | Best Val F1: 0.9113 | Val F1: 0.9068 | LR: 0.000627\n",
      "Epoch  42/150 | Best Val F1: 0.9113 | Val F1: 0.9039 | LR: 0.000647\n",
      "Epoch  45/150 | Best Val F1: 0.9113 | Val F1: 0.9091 | LR: 0.000654\n",
      "Epoch  48/150 | Best Val F1: 0.9113 | Val F1: 0.8985 | LR: 0.000652\n",
      "Epoch  51/150 | Best Val F1: 0.9113 | Val F1: 0.8946 | LR: 0.000649\n",
      "Epoch  54/150 | Best Val F1: 0.9113 | Val F1: 0.9037 | LR: 0.000642\n",
      "Epoch  57/150 | Best Val F1: 0.9126 | Val F1: 0.9126 | LR: 0.000633\n",
      "Epoch  60/150 | Best Val F1: 0.9126 | Val F1: 0.8946 | LR: 0.000621\n",
      "Epoch  63/150 | Best Val F1: 0.9126 | Val F1: 0.9007 | LR: 0.000608\n",
      "Epoch  66/150 | Best Val F1: 0.9126 | Val F1: 0.9054 | LR: 0.000591\n",
      "Epoch  69/150 | Best Val F1: 0.9126 | Val F1: 0.9052 | LR: 0.000573\n",
      "Epoch  72/150 | Best Val F1: 0.9126 | Val F1: 0.9009 | LR: 0.000553\n",
      "Epoch  75/150 | Best Val F1: 0.9126 | Val F1: 0.9016 | LR: 0.000531\n",
      "Epoch  78/150 | Best Val F1: 0.9126 | Val F1: 0.9049 | LR: 0.000507\n",
      "Epoch  81/150 | Best Val F1: 0.9126 | Val F1: 0.8945 | LR: 0.000482\n",
      "Epoch  84/150 | Best Val F1: 0.9126 | Val F1: 0.9022 | LR: 0.000455\n",
      "Epoch  87/150 | Best Val F1: 0.9126 | Val F1: 0.9114 | LR: 0.000428\n",
      "Epoch  90/150 | Best Val F1: 0.9126 | Val F1: 0.9047 | LR: 0.000400\n",
      "Epoch  93/150 | Best Val F1: 0.9126 | Val F1: 0.9013 | LR: 0.000371\n",
      "Epoch  96/150 | Best Val F1: 0.9126 | Val F1: 0.8986 | LR: 0.000342\n",
      "Epoch  99/150 | Best Val F1: 0.9126 | Val F1: 0.9020 | LR: 0.000312\n",
      "Epoch 102/150 | Best Val F1: 0.9126 | Val F1: 0.9008 | LR: 0.000283\n",
      "Epoch 105/150 | Best Val F1: 0.9126 | Val F1: 0.8984 | LR: 0.000254\n",
      "Epoch 108/150 | Best Val F1: 0.9126 | Val F1: 0.8888 | LR: 0.000226\n",
      "Epoch 111/150 | Best Val F1: 0.9126 | Val F1: 0.9064 | LR: 0.000198\n",
      "Epoch 114/150 | Best Val F1: 0.9126 | Val F1: 0.9047 | LR: 0.000172\n",
      "Epoch 117/150 | Best Val F1: 0.9126 | Val F1: 0.9070 | LR: 0.000147\n",
      "Epoch 120/150 | Best Val F1: 0.9126 | Val F1: 0.9081 | LR: 0.000123\n",
      "Epoch 123/150 | Best Val F1: 0.9126 | Val F1: 0.9058 | LR: 0.000101\n",
      "Epoch 126/150 | Best Val F1: 0.9126 | Val F1: 0.9061 | LR: 0.000081\n",
      "Early stopping at epoch 127. Best F1: 0.9126\n",
      "--- Finished Training --- Best F1: 0.9126\n",
      "Fold 5 Final Val F1: 0.9126\n",
      "\n",
      "--- üèÜ K-Fold Training Complete --- Average F1: 0.9333\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "fold_val_f1_list = []\n",
    "# --- MODIFICATION: The number of reordered continuous features is now 31 ---\n",
    "continuous_indices_reordered = list(range(31))\n",
    "EPOCHS = 150\n",
    "PATIENCE = 70\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_full_reordered, y_train_full)):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} --- ({fold_name}) ---\")\n",
    "    \n",
    "    X_train_fold, y_train_fold = X_train_full_reordered[train_idx], y_train_full[train_idx]\n",
    "    X_val_fold, y_val_fold = X_train_full_reordered[val_idx], y_train_full[val_idx]\n",
    "\n",
    "    preprocessor_fold = ColumnTransformer([('s', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "    ns, ts, f = X_train_fold.shape\n",
    "    X_train_scaled = preprocessor_fold.fit_transform(X_train_fold.reshape(ns*ts, f)).reshape(ns, ts, f)\n",
    "    ns_val, ts_val, f_val = X_val_fold.shape\n",
    "    X_val_scaled = preprocessor_fold.transform(X_val_fold.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "    \n",
    "    X_train_w, y_train_w, _ = create_sliding_windows(X_train_scaled, y_train_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(X_val_scaled, y_val_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    \n",
    "    # --- MODIFICATION: Augment the training data with noise injection for minority classes ---\n",
    "    X_train_w_aug, y_train_w_aug = augment_minority_classes(X_train_w, y_train_w, continuous_feature_count=len(continuous_indices_reordered), aug_factor=2)\n",
    "\n",
    "    # --- MODIFICATION: Create WeightedRandomSampler to handle class imbalance at the batch level ---\n",
    "    print(\"Creating WeightedRandomSampler for the training loader...\")\n",
    "    class_counts = np.bincount(y_train_w_aug)\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = np.array([class_weights[t] for t in y_train_w_aug])\n",
    "    sampler = WeightedRandomSampler(torch.from_numpy(sample_weights).double(), num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    # --- MODIFICATION: Pass sampler to the training loader and use augmented data ---\n",
    "    train_ds = TensorDataset(torch.from_numpy(X_train_w_aug).float(), torch.from_numpy(y_train_w_aug).long())\n",
    "    val_ds = TensorDataset(torch.from_numpy(X_val_w).float(), torch.from_numpy(y_val_w).long())\n",
    "    train_loader = make_loader(train_ds, FINAL_CONFIG['batch_size'], shuffle=False, drop_last=True, sampler=sampler) # Shuffle is False when sampler is used\n",
    "    val_loader = make_loader(val_ds, FINAL_CONFIG['batch_size'], shuffle=False, drop_last=False)\n",
    "\n",
    "    model_config_kfold = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "    model_fold = RecurrentClassifier(**model_config_kfold, num_classes=N_CLASSES).to(device)\n",
    "    model_fold = torch.compile(model_fold, backend=\"eager\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model_fold.parameters(), lr=FINAL_CONFIG['lr'], weight_decay=FINAL_CONFIG['l2_lambda'])\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=FINAL_CONFIG['lr'], epochs=EPOCHS, steps_per_epoch=len(train_loader))\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = FocalLoss(alpha=alpha_tensor, gamma=FINAL_CONFIG['focal_loss_gamma'])\n",
    "\n",
    "    model_fold_uncompiled = fit(model_fold, train_loader, val_loader, EPOCHS, criterion, optimizer, scheduler, scaler, device, PATIENCE, fold_name)\n",
    "    \n",
    "    _, val_f1 = validate_one_epoch(model_fold_uncompiled, val_loader, criterion, device)\n",
    "    fold_val_f1_list.append(val_f1)\n",
    "    print(f\"Fold {fold+1} Final Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Memory cleanup for the next fold\n",
    "    del X_train_w, y_train_w, X_val_w, y_val_w, X_train_w_aug, y_train_w_aug, train_ds, val_ds, train_loader, val_loader, model_fold, model_fold_uncompiled\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n--- üèÜ K-Fold Training Complete --- Average F1: {np.mean(fold_val_f1_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9d380",
   "metadata": {},
   "source": [
    "## üì¨ 7. Phase 3: Ensemble Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b31871e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing test dataset for submission ---\n",
      "Loading model 1/5 from models/FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_1_best_model.pt...\n",
      "Loading model 2/5 from models/FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_2_best_model.pt...\n",
      "Loading model 3/5 from models/FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_3_best_model.pt...\n",
      "Loading model 4/5 from models/FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_4_best_model.pt...\n",
      "Loading model 5/5 from models/FinalStrategy-GRU_H384_L3_C128_K5_v13_fold_5_best_model.pt...\n",
      "\n",
      "--- Aggregating predictions using standard Mean Aggregation ---\n",
      "\n",
      "Successfully saved to submissions\\submission_FinalStrategy-GRU_H384_L3_C128_K5_v13.csv!\n",
      "  sample_index    label\n",
      "0          000  no_pain\n",
      "1          001  no_pain\n",
      "2          002  no_pain\n",
      "3          003  no_pain\n",
      "4          004  no_pain\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Preparing test dataset for submission ---\")\n",
    "# --- MODIFICATION: Updated column indices to reflect removal of joint_30 ---\n",
    "continuous_indices_orig = list(range(30)) + [34]\n",
    "categorical_indices_orig = list(range(30, 34)) + [35]\n",
    "X_test_full_reordered = np.concatenate([\n",
    "    X_test_full_engineered[:, :, continuous_indices_orig],\n",
    "    X_test_full_engineered[:, :, categorical_indices_orig]], axis=2)\n",
    "\n",
    "continuous_indices_reordered = list(range(31))\n",
    "preprocessor_final = ColumnTransformer([('scaler', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "\n",
    "# Fit the final scaler on the ENTIRE reordered training data\n",
    "ns, ts, f = X_train_full_reordered.shape\n",
    "preprocessor_final.fit(X_train_full_reordered.reshape(ns * ts, f))\n",
    "\n",
    "ns_test, ts_test, f_test = X_test_full_reordered.shape\n",
    "X_test_scaled = preprocessor_final.transform(X_test_full_reordered.reshape(ns_test * ts_test, f_test)).reshape(ns_test, ts_test, f_test)\n",
    "X_test_w, test_window_indices = create_sliding_windows(X_test_scaled, y=None, window_size=FINAL_CONFIG['window_size'], stride=FINAL_CONFIG['stride'])\n",
    "test_loader = make_loader(TensorDataset(torch.from_numpy(X_test_w).float()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "model_config_final = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "all_fold_probabilities = []\n",
    "\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    print(f\"Loading model {fold+1}/{N_SPLITS} from {model_path}...\")\n",
    "    model_fold = RecurrentClassifier(**model_config_final, num_classes=N_CLASSES).to(device)\n",
    "    model_fold.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    # Using 'eager' backend for inference is fine and avoids potential cudagraphs state issues.\n",
    "    model_fold = torch.compile(model_fold, backend=\"eager\")\n",
    "    model_fold.eval()\n",
    "    \n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in test_loader:\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                probs = torch.softmax(model_fold(inputs.to(device)), dim=1)\n",
    "                fold_preds.append(probs.cpu().numpy())\n",
    "    all_fold_probabilities.append(np.concatenate(fold_preds))\n",
    "\n",
    "# --- REVERTED TO NORMAL: Using robust mean aggregation as requested ---\n",
    "print(\"\\n--- Aggregating predictions using standard Mean Aggregation ---\")\n",
    "\n",
    "# First, get the mean probabilities across all folds for each window\n",
    "mean_probabilities = np.mean(all_fold_probabilities, axis=0)\n",
    "df_probs = pd.DataFrame(mean_probabilities, columns=[f\"prob_{i}\" for i in range(N_CLASSES)])\n",
    "df_probs['original_index'] = test_window_indices\n",
    "\n",
    "# Group by the original sample index and calculate the mean of the probabilities for all its windows\n",
    "agg_probs = df_probs.groupby('original_index')[[f\"prob_{i}\" for i in range(N_CLASSES)]].mean().values\n",
    "final_predictions = le.inverse_transform(np.argmax(agg_probs, axis=1))\n",
    "\n",
    "submission_df = pd.DataFrame({'sample_index': sorted(X_test_long_df['sample_index'].unique()), 'label': final_predictions})\n",
    "submission_df['sample_index'] = submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "submission_filepath = os.path.join(\"submissions\", submission_filename_base)\n",
    "submission_df.to_csv(submission_filepath, index=False)\n",
    "print(f\"\\nSuccessfully saved to {submission_filepath}!\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
