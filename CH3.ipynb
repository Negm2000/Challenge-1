{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5222aca",
   "metadata": {
    "id": "markdown-notebook-intro"
   },
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è (v2: K-Fold Ensemble)**\n",
    "\n",
    "This notebook implements a robust K-Fold Cross-Validation and Ensembling strategy to improve on the baseline model. \n",
    "\n",
    "**Strategy:**\n",
    "1.  **Hyperparameter Search:** Use Ray Tune & Optuna on a single 80/20 split to find a good set of hyperparameters (the `FINAL_CONFIG`).\n",
    "2.  **K-Fold Training:** Instead of training one model on 100% of the data, we train `K` (e.g., 5) models on `K` different 80/20 splits (\"folds\"). We use early stopping to find the best model for each fold and save it to disk.\n",
    "3.  **Ensemble Prediction:** To create the final submission, we load all `K` models. We get `K` different predictions for the test set, average their (softmax) probabilities, and then aggregate these averaged probabilities for the final submission. This is far more robust than training a single, \"blind\" model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e25d9",
   "metadata": {
    "id": "markdown-libraries"
   },
   "source": [
    "## ‚öôÔ∏è **1. Setup & Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f67f7de3",
   "metadata": {
    "id": "code-libraries"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GPU (RTX 3070, here we come!) ---\n",
      "PyTorch version: 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Sklearn Imports ---\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Ray[tune] & Optuna Imports ---\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from functools import partial\n",
    "\n",
    "# --- Setup Directories & Device ---\n",
    "logs_dir = \"tensorboard\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU (RTX 3070, here we come!) ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0184001",
   "metadata": {
    "id": "markdown-load-reshape"
   },
   "source": [
    "## üîÑ **2. Data Loading & Reshaping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7115f",
   "metadata": {
    "id": "code-load-reshape"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Data and Preparing for Feature Engineering ---\n",
      "Using device: cuda\n",
      "Loaded X_train_full (shape: (661, 160, 35)) and y_train_full (shape: (661,))\n",
      "\n",
      "--- Step 2: Engineering 'is_pirate' Feature ---\n",
      "Identified 6 pirate samples.\n",
      "Created broadcasted pirate feature (shape: (661, 160, 1))\n",
      "Created X_train_full_engineered (shape: (661, 160, 36))\n",
      "N_FEATURES is now: 36\n",
      "\n",
      "--- Step 3: Calculating Class Weights ---\n",
      "Class counts (0, 1, 2): [511  94  56]\n",
      "Calculated class weights: tensor([0.0020, 0.0106, 0.0179], device='cuda:0')\n",
      "\n",
      "--- Setup Complete ---\n",
      "The variables 'X_train_full_engineered', 'y_train_full', 'N_FEATURES_NEW', and 'class_weights_tensor' are ready.\n",
      "You can now use the code block in the next response to run your K-Fold training.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define File Paths and Features ---\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "print(\"--- Step 1: Loading Data and Preparing for Feature Engineering ---\")\n",
    "\n",
    "try:\n",
    "    # Load features and labels\n",
    "    features_long_df = pd.read_csv(X_TRAIN_PATH)\n",
    "    labels_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "    X_test_long = pd.read_csv(X_TEST_PATH)\n",
    "    \n",
    "    # --- Define constants from the notebook ---\n",
    "    N_TIMESTEPS = 160\n",
    "    JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(31)]\n",
    "    PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "    FEATURES = JOINT_FEATURES + PAIN_FEATURES\n",
    "    N_FEATURES_ORIGINAL = len(FEATURES) # This is 35\n",
    "    LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Reshape function (copied from notebook) ---\n",
    "    def reshape_data(df, features_list, n_timesteps):\n",
    "        df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "        data_2d = df_pivot.values\n",
    "        n_samples = data_2d.shape[0]\n",
    "        data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "        return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "    # --- Load and reshape X_train_full ---\n",
    "    # This recreates X_train_full as it exists in the user's notebook\n",
    "    X_train_full = reshape_data(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())], FEATURES, N_TIMESTEPS)\n",
    "    \n",
    "    # --- Load and prepare y_train_full ---\n",
    "    # This recreates y_train_full\n",
    "    y_train_full_df = labels_df.sort_values(by='sample_index')\n",
    "    y_train_labels_str = y_train_full_df['label'].values\n",
    "    \n",
    "    X_test = reshape_data(\n",
    "        X_test_long, FEATURES, N_TIMESTEPS\n",
    "    )\n",
    "    \n",
    "    # We need a LabelEncoder, but can just use the mapping\n",
    "    y_train_full = y_train_full_df['label'].map(LABEL_MAPPING).values\n",
    "    \n",
    "    print(f\"Loaded X_train_full (shape: {X_train_full.shape}) and y_train_full (shape: {y_train_full.shape})\")\n",
    "\n",
    "    # --- Step 2: Create 'is_pirate' Feature ---\n",
    "    print(\"\\n--- Step 2: Engineering 'is_pirate' Feature ---\")\n",
    "    static_cols = ['sample_index', 'n_legs', 'n_hands', 'n_eyes']\n",
    "    static_df = features_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "    \n",
    "    pirate_filter = (\n",
    "        (static_df['n_legs'] == 'one+peg_leg') |\n",
    "        (static_df['n_hands'] == 'one+hook_hand') |\n",
    "        (static_df['n_eyes'] == 'one+eye_patch')\n",
    "    )\n",
    "    pirate_indices = static_df[pirate_filter].index\n",
    "    print(f\"Identified {len(pirate_indices)} pirate samples.\")\n",
    "\n",
    "    # Get the sample indices in the same order as X_train_full\n",
    "    sample_indices_ordered = features_long_df['sample_index'].unique()\n",
    "    sample_indices_ordered.sort() # Corresponds to X_train_full's order\n",
    "    \n",
    "    # Create the (661, 1) feature map\n",
    "    is_pirate_map = np.array([1 if idx in pirate_indices else 0 for idx in sample_indices_ordered])\n",
    "    \n",
    "    # \"Broadcast\" this (661, 1) map to (661, 160, 1)\n",
    "    pirate_feature_broadcast = np.tile(is_pirate_map.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "    \n",
    "    print(f\"Created broadcasted pirate feature (shape: {pirate_feature_broadcast.shape})\")\n",
    "    \n",
    "    # Concatenate with X_train_full\n",
    "    X_train_full_engineered = np.concatenate([X_train_full, pirate_feature_broadcast], axis=2)\n",
    "    \n",
    "    # --- UPDATE N_FEATURES ---\n",
    "    N_FEATURES_NEW = X_train_full_engineered.shape[2] # This will be 36\n",
    "    \n",
    "    print(f\"Created X_train_full_engineered (shape: {X_train_full_engineered.shape})\")\n",
    "    print(f\"N_FEATURES is now: {N_FEATURES_NEW}\")\n",
    "\n",
    "    # --- Step 3: Calculate Class Weights ---\n",
    "    print(\"\\n--- Step 3: Calculating Class Weights ---\")\n",
    "    class_counts_series = labels_df['label'].value_counts()\n",
    "    \n",
    "    # Re-order the counts to match the label encoding: [no_pain (0), low_pain (1), high_pain (2)]\n",
    "    counts_ordered = [\n",
    "        class_counts_series[LABEL_MAPPING.keys()],\n",
    "    ]\n",
    "    counts_ordered = class_counts_series.reindex(LABEL_MAPPING.keys()).values\n",
    "    # Should be [511, 94, 56]\n",
    "    \n",
    "    # Calculate inverse weights\n",
    "    class_weights_tensor = 1.0 / torch.tensor(counts_ordered, dtype=torch.float)\n",
    "    \n",
    "    # Normalize weights (optional but good practice)\n",
    "    class_weights_tensor = class_weights_tensor / class_weights_tensor.sum()\n",
    "    \n",
    "    class_weights_tensor = class_weights_tensor.to(device)\n",
    "    \n",
    "    print(f\"Class counts (0, 1, 2): {counts_ordered}\")\n",
    "    print(f\"Calculated class weights: {class_weights_tensor}\")\n",
    "\n",
    "    print(\"\\n--- Setup Complete ---\")\n",
    "    print(\"The variables 'X_train_full_engineered', 'y_train_full', 'N_FEATURES_NEW', and 'class_weights_tensor' are ready.\")\n",
    "    print(\"You can now use the code block in the next response to run your K-Fold training.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find a required file. {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c04ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(X_3d, y=None, window_size=100, stride=20):\n",
    "    \"\"\"\n",
    "    Takes 3D data (n_samples, n_timesteps, n_features)\n",
    "    and creates overlapping windows.\n",
    "    \n",
    "    Returns:\n",
    "    - new_X: (n_windows, window_size, n_features)\n",
    "    - new_y (if y is provided): (n_windows,)\n",
    "    - window_indices: (n_windows,) array tracking which original sample\n",
    "                      (e..g, 0, 1, 2...) each window came from.\n",
    "    \"\"\"\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    # This new array tracks which original sample each window came from.\n",
    "    window_indices = [] \n",
    "    \n",
    "    n_samples, n_timesteps, n_features = X_3d.shape\n",
    "    \n",
    "    # Iterate over each original sample\n",
    "    for i in range(n_samples):\n",
    "        sample = X_3d[i] # Shape (160, 35)\n",
    "        \n",
    "        # Slide a window over this sample\n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            window = sample[idx : idx + window_size]\n",
    "            new_X.append(window)\n",
    "            window_indices.append(i) # Track the original sample index (0, 1, 2...)\n",
    "            \n",
    "            if y is not None:\n",
    "                new_y.append(y[i]) # The label is the same for all windows\n",
    "                \n",
    "            idx += stride\n",
    "            \n",
    "    if y is not None:\n",
    "        # Return new X, new y, and the index mapping\n",
    "        return np.array(new_X), np.array(new_y), np.array(window_indices)\n",
    "    else:\n",
    "        # Return new X and the index mapping\n",
    "        return np.array(new_X), np.array(window_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d77fc",
   "metadata": {
    "id": "markdown-preprocessing"
   },
   "source": [
    "## üöß **3. Preprocessing: Split & Scale**\n",
    "\n",
    "1.  **Encode Labels:** Convert `no_pain`, `low_pain`, `high_pain` to `0`, `1`, `2`.\n",
    "2.  **Split Data:** Use `StratifiedShuffleSplit` to create a single 80/20 train/validation split **for the HPO phase**.\n",
    "3.  **Scale Features:** Use `StandardScaler`. We `fit` it *only* on the training data and `transform` all sets (train, val, and test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2355a626",
   "metadata": {
    "id": "code-preprocessing"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels encoded. 3 classes: {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
      "\n",
      "--- Splitting NON-WINDOWED data for HPO ---\n",
      "  X_train_split_full: (528, 160, 36)\n",
      "  y_train_split_full: (528,)\n",
      "  X_val_split_full:   (133, 160, 36)\n",
      "  y_val_split_full:   (133,)\n",
      "\n",
      "Fitting StandardScaler on X_train_2d shape: (84480, 36)\n",
      "Scaling complete for HPO data.\n",
      "  X_train_full_scaled: (528, 160, 36)\n",
      "  X_val_full_scaled:   (133, 160, 36)\n"
     ]
    }
   ],
   "source": [
    "# --- !! CORRECTED CELL [15] (2355a626) -- USE ENGINEERED DATA -- ---\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# --- 1. Encode Labels ---\n",
    "LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "le = LabelEncoder()\n",
    "le.fit(list(LABEL_MAPPING.keys()))\n",
    "y_train_full = le.transform(y_train_labels_str)\n",
    "N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "print(f\"Labels encoded. {N_CLASSES} classes: {LABEL_MAPPING}\")\n",
    "\n",
    "# --- 2. Split Data (NON-WINDOWED) ---\n",
    "print(\"\\n--- Splitting NON-WINDOWED data for HPO ---\")\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# --- !!! THIS IS THE FIX !!! ---\n",
    "# We split the 36-feature data, not the 35-feature data\n",
    "for train_idx, val_idx in sss.split(X_train_full_engineered, y_train_full):\n",
    "    X_train_split_full = X_train_full_engineered[train_idx]\n",
    "    y_train_split_full = y_train_full[train_idx]\n",
    "    X_val_split_full = X_train_full_engineered[val_idx]\n",
    "    y_val_split_full = y_train_full[val_idx]\n",
    "# --- !!! END OF FIX !!! ---\n",
    "\n",
    "print(f\"  X_train_split_full: {X_train_split_full.shape}\")\n",
    "print(f\"  y_train_split_full: {y_train_split_full.shape}\")\n",
    "print(f\"  X_val_split_full:   {X_val_split_full.shape}\")\n",
    "print(f\"  y_val_split_full:   {y_val_split_full.shape}\")\n",
    "\n",
    "# --- 3. Scale Features (The \"No-Cheating\" Rule) ---\n",
    "# We fit the scaler ONLY on the 2D-reshaped training split\n",
    "scaler = StandardScaler() \n",
    "ns, ts, f = X_train_split_full.shape\n",
    "X_train_2d = X_train_split_full.reshape(ns * ts, f)\n",
    "print(f\"\\nFitting StandardScaler on X_train_2d shape: {X_train_2d.shape}\")\n",
    "scaler.fit(X_train_2d)\n",
    "\n",
    "# Transform both train and val splits\n",
    "X_train_scaled_2d = scaler.transform(X_train_2d)\n",
    "X_train_full_scaled = X_train_scaled_2d.reshape(ns, ts, f)\n",
    "\n",
    "ns_val, ts_val, f_val = X_val_split_full.shape\n",
    "X_val_2d = X_val_split_full.reshape(ns_val * ts_val, f_val)\n",
    "X_val_scaled_2d = scaler.transform(X_val_2d)\n",
    "X_val_full_scaled = X_val_scaled_2d.reshape(ns_val, ts_val, f_val)\n",
    "\n",
    "print(\"Scaling complete for HPO data.\")\n",
    "print(f\"  X_train_full_scaled: {X_train_full_scaled.shape}\") # Should show 36 features\n",
    "print(f\"  X_val_full_scaled:   {X_val_full_scaled.shape}\") # Should show 36 features\n",
    "\n",
    "# Clean up\n",
    "del X_train_2d, X_val_2d, X_train_scaled_2d, X_val_scaled_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c883b33",
   "metadata": {
    "id": "markdown-dataloaders"
   },
   "source": [
    "## üöö **4. PyTorch DataLoaders (for HPO)**\n",
    "\n",
    "This section prepares the DataLoaders for the Ray Tune HPO phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8702f5a",
   "metadata": {
    "id": "code-dataloaders"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders will be created inside the tuning loop.\n",
      "Keeping HPO numpy arrays (non-windowed, scaled) in memory...\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define make_loader function (from Lecture 4) ---\n",
    "BATCH_SIZE = 128 # This will be our default, but Optuna can tune it\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    # Set num_workers=0 for Windows-friendly loading (from our debugging)\n",
    "    num_workers = 0 \n",
    "    \n",
    "    # Create DataLoader with performance optimizations\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=int(batch_size), # Ensure batch_size is an int for the DataLoader\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=None,\n",
    "    )\n",
    "\n",
    "print(\"DataLoaders will be created inside the tuning loop.\")\n",
    "print(\"Keeping HPO numpy arrays (non-windowed, scaled) in memory...\")\n",
    "\n",
    "# We no longer create Tensors or TensorDatasets here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16263592",
   "metadata": {
    "id": "markdown-model-training"
   },
   "source": [
    "## üõ†Ô∏è **5. Model & Training Engine**\n",
    "\n",
    "These are the core components from Lecture 4, modified for Ray Tune and K-Fold.\n",
    "\n",
    "-   `RecurrentClassifier`: Our flexible model (RNN, LSTM, GRU).\n",
    "-   `train_one_epoch` / `validate_one_epoch`: Standard loops.\n",
    "-   `objective_function`: The wrapper for Ray Tune's HPO.\n",
    "-   `fit`: The **original** training loop from Lecture 4, **b-rought back** to handle our K-Fold training, complete with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06613e58",
   "metadata": {
    "id": "code-recurrent-summary"
   },
   "outputs": [],
   "source": [
    "def recurrent_summary(model, input_size):\n",
    "    \"\"\"\n",
    "    Custom summary function that emulates torchinfo's output while correctly\n",
    "    counting parameters for RNN/GRU/LSTM layers.\n",
    "    \"\"\"\n",
    "\n",
    "    output_shapes = {}\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1  # Replace batch dimension with -1\n",
    "\n",
    "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
    "                    shape2 = list(output[1][0].shape)\n",
    "                else:  # RNN/GRU case: h_n only\n",
    "                    shape2 = list(output[1].shape)\n",
    "                shape2[1] = -1\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "        return hook\n",
    "\n",
    "    try:\n",
    "        device_summary = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device_summary = torch.device(\"cpu\")\n",
    "\n",
    "    dummy_input = torch.randn(1, *input_size).to(device_summary)\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(dummy_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ac4fc09",
   "metadata": {
    "id": "code-recurrent-classifier"
   },
   "outputs": [],
   "source": [
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic RNN classifier (RNN, LSTM, GRU) from Lecture 4.\n",
    "    Uses the last hidden state for classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',        # 'RNN', 'LSTM', or 'GRU'\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "\n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "\n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "\n",
    "        # Dropout is only applied between layers (if num_layers > 1)\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        if self.bidirectional:\n",
    "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
    "        else:\n",
    "            classifier_input_size = hidden_size\n",
    "\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_length, input_size)\n",
    "        \"\"\"\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0]\n",
    "\n",
    "        if self.bidirectional:\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
    "        else:\n",
    "            hidden_to_classify = hidden[-1]\n",
    "\n",
    "        logits = self.classifier(hidden_to_classify)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b67a46",
   "metadata": {
    "id": "code-training-loop-MODIFIED"
   },
   "outputs": [],
   "source": [
    "# --- !! CORRECTED CELL [8] (17b67a46) -- Fix HPO Signature !! ---\n",
    "# This cell contains all our training functions\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # --- !! START OF GRADIENT CLIPPING FIX !! ---\n",
    "        # Unscale gradients before clipping\n",
    "        scaler.unscale_(optimizer) \n",
    "        # Clip the (now unscaled) gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5) \n",
    "        # --- !! END OF GRADIENT CLIPPING FIX !! ---\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Check for nan loss *after* the step\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Warning: NaN loss detected in batch {batch_idx}. Skipping batch.\")\n",
    "            continue # Skip this batch's metrics\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Check if all_targets is empty (if all batches were nan)\n",
    "    if not all_targets:\n",
    "        return epoch_loss, 0.0 # Return 0 F1 score if no valid batches\n",
    "\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
    "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "\n",
    "# --- !! THIS IS THE FIX !! ---\n",
    "# We must add 'class_weights_tensor' to the function signature\n",
    "def objective_function(config, X_train_full_scaled, y_train_split_full, X_val_full_scaled, y_val_split_full, class_weights_tensor):\n",
    "    \"\"\"\n",
    "    This is the main function that Ray Tune will call for each trial.\n",
    "    'config' is a dictionary of hyperparameters from Optuna.\n",
    "    The X, y data is the NON-WINDOWED, scaled 80/20 split.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Create Sliding Windows based on config ---\n",
    "    X_train_w, y_train_w, _ = create_sliding_windows(\n",
    "        X_train_full_scaled, \n",
    "        y_train_split_full, \n",
    "        window_size=config[\"window_size\"], \n",
    "        stride=config[\"stride\"]\n",
    "    )\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(\n",
    "        X_val_full_scaled, \n",
    "        y_val_split_full, \n",
    "        window_size=config[\"window_size\"], \n",
    "        stride=config[\"stride\"] # Use same stride for validation\n",
    "    )\n",
    "\n",
    "    # --- 2. Create Tensors and DataLoaders ---\n",
    "    train_features = torch.from_numpy(X_train_w).float()\n",
    "    train_targets = torch.from_numpy(y_train_w).long()\n",
    "    val_features = torch.from_numpy(X_val_w).float()\n",
    "    val_targets = torch.from_numpy(y_val_w).long()\n",
    "\n",
    "    train_ds = TensorDataset(train_features, train_targets)\n",
    "    val_ds = TensorDataset(val_features, val_targets)\n",
    "    \n",
    "    train_loader = make_loader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "    val_loader = make_loader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=False)\n",
    "    \n",
    "    # --- 3. Create Model --- \n",
    "    model = RecurrentClassifier(\n",
    "        input_size=N_FEATURES_NEW, \n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        num_classes=N_CLASSES,\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        bidirectional=config[\"bidirectional\"],\n",
    "        rnn_type=config[\"rnn_type\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    if torch.__version__[0] >= \"2\":\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    # --- 4. Create Optimizer, Loss, Scaler ---\n",
    "    \n",
    "    # This line is now correct, because class_weights_tensor is passed in\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "    # --- 5. The Training Loop (adapted from fit) ---\n",
    "    EPOCHS = 200 # Max epochs\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, 0, config[\"l2_lambda\"]\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # --- Send Results to Ray Tune --- \n",
    "        tune.report({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_f1\": val_f1\n",
    "        })\n",
    "\n",
    "# --- !! MODIFICATION !! ---\n",
    "# This is the original 'fit' function from Lecture 4, brought back.\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    print(f\"Will train for {epochs} epochs with patience={patience} monitoring {evaluation_metric}\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
    "            )\n",
    "\n",
    "        if verbose > 0 and (epoch % verbose == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                  f\"Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n",
    "                  f\"Val: Loss={val_loss:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    if restore_best_weights and patience > 0:\n",
    "        print(f\"Restoring best model from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    if patience == 0:\n",
    "        print(\"Training complete. Saving final model.\")\n",
    "        torch.save(model.state_dict(), model_path.replace(\"_best_model.pt\", \"_final_model.pt\"))\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    \n",
    "    print(f\"--- Finished Training: {experiment_name} ---\")\n",
    "    return model, training_history, best_epoch if 'best_epoch' in locals() else epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a903e3",
   "metadata": {
    "id": "markdown-optuna-search"
   },
   "source": [
    "## üß™ **6. Phase 1: Hyperparameter Search**\n",
    "\n",
    "This cell is identical to before. We run HPO on our single 80/20 split to find a good `FINAL_CONFIG` to use for our K-Fold training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8c96b",
   "metadata": {
    "id": "code-optuna-search"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-12 09:47:17</td></tr>\n",
       "<tr><td>Running for: </td><td>00:36:58.90        </td></tr>\n",
       "<tr><td>Memory:      </td><td>8.1/13.9 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=30<br>Bracket: Iter 80.000: 0.8956867317870743 | Iter 40.000: 0.8769775410955908 | Iter 20.000: 0.8523070024008605<br>Logical resource usage: 4.0/16 CPUs, 0.25/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th>bidirectional  </th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  l2_lambda</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th>rnn_type  </th><th style=\"text-align: right;\">  stride</th><th style=\"text-align: right;\">  window_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_f1</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_function_c94c81dc</td><td>TERMINATED</td><td>127.0.0.1:29460</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.564653</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">7.6782e-06 </td><td style=\"text-align: right;\">6.69772e-05</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        158.162 </td><td style=\"text-align: right;\"> 0.0139285  </td><td style=\"text-align: right;\">  0.989831</td><td style=\"text-align: right;\">  0.375912</td></tr>\n",
       "<tr><td>objective_function_7d1fb19a</td><td>TERMINATED</td><td>127.0.0.1:21480</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.278128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000555137</td><td style=\"text-align: right;\">9.12073e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        273.492 </td><td style=\"text-align: right;\"> 0.225439   </td><td style=\"text-align: right;\">  0.94676 </td><td style=\"text-align: right;\">  0.381214</td></tr>\n",
       "<tr><td>objective_function_21188c3b</td><td>TERMINATED</td><td>127.0.0.1:30776</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.557946</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000500221</td><td style=\"text-align: right;\">9.44136e-05</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           60</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         95.1177</td><td style=\"text-align: right;\"> 0.253757   </td><td style=\"text-align: right;\">  0.93272 </td><td style=\"text-align: right;\">  0.205738</td></tr>\n",
       "<tr><td>objective_function_0d732382</td><td>TERMINATED</td><td>127.0.0.1:23192</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.263743</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.21201e-07</td><td style=\"text-align: right;\">0.000326268</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        129.029 </td><td style=\"text-align: right;\"> 0.000610584</td><td style=\"text-align: right;\">  0.999609</td><td style=\"text-align: right;\">  0.545082</td></tr>\n",
       "<tr><td>objective_function_f0699cb7</td><td>TERMINATED</td><td>127.0.0.1:26528</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.170731</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000177043</td><td style=\"text-align: right;\">2.1156e-05 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         29.5545</td><td style=\"text-align: right;\"> 0.516258   </td><td style=\"text-align: right;\">  0.729517</td><td style=\"text-align: right;\">  0.351544</td></tr>\n",
       "<tr><td>objective_function_e87ec1da</td><td>TERMINATED</td><td>127.0.0.1:19500</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.566646</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">2.70562e-07</td><td style=\"text-align: right;\">7.47335e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           60</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        420.285 </td><td style=\"text-align: right;\"> 0.00369397 </td><td style=\"text-align: right;\">  0.998784</td><td style=\"text-align: right;\">  0.502633</td></tr>\n",
       "<tr><td>objective_function_ce986945</td><td>TERMINATED</td><td>127.0.0.1:30340</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.241343</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.0007533  </td><td style=\"text-align: right;\">0.000648807</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           60</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        134.321 </td><td style=\"text-align: right;\"> 0.0757605  </td><td style=\"text-align: right;\">  0.92779 </td><td style=\"text-align: right;\">  0.216949</td></tr>\n",
       "<tr><td>objective_function_79837163</td><td>TERMINATED</td><td>127.0.0.1:26504</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.239829</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.37343e-07</td><td style=\"text-align: right;\">0.000238546</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           60</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         26.21  </td><td style=\"text-align: right;\"> 0.0533753  </td><td style=\"text-align: right;\">  0.921891</td><td style=\"text-align: right;\">  0.307523</td></tr>\n",
       "<tr><td>objective_function_747dcace</td><td>TERMINATED</td><td>127.0.0.1:31404</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.353224</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">7.77338e-06</td><td style=\"text-align: right;\">3.3528e-05 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           60</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         50.858 </td><td style=\"text-align: right;\"> 0.19279    </td><td style=\"text-align: right;\">  0.793684</td><td style=\"text-align: right;\">  0.273825</td></tr>\n",
       "<tr><td>objective_function_186b9ca3</td><td>TERMINATED</td><td>127.0.0.1:28700</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.407387</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.75994e-07</td><td style=\"text-align: right;\">5.62714e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        172.618 </td><td style=\"text-align: right;\"> 0.0191664  </td><td style=\"text-align: right;\">  0.981531</td><td style=\"text-align: right;\">  0.423334</td></tr>\n",
       "<tr><td>objective_function_bbe6c1e5</td><td>TERMINATED</td><td>127.0.0.1:30812</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.509759</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">6.70938e-06</td><td style=\"text-align: right;\">0.000908206</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         41.8964</td><td style=\"text-align: right;\"> 0.00772745 </td><td style=\"text-align: right;\">  0.9929  </td><td style=\"text-align: right;\">  0.610208</td></tr>\n",
       "<tr><td>objective_function_b2a6546e</td><td>TERMINATED</td><td>127.0.0.1:26532</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.41734 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000176229</td><td style=\"text-align: right;\">5.36347e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           60</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        218.427 </td><td style=\"text-align: right;\"> 0.214277   </td><td style=\"text-align: right;\">  0.947333</td><td style=\"text-align: right;\">  0.391753</td></tr>\n",
       "<tr><td>objective_function_b821f178</td><td>TERMINATED</td><td>127.0.0.1:29864</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.353566</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">5.60377e-06</td><td style=\"text-align: right;\">0.000936254</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        266.938 </td><td style=\"text-align: right;\"> 0.00510432 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.417154</td></tr>\n",
       "<tr><td>objective_function_c1f0948e</td><td>TERMINATED</td><td>127.0.0.1:29708</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.339438</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">2.68197e-05</td><td style=\"text-align: right;\">4.10279e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           60</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         32.6869</td><td style=\"text-align: right;\"> 0.188585   </td><td style=\"text-align: right;\">  0.823029</td><td style=\"text-align: right;\">  0.257187</td></tr>\n",
       "<tr><td>objective_function_279176c1</td><td>TERMINATED</td><td>127.0.0.1:2340 </td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.112157</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">9.07837e-07</td><td style=\"text-align: right;\">0.000245231</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         19.6996</td><td style=\"text-align: right;\"> 0.0731899  </td><td style=\"text-align: right;\">  0.906126</td><td style=\"text-align: right;\">  0.268836</td></tr>\n",
       "<tr><td>objective_function_3dd82d23</td><td>TERMINATED</td><td>127.0.0.1:29240</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.114595</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">6.32662e-07</td><td style=\"text-align: right;\">0.000226084</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         19.7711</td><td style=\"text-align: right;\"> 0.0583201  </td><td style=\"text-align: right;\">  0.932877</td><td style=\"text-align: right;\">  0.335019</td></tr>\n",
       "<tr><td>objective_function_613dd78f</td><td>TERMINATED</td><td>127.0.0.1:28544</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.481777</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">9.24789e-07</td><td style=\"text-align: right;\">0.00023867 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         40.3865</td><td style=\"text-align: right;\"> 0.0257916  </td><td style=\"text-align: right;\">  0.966656</td><td style=\"text-align: right;\">  0.402028</td></tr>\n",
       "<tr><td>objective_function_c3beec23</td><td>TERMINATED</td><td>127.0.0.1:28660</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.493069</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">9.06581e-07</td><td style=\"text-align: right;\">0.000220803</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         28.3488</td><td style=\"text-align: right;\"> 0.0550376  </td><td style=\"text-align: right;\">  0.931546</td><td style=\"text-align: right;\">  0.340211</td></tr>\n",
       "<tr><td>objective_function_f198b745</td><td>TERMINATED</td><td>127.0.0.1:7536 </td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.462896</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.15021e-06</td><td style=\"text-align: right;\">0.000427192</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        139.915 </td><td style=\"text-align: right;\"> 0.0113323  </td><td style=\"text-align: right;\">  0.998356</td><td style=\"text-align: right;\">  0.561225</td></tr>\n",
       "<tr><td>objective_function_1c840550</td><td>TERMINATED</td><td>127.0.0.1:29072</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.510107</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.60958e-06</td><td style=\"text-align: right;\">0.000464661</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        177.421 </td><td style=\"text-align: right;\"> 0.00110156 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.522723</td></tr>\n",
       "<tr><td>objective_function_1af55235</td><td>TERMINATED</td><td>127.0.0.1:26296</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.598334</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">3.30002e-06</td><td style=\"text-align: right;\">1.10904e-05</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         65.4835</td><td style=\"text-align: right;\"> 0.282893   </td><td style=\"text-align: right;\">  0.738837</td><td style=\"text-align: right;\">  0.355949</td></tr>\n",
       "<tr><td>objective_function_c4854e5a</td><td>TERMINATED</td><td>127.0.0.1:12336</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.424407</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">3.46691e-06</td><td style=\"text-align: right;\">0.000513025</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        135.737 </td><td style=\"text-align: right;\"> 0.00979662 </td><td style=\"text-align: right;\">  0.998474</td><td style=\"text-align: right;\">  0.376165</td></tr>\n",
       "<tr><td>objective_function_a06b02e8</td><td>TERMINATED</td><td>127.0.0.1:3628 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.419546</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">3.03741e-05</td><td style=\"text-align: right;\">1.01346e-05</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         63.1486</td><td style=\"text-align: right;\"> 0.311091   </td><td style=\"text-align: right;\">  0.739446</td><td style=\"text-align: right;\">  0.352404</td></tr>\n",
       "<tr><td>objective_function_0c7c963f</td><td>TERMINATED</td><td>127.0.0.1:27008</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.405774</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">3.26538e-05</td><td style=\"text-align: right;\">0.000507654</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         28.7428</td><td style=\"text-align: right;\"> 0.0635282  </td><td style=\"text-align: right;\">  0.957391</td><td style=\"text-align: right;\">  0.405007</td></tr>\n",
       "<tr><td>objective_function_88358e9a</td><td>TERMINATED</td><td>127.0.0.1:13196</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.339427</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">6.00881e-05</td><td style=\"text-align: right;\">0.000150672</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">          120</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         14.0531</td><td style=\"text-align: right;\"> 0.108595   </td><td style=\"text-align: right;\">  0.917322</td><td style=\"text-align: right;\">  0.298453</td></tr>\n",
       "<tr><td>objective_function_a1dcc77a</td><td>TERMINATED</td><td>127.0.0.1:13308</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.541245</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">2.57527e-06</td><td style=\"text-align: right;\">0.000141222</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       1071.63  </td><td style=\"text-align: right;\"> 0.00547162 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.435599</td></tr>\n",
       "<tr><td>objective_function_f11bef2f</td><td>TERMINATED</td><td>127.0.0.1:30200</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.549752</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">3.52336e-07</td><td style=\"text-align: right;\">0.000145858</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        919.286 </td><td style=\"text-align: right;\"> 0.00125255 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.470677</td></tr>\n",
       "<tr><td>objective_function_5fce2f04</td><td>TERMINATED</td><td>127.0.0.1:4716 </td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.535633</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">2.55029e-06</td><td style=\"text-align: right;\">0.000913999</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        946.834 </td><td style=\"text-align: right;\"> 0.00302186 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">  0.555329</td></tr>\n",
       "<tr><td>objective_function_213b59bb</td><td>TERMINATED</td><td>127.0.0.1:21736</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.535324</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">3.86128e-07</td><td style=\"text-align: right;\">0.000951115</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       1110.25  </td><td style=\"text-align: right;\"> 0.114855   </td><td style=\"text-align: right;\">  0.969135</td><td style=\"text-align: right;\">  0.343272</td></tr>\n",
       "<tr><td>objective_function_27a1db82</td><td>TERMINATED</td><td>127.0.0.1:31512</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.553347</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">2.38656e-06</td><td style=\"text-align: right;\">0.000868351</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        413.115 </td><td style=\"text-align: right;\"> 0.044835   </td><td style=\"text-align: right;\">  0.973895</td><td style=\"text-align: right;\">  0.280939</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-12 09:10:41,170 E 31124 7808] (gcs_server.exe) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-12 09:10:45,307 E 20432 25120] (raylet.exe) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "2025-11-12 09:47:17,533\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'c:/Users/Karim Negm/Documents/AN2DL/Challenge 1/ray_results/pirate_pain_optuna_search_with_windowing' in 0.0559s.\n",
      "2025-11-12 09:47:17,557\tINFO tune.py:1041 -- Total run time: 2218.94 seconds (2218.84 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Search Complete ---\n",
      "\n",
      "Getting best trial from analysis...\n",
      "Best validation F1 score: 0.8918\n",
      "Best hyperparameters found:\n",
      "{'window_size': 100, 'stride': 10, 'rnn_type': 'GRU', 'lr': 0.0008683514169397042, 'batch_size': 64, 'hidden_size': 256, 'num_layers': 3, 'dropout_rate': 0.5533467006072341, 'bidirectional': True, 'l2_lambda': 2.3865599432221854e-06}\n"
     ]
    }
   ],
   "source": [
    "# --- !! CORRECTED CELL [10] (41d8c96b) -- Pass Weights to HPO !! ---\n",
    "\n",
    "# --- 1. Define the Search Space for Optuna --\n",
    "search_space = {\n",
    "    # NEW HPO PARAMS\n",
    "    \"window_size\": tune.choice([60, 80, 100, 120]),\n",
    "    \"stride\": tune.choice([10, 20]),\n",
    "    \n",
    "    # OLD HPO PARAMS\n",
    "    \"rnn_type\": tune.choice(['GRU', 'LSTM']),\n",
    "    \"lr\": tune.loguniform(1e-5, 5e-3),\n",
    "    \"batch_size\": tune.choice([64, 128, 256]),  \n",
    "    \"hidden_size\": tune.choice([128, 256, 384]),\n",
    "    \"num_layers\": tune.choice([2, 3]),\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.6),\n",
    "    \"bidirectional\": tune.choice([True, False]),\n",
    "    \"l2_lambda\": tune.loguniform(1e-7, 1e-3)\n",
    "}\n",
    "# --- 2. Define the Optimizer (Optuna) and Scheduler (ASHA) ---\n",
    "optuna_search = OptunaSearch(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\",\n",
    "    grace_period=20,  # Min epochs a trial must run\n",
    "    reduction_factor=2  # How aggressively to stop trials\n",
    ")\n",
    "\n",
    "# --- 3. Initialize Ray (WITH THE BIG HAMMER FIX) ---\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "ray_logs_path = os.path.abspath(\"./ray_results\")\n",
    "os.makedirs(ray_logs_path, exist_ok=True)\n",
    "os.environ[\"RAY_TEMP_DIR\"] = ray_logs_path\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=16, \n",
    "    num_gpus=1, \n",
    "    ignore_reinit_error=True\n",
    ")\n",
    "\n",
    "def short_trial_name(trial):\n",
    "    \"\"\"Creates a short, unique name for each trial folder.\"\"\"\n",
    "    return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "\n",
    "# --- 4. Run the Tuner --\n",
    "print(\"Starting hyperparameter search (including window_size and stride)...\")\n",
    "\n",
    "# We use tune.with_parameters to pass our NON-WINDOWED, SCALED numpy arrays\n",
    "# to the objective function\n",
    "objective_with_data = tune.with_parameters(\n",
    "    objective_function,\n",
    "    X_train_full_scaled=X_train_full_scaled,\n",
    "    y_train_split_full=y_train_split_full,\n",
    "    X_val_full_scaled=X_val_full_scaled,\n",
    "    y_val_split_full=y_val_split_full,\n",
    "    \n",
    "    # --- !! THIS IS THE FIX !! ---\n",
    "    # Pass the class weights (defined in cell [83c7115f]) to the HPO function\n",
    "    class_weights_tensor=class_weights_tensor\n",
    "    # --- !! END OF FIX !! ---\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    objective_with_data, # Pass the new objective function\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 0.25}, \n",
    "    config=search_space,\n",
    "    num_samples=30, # You may want to INCREASE this (e.g., to 30 or 40)\n",
    "    search_alg=optuna_search,\n",
    "    scheduler=scheduler,\n",
    "    name=\"pirate_pain_optuna_search_with_windowing\", # New name\n",
    "    storage_path=ray_logs_path,\n",
    "    trial_dirname_creator= short_trial_name,\n",
    "    log_to_file=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n--- Search Complete ---\\n\")\n",
    "\n",
    "# --- 5. Get Best Results (FIX 3) ---\n",
    "print(\"Getting best trial from analysis...\")\n",
    "best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "if best_trial:\n",
    "    best_config = best_trial.config\n",
    "    best_val_f1 = best_trial.last_result[\"val_f1\"]\n",
    "    \n",
    "    print(f\"Best validation F1 score: {best_val_f1:.4f}\")\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(best_config)\n",
    "else:\n",
    "    print(\"ERROR: No trials completed successfully. Check the 'ray_results' folder for logs.\")\n",
    "    best_config = None # Handle the case where all trials failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b564a8e",
   "metadata": {
    "id": "markdown-submission-config"
   },
   "source": [
    "## üèÜ **7. Final Model Configuration**\n",
    "\n",
    "This cell now holds our winning configuration, ready for the K-Fold training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab5802d6",
   "metadata": {
    "id": "code-submission-config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New Helper Functions and Variables Defined ---\n",
      "  create_sliding_windows(): Ready\n",
      "  make_loader(): Ready\n",
      "  make_final_loader(): Ready\n",
      "  N_SPLITS set to: 5\n",
      "  New feature count set to: 36\n",
      "\n",
      "--- Engineering 'is_pirate' Feature ---\n",
      "Identified 6 pirate samples in training set.\n",
      "Created X_train_full_engineered (shape: (661, 160, 36))\n",
      "\n",
      "--- Calculating Class Weights ---\n",
      "Class counts (0, 1, 2): [511  94  56]\n",
      "Calculated class weights: tensor([0.0020, 0.0106, 0.0179], device='cuda:0')\n",
      "\n",
      "LabelEncoder 'le' is ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder # Use RobustScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- 1. Re-define functions we need (to prevent NameErrors) ---\n",
    "\n",
    "def create_sliding_windows(X_3d, y=None, window_size=100, stride=20):\n",
    "    \"\"\"\n",
    "    Takes 3D data (n_samples, n_timesteps, n_features)\n",
    "    and creates overlapping windows. Copied from cell [14].\n",
    "    \"\"\"\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    window_indices = [] \n",
    "    n_samples, n_timesteps, n_features = X_3d.shape\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        sample = X_3d[i]\n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            window = sample[idx : idx + window_size]\n",
    "            new_X.append(window)\n",
    "            window_indices.append(i)\n",
    "            if y is not None:\n",
    "                new_y.append(y[i])\n",
    "            idx += stride\n",
    "            \n",
    "    if y is not None:\n",
    "        return np.array(new_X), np.array(new_y), np.array(window_indices)\n",
    "    else:\n",
    "        return np.array(new_X), np.array(window_indices)\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    \"\"\" Copied from cell [16] (b8702f5a) \"\"\"\n",
    "    return DataLoader(\n",
    "        ds, batch_size=int(batch_size), shuffle=shuffle, drop_last=drop_last,\n",
    "        num_workers=0, pin_memory=True,\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=None,\n",
    "    )\n",
    "\n",
    "def make_final_loader(ds, batch_size, shuffle, drop_last):\n",
    "    \"\"\" Copied from original cell [36] (acc486f2) \"\"\"\n",
    "    return DataLoader(\n",
    "        ds, batch_size=int(batch_size), shuffle=shuffle, drop_last=drop_last,\n",
    "        num_workers=0, pin_memory=True, pin_memory_device=\"cuda\", prefetch_factor=None\n",
    "    )\n",
    "\n",
    "# --- 2. Define New Global Training Variables ---\n",
    "N_SPLITS = 5\n",
    "N_FEATURES_NEW = 36 # Original (35) + is_pirate (1)\n",
    "\n",
    "print(f\"--- New Helper Functions and Variables Defined ---\")\n",
    "print(f\"  create_sliding_windows(): Ready\")\n",
    "print(f\"  make_loader(): Ready\")\n",
    "print(f\"  make_final_loader(): Ready\")\n",
    "print(f\"  N_SPLITS set to: {N_SPLITS}\")\n",
    "print(f\"  New feature count set to: {N_FEATURES_NEW}\")\n",
    "\n",
    "# --- 3. Engineer 'is_pirate' Feature ---\n",
    "print(\"\\n--- Engineering 'is_pirate' Feature ---\")\n",
    "# X_train_full and y_train_full were defined in cell [14] (83c7115f)\n",
    "# features_long_df was defined in cell [14]\n",
    "static_cols = ['sample_index', 'n_legs', 'n_hands', 'n_eyes']\n",
    "static_df = features_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "pirate_filter = (\n",
    "    (static_df['n_legs'] == 'one+peg_leg') |\n",
    "    (static_df['n_hands'] == 'one+hook_hand') |\n",
    "    (static_df['n_eyes'] == 'one+eye_patch')\n",
    ")\n",
    "pirate_indices = static_df[pirate_filter].index\n",
    "print(f\"Identified {len(pirate_indices)} pirate samples in training set.\")\n",
    "\n",
    "sample_indices_ordered = sorted(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())]['sample_index'].unique())\n",
    "is_pirate_map = np.array([1 if idx in pirate_indices else 0 for idx in sample_indices_ordered])\n",
    "pirate_feature_broadcast = np.tile(is_pirate_map.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "\n",
    "# Create the new 36-feature X_train_full\n",
    "X_train_full_engineered = np.concatenate([X_train_full, pirate_feature_broadcast], axis=2)\n",
    "print(f\"Created X_train_full_engineered (shape: {X_train_full_engineered.shape})\")\n",
    "\n",
    "# --- 4. Calculate Class Weights ---\n",
    "print(\"\\n--- Calculating Class Weights ---\")\n",
    "class_counts_series = labels_df['label'].value_counts()\n",
    "counts_ordered = class_counts_series.reindex(LABEL_MAPPING.keys()).values\n",
    "class_weights_tensor = 1.0 / torch.tensor(counts_ordered, dtype=torch.float)\n",
    "class_weights_tensor = class_weights_tensor.to(device)\n",
    "print(f\"Class counts (0, 1, 2): {counts_ordered}\")\n",
    "print(f\"Calculated class weights: {class_weights_tensor}\")\n",
    "\n",
    "# --- 5. Define the Label Encoder (le) ---\n",
    "# le was defined in cell [15] (2355a626), but we define it here for safety\n",
    "le = LabelEncoder()\n",
    "le.fit(list(LABEL_MAPPING.keys()))\n",
    "print(\"\\nLabelEncoder 'le' is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "581444aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèÜ Final Configuration Set --- \n",
      "Best Val F1 from HPO search: 0.8918\n",
      "{'window_size': 100, 'stride': 10, 'rnn_type': 'GRU', 'lr': 0.0008683514169397042, 'batch_size': 64, 'hidden_size': 256, 'num_layers': 3, 'dropout_rate': 0.5533467006072341, 'bidirectional': True, 'l2_lambda': 2.3865599432221854e-06}\n",
      "Submission name will be: submission_GRU_H256_L3_BTrue_Optuna_KFold_Ensemble_w100_s10.csv\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# --- üèÜ FINAL MODEL CONFIGURATION üèÜ ---\n",
    "# ===================================================================\n",
    "FINAL_CONFIG = best_config\n",
    "FINAL_BEST_VAL_F1 = best_val_f1\n",
    "\n",
    "print(\"--- üèÜ Final Configuration Set --- \")\n",
    "print(f\"Best Val F1 from HPO search: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "print(FINAL_CONFIG)\n",
    "\n",
    "# --- Set variables for the K-Fold & submission cells ---\n",
    "FINAL_MODEL_TYPE = FINAL_CONFIG[\"rnn_type\"]\n",
    "FINAL_HIDDEN_SIZE = FINAL_CONFIG[\"hidden_size\"]\n",
    "FINAL_HIDDEN_LAYERS = FINAL_CONFIG[\"num_layers\"]\n",
    "FINAL_BIDIRECTIONAL = FINAL_CONFIG[\"bidirectional\"]\n",
    "FINAL_DROPOUT_RATE = FINAL_CONFIG[\"dropout_rate\"]\n",
    "FINAL_LEARNING_RATE = FINAL_CONFIG[\"lr\"]\n",
    "FINAL_L2_LAMBDA = FINAL_CONFIG[\"l2_lambda\"]\n",
    "FINAL_BATCH_SIZE = FINAL_CONFIG[\"batch_size\"]\n",
    "\n",
    "# --- ADD THESE LINES to capture window/stride ---\n",
    "FINAL_WINDOW_SIZE = FINAL_CONFIG[\"window_size\"]\n",
    "FINAL_STRIDE = FINAL_CONFIG[\"stride\"]\n",
    "# --- END ADDED LINES ---\n",
    "\n",
    "FINAL_EXPERIMENT_NAME = f\"{FINAL_MODEL_TYPE}_H{FINAL_HIDDEN_SIZE}_L{FINAL_HIDDEN_LAYERS}_B{FINAL_BIDIRECTIONAL}_Optuna_KFold_Ensemble\"\n",
    "\n",
    "# --- MODIFY THIS LINE to include window info in the name ---\n",
    "submission_filename_base = f\"submission_{FINAL_EXPERIMENT_NAME}_w{FINAL_WINDOW_SIZE}_s{FINAL_STRIDE}.csv\"\n",
    "print(f\"Submission name will be: {submission_filename_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03626778",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è **8. Phase 2: K-Fold Ensemble Training**\n",
    "\n",
    "This is the **NEW** robust training step.\n",
    "\n",
    "Instead of finding one \"best epoch,\" we will train 5 separate models on 5 different splits (folds) of the data. We use the `FINAL_CONFIG` from our HPO search as the configuration for all 5 models.\n",
    "\n",
    "We use the `fit` function's early stopping to find the best version of each model and save its weights (e.g., `kfold_fold_1_best_model.pt`, `kfold_fold_2_best_model.pt`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5cfc58f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting 5-Fold CV Training (Corrected) ---\n",
      "Splitting original engineered data: (661, 160, 36)\n",
      "Using N_FEATURES = 36\n",
      "Using Class Weights: tensor([0.0020, 0.0106, 0.0179], device='cuda:0')\n",
      "\n",
      "--- Fold 1/5 --- (kfold_fold_1) ---\n",
      "  Fold Train Windows: (3696, 100, 36), Fold Val Windows: (931, 100, 36)\n",
      "--- Starting Training: kfold_fold_1 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.3259, F1=0.7580 | Val: Loss=0.2200, F1=0.8100\n",
      "Epoch  25/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.3460, F1=0.9011\n",
      "\n",
      "Early stopping triggered after 36 epochs.\n",
      "Restoring best model from epoch 6 with val_f1 0.9126\n",
      "--- Finished Training: kfold_fold_1 ---\n",
      "Fold 1 Best Model Val F1: 0.9126\n",
      "\n",
      "--- Fold 2/5 --- (kfold_fold_2) ---\n",
      "  Fold Train Windows: (3703, 100, 36), Fold Val Windows: (924, 100, 36)\n",
      "--- Starting Training: kfold_fold_2 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.3107, F1=0.7593 | Val: Loss=0.2510, F1=0.8249\n",
      "Epoch  25/300 | Train: Loss=0.0145, F1=0.9905 | Val: Loss=0.3963, F1=0.9257\n",
      "Epoch  50/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.4064, F1=0.9210\n",
      "\n",
      "Early stopping triggered after 58 epochs.\n",
      "Restoring best model from epoch 28 with val_f1 0.9336\n",
      "--- Finished Training: kfold_fold_2 ---\n",
      "Fold 2 Best Model Val F1: 0.9336\n",
      "\n",
      "--- Fold 3/5 --- (kfold_fold_3) ---\n",
      "  Fold Train Windows: (3703, 100, 36), Fold Val Windows: (924, 100, 36)\n",
      "--- Starting Training: kfold_fold_3 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.3170, F1=0.7458 | Val: Loss=0.2381, F1=0.8239\n",
      "Epoch  25/300 | Train: Loss=0.0031, F1=0.9956 | Val: Loss=0.7220, F1=0.8988\n",
      "\n",
      "Early stopping triggered after 44 epochs.\n",
      "Restoring best model from epoch 14 with val_f1 0.9347\n",
      "--- Finished Training: kfold_fold_3 ---\n",
      "Fold 3 Best Model Val F1: 0.9347\n",
      "\n",
      "--- Fold 4/5 --- (kfold_fold_4) ---\n",
      "  Fold Train Windows: (3703, 100, 36), Fold Val Windows: (924, 100, 36)\n",
      "--- Starting Training: kfold_fold_4 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.2968, F1=0.7692 | Val: Loss=0.2193, F1=0.8350\n",
      "Epoch  25/300 | Train: Loss=0.0009, F1=0.9975 | Val: Loss=0.2822, F1=0.8990\n",
      "\n",
      "Early stopping triggered after 49 epochs.\n",
      "Restoring best model from epoch 19 with val_f1 0.9117\n",
      "--- Finished Training: kfold_fold_4 ---\n",
      "Fold 4 Best Model Val F1: 0.9117\n",
      "\n",
      "--- Fold 5/5 --- (kfold_fold_5) ---\n",
      "  Fold Train Windows: (3703, 100, 36), Fold Val Windows: (924, 100, 36)\n",
      "--- Starting Training: kfold_fold_5 ---\n",
      "Will train for 300 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/300 | Train: Loss=0.3193, F1=0.7543 | Val: Loss=0.2061, F1=0.8423\n",
      "Epoch  25/300 | Train: Loss=0.0055, F1=0.9933 | Val: Loss=0.1527, F1=0.9289\n",
      "Epoch  50/300 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.3671, F1=0.9247\n",
      "\n",
      "Early stopping triggered after 55 epochs.\n",
      "Restoring best model from epoch 25 with val_f1 0.9289\n",
      "--- Finished Training: kfold_fold_5 ---\n",
      "Fold 5 Best Model Val F1: 0.9289\n",
      "\n",
      "--- üèÜ K-Fold Training Complete ---\n",
      "Fold F1 scores: [0.9126, 0.9336, 0.9347, 0.9117, 0.9289]\n",
      "Average F1 across folds: 0.9243\n"
     ]
    }
   ],
   "source": [
    "# --- !! CORRECTED K-FOLD TRAINING CELL (Uses new setup cell) !! ---\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED) \n",
    "print(f\"--- Starting {N_SPLITS}-Fold CV Training (Corrected) ---\")\n",
    "print(f\"Splitting original engineered data: {X_train_full_engineered.shape}\")\n",
    "\n",
    "# --- Use New N_FEATURES and Class Weights ---\n",
    "# These are loaded from our new setup cell\n",
    "N_FEATURES = N_FEATURES_NEW # This is 36\n",
    "class_weights = class_weights_tensor\n",
    "\n",
    "print(f\"Using N_FEATURES = {N_FEATURES}\")\n",
    "print(f\"Using Class Weights: {class_weights}\")\n",
    "\n",
    "fold_val_f1_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_full_engineered, y_train_full)):\n",
    "    fold_name = f\"kfold_fold_{fold+1}\"\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} --- ({fold_name}) ---\")\n",
    "    \n",
    "    X_train_fold_full = X_train_full_engineered[train_idx]\n",
    "    y_train_fold_full = y_train_full[train_idx]\n",
    "    X_val_fold_full = X_train_full_engineered[val_idx]\n",
    "    y_val_fold_full = y_train_full[val_idx]\n",
    "\n",
    "    # --- Scale INSIDE the fold (No-Leakage) ---\n",
    "    scaler_fold = StandardScaler() # <-- Use StandardScaler\n",
    "    \n",
    "    ns, ts, f = X_train_fold_full.shape\n",
    "    X_train_fold_2d = X_train_fold_full.reshape(ns * ts, f)\n",
    "    scaler_fold.fit(X_train_fold_2d)\n",
    "    X_train_scaled_2d = scaler_fold.transform(X_train_fold_2d)\n",
    "    X_train_fold_scaled = X_train_scaled_2d.reshape(ns, ts, f)\n",
    "    \n",
    "    ns_val, ts_val, f_val = X_val_fold_full.shape\n",
    "    X_val_fold_2d = X_val_fold_full.reshape(ns_val * ts_val, f_val)\n",
    "    X_val_scaled_2d = scaler_fold.transform(X_val_fold_2d)\n",
    "    X_val_fold_scaled = X_val_scaled_2d.reshape(ns_val, ts_val, f_val)\n",
    "\n",
    "    # --- Create Sliding Windows (POST-SPLIT) ---\n",
    "    X_train_w, y_train_w, _ = create_sliding_windows(\n",
    "        X_train_fold_scaled, y_train_fold_full, \n",
    "        window_size=FINAL_WINDOW_SIZE, stride=FINAL_STRIDE\n",
    "    )\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(\n",
    "        X_val_fold_scaled, y_val_fold_full, \n",
    "        window_size=FINAL_WINDOW_SIZE, stride=FINAL_STRIDE\n",
    "    )\n",
    "    print(f\"  Fold Train Windows: {X_train_w.shape}, Fold Val Windows: {X_val_w.shape}\")\n",
    "\n",
    "    # --- Create Tensors, datasets and dataloaders --\n",
    "    X_train_fold = torch.from_numpy(X_train_w).float()\n",
    "    y_train_fold = torch.from_numpy(y_train_w).long()\n",
    "    X_val_fold = torch.from_numpy(X_val_w).float()\n",
    "    y_val_fold = torch.from_numpy(y_val_w).long()\n",
    "    train_ds_fold = TensorDataset(X_train_fold, y_train_fold)\n",
    "    val_ds_fold = TensorDataset(X_val_fold, y_val_fold)\n",
    "    \n",
    "    # make_loader is defined in our new setup cell\n",
    "    train_loader_fold = make_loader(train_ds_fold, batch_size=FINAL_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    val_loader_fold = make_loader(val_ds_fold, batch_size=FINAL_BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "    \n",
    "    # --- Create a fresh model (using FINAL_CONFIG) ---\n",
    "    model_fold = RecurrentClassifier(\n",
    "        input_size=N_FEATURES, # This is 36\n",
    "        hidden_size=FINAL_HIDDEN_SIZE, num_layers=FINAL_HIDDEN_LAYERS,\n",
    "        num_classes=N_CLASSES, dropout_rate=FINAL_DROPOUT_RATE,\n",
    "        bidirectional=FINAL_BIDIRECTIONAL, rnn_type=FINAL_MODEL_TYPE\n",
    "    ).to(device)\n",
    "    \n",
    "    if torch.__version__[0] >= \"2\": model_fold = torch.compile(model_fold)\n",
    "    optimizer_fold = torch.optim.AdamW(model_fold.parameters(), lr=FINAL_LEARNING_RATE, weight_decay=FINAL_L2_LAMBDA)\n",
    "    scaler_fold_amp = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    \n",
    "    # --- Create Loss Function with Class Weights ---\n",
    "    criterion_fold = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # --- Train this fold with early stopping ---\n",
    "    model_fold, _, _ = fit(\n",
    "        model=model_fold, train_loader=train_loader_fold,\n",
    "        val_loader=val_loader_fold, epochs=300,\n",
    "        criterion=criterion_fold, optimizer=optimizer_fold,\n",
    "        scaler=scaler_fold_amp, device=device,\n",
    "        writer=None, verbose=25,\n",
    "        experiment_name=fold_name, patience=30\n",
    "    )\n",
    "    \n",
    "    val_loss, val_f1 = validate_one_epoch(model_fold, val_loader_fold, criterion_fold, device)\n",
    "    fold_val_f1_list.append(val_f1)\n",
    "    print(f\"Fold {fold+1} Best Model Val F1: {val_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n--- üèÜ K-Fold Training Complete ---\")\n",
    "print(f\"Fold F1 scores: {[round(f, 4) for f in fold_val_f1_list]}\")\n",
    "print(f\"Average F1 across folds: {np.mean(fold_val_f1_list):.4f}\")\n",
    "\n",
    "# Clean up\n",
    "del X_train_fold, y_train_fold, X_val_fold, y_val_fold\n",
    "del X_train_w, y_train_w, X_val_w, y_val_w\n",
    "del X_train_fold_full, y_train_fold_full, X_val_fold_full, y_val_fold_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f19754",
   "metadata": {
    "id": "markdown-submission"
   },
   "source": [
    "## üì¨ **9. Phase 3: K-Fold Ensemble Submission**\n",
    "\n",
    "This is the **NEW** robust submission step.\n",
    "\n",
    "1.  Prepare the `X_test` data (scaling and windowing) exactly as before.\n",
    "2.  Create the `test_loader`.\n",
    "3.  Load each of our 5 saved fold-models.\n",
    "4.  Get 5 sets of (softmax) probability predictions for the test set.\n",
    "5.  Average these 5 probability sets into a single, robust probability matrix.\n",
    "6.  Aggregate these mean probabilities (using mean) from windows to full samples.\n",
    "7.  Take the `argmax` of the final aggregated probabilities to get the submission class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc486f2",
   "metadata": {
    "id": "code-submission"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing full dataset for FINAL SCALER (36 Features) ---\n",
      "Fitting FINAL StandardScaler on X_train_full_engineered_2d shape: (105760, 36)\n",
      "Loading test data (X_test) for engineering...\n",
      "Original X_test shape: (1324, 160, 35)\n",
      "Identified 13 pirate samples in test set.\n",
      "Created X_test_engineered shape: (1324, 160, 36)\n",
      "Final scaling of test set complete.\n",
      "--- Applying sliding windows to final test set ---\n",
      "\n",
      "Test windowed shape: (9268, 100, 36)\n",
      "Final TestLoader created.\n",
      "\n",
      "--- Generating predictions from 5 fold models ---\n",
      "Loading model 1/5 from models/kfold_fold_1_best_model.pt...\n",
      "Loading model 2/5 from models/kfold_fold_2_best_model.pt...\n",
      "Loading model 3/5 from models/kfold_fold_3_best_model.pt...\n",
      "Loading model 4/5 from models/kfold_fold_4_best_model.pt...\n",
      "Loading model 5/5 from models/kfold_fold_5_best_model.pt...\n",
      "\n",
      "--- Averaging 5 sets of probabilities... ---\n",
      "Mean probability matrix shape: (9268, 3)\n",
      "Aggregating window probabilities to sample predictions (using MEAN)...\n",
      "Aggregated to 1324 final probability vectors.\n",
      "Loading sample submission file for correct formatting...\n",
      "Prediction count matches. Creating submission.\n",
      "\n",
      "Successfully saved to submissions\\submission_GRU_H256_L3_BTrue_Optuna_KFold_Ensemble_w100_s10.csv!\n",
      "This file is correctly formatted for Kaggle:\n",
      "  sample_index    label\n",
      "0          000  no_pain\n",
      "1          001  no_pain\n",
      "2          002  no_pain\n",
      "3          003  no_pain\n",
      "4          004  no_pain\n"
     ]
    }
   ],
   "source": [
    "# --- !! CORRECTED SUBMISSION CELL (Uses new setup cell) !! ---\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "print(\"\\n--- Preparing full dataset for FINAL SCALER (36 Features) ---\")\n",
    "\n",
    "# --- 1. Prepare Full Training Set for Final Scaler ---\n",
    "scaler_final = StandardScaler() # <-- Use StandardScaler\n",
    "ns, ts, f = X_train_full_engineered.shape # Use the 36-feature version\n",
    "X_train_full_2d = X_train_full_engineered.reshape(ns * ts, f)\n",
    "\n",
    "print(f\"Fitting FINAL StandardScaler on X_train_full_engineered_2d shape: {X_train_full_2d.shape}\")\n",
    "scaler_final.fit(X_train_full_2d)\n",
    "\n",
    "# --- 2. Load, Engineer, Scale, and Window the TEST data ---\n",
    "print(\"Loading test data (X_test) for engineering...\")\n",
    "X_test_long = pd.read_csv(X_TEST_PATH) \n",
    "# X_test (35 features) was defined in cell [14]\n",
    "print(f\"Original X_test shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2c. Engineer 'is_pirate' feature for TEST set ---\n",
    "static_cols = ['sample_index', 'n_legs', 'n_hands', 'n_eyes']\n",
    "static_df_test = X_test_long[static_cols].drop_duplicates().set_index('sample_index')\n",
    "pirate_filter_test = (\n",
    "    (static_df_test['n_legs'] == 'one+peg_leg') |\n",
    "    (static_df_test['n_hands'] == 'one+hook_hand') |\n",
    "    (static_df_test['n_eyes'] == 'one+eye_patch')\n",
    ")\n",
    "pirate_indices_test = static_df_test[pirate_filter_test].index\n",
    "print(f\"Identified {len(pirate_indices_test)} pirate samples in test set.\")\n",
    "sample_indices_test_ordered = sorted(X_test_long['sample_index'].unique())\n",
    "is_pirate_map_test = np.array([1 if idx in pirate_indices_test else 0 for idx in sample_indices_test_ordered])\n",
    "pirate_feature_broadcast_test = np.tile(is_pirate_map_test.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "\n",
    "# --- 2d. Concatenate to create X_test_engineered (36 features) ---\n",
    "X_test_engineered = np.concatenate([X_test, pirate_feature_broadcast_test], axis=2)\n",
    "print(f\"Created X_test_engineered shape: {X_test_engineered.shape}\")\n",
    "\n",
    "# --- 2e. Scale X_test_engineered ---\n",
    "ns_test, ts_test, f_test = X_test_engineered.shape\n",
    "X_test_2d = X_test_engineered.reshape(ns_test * ts_test, f_test)\n",
    "X_test_final_scaled_2d = scaler_final.transform(X_test_2d)\n",
    "X_test_final_scaled = X_test_final_scaled_2d.reshape(ns_test, ts_test, f_test)\n",
    "print(\"Final scaling of test set complete.\")\n",
    "\n",
    "# --- 2f. Apply Sliding Windows ---\n",
    "# create_sliding_windows is defined in our new setup cell\n",
    "print(\"--- Applying sliding windows to final test set ---\\n\")\n",
    "X_test_final_windowed, test_window_indices = create_sliding_windows(\n",
    "    X_test_final_scaled, y=None, \n",
    "    window_size=FINAL_WINDOW_SIZE, stride=FINAL_STRIDE\n",
    ")\n",
    "print(f\"Test windowed shape: {X_test_final_windowed.shape}\")\n",
    "\n",
    "# --- 3. Create Final TestLoader ---\n",
    "final_test_features = torch.from_numpy(X_test_final_windowed).float()\n",
    "final_test_ds = TensorDataset(final_test_features)\n",
    "# make_final_loader is defined in our new setup cell\n",
    "test_loader = make_final_loader(final_test_ds, batch_size=FINAL_BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "print(\"Final TestLoader created.\")\n",
    "\n",
    "# --- 4. Get Predictions from all K-Fold Models ---\n",
    "all_fold_probabilities = []\n",
    "# N_SPLITS is defined in our new setup cell\n",
    "print(f\"\\n--- Generating predictions from {N_SPLITS} fold models ---\")\n",
    "\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_name = f\"kfold_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    print(f\"Loading model {fold+1}/{N_SPLITS} from {model_path}...\")\n",
    "\n",
    "    # --- 4a. Create a fresh model shell (with 36 features) ---\n",
    "    model_fold = RecurrentClassifier(\n",
    "        input_size=N_FEATURES_NEW, # Use 36\n",
    "        hidden_size=FINAL_HIDDEN_SIZE, num_layers=FINAL_HIDDEN_LAYERS,\n",
    "        num_classes=N_CLASSES, dropout_rate=FINAL_DROPOUT_RATE,\n",
    "        bidirectional=FINAL_BIDIRECTIONAL, rnn_type=FINAL_MODEL_TYPE\n",
    "    ).to(device)\n",
    "    \n",
    "    # --- 4b. Load the saved weights (with compile-fix) ---\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('_orig_mod.'):\n",
    "            new_state_dict[k[len('_orig_mod.'):]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    model_fold.load_state_dict(new_state_dict)\n",
    "    model_fold.eval()\n",
    "\n",
    "    # --- 4c. Get Softmax probabilities ---\n",
    "    fold_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in test_loader: \n",
    "            inputs = inputs.to(device)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model_fold(inputs)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                fold_predictions.append(probs.cpu().numpy())\n",
    "    all_fold_probabilities.append(np.concatenate(fold_predictions))\n",
    "\n",
    "# --- 5. Average the Probabilities ---\n",
    "print(f\"\\n--- Averaging {len(all_fold_probabilities)} sets of probabilities... ---\")\n",
    "mean_probabilities = np.mean(all_fold_probabilities, axis=0)\n",
    "print(f\"Mean probability matrix shape: {mean_probabilities.shape}\")\n",
    "\n",
    "# --- 6. Aggregate Mean Probabilities (MEAN) ---\n",
    "print(\"Aggregating window probabilities to sample predictions (using MEAN)...\")\n",
    "prob_cols = [f\"prob_{i}\" for i in range(N_CLASSES)]\n",
    "df_probs = pd.DataFrame(mean_probabilities, columns=prob_cols)\n",
    "df_probs['original_index'] = test_window_indices \n",
    "agg_probs = df_probs.groupby('original_index')[prob_cols].mean().values\n",
    "print(f\"Aggregated to {len(agg_probs)} final probability vectors.\")\n",
    "\n",
    "# --- 7. Get Final Predictions and Save ---\n",
    "# le (LabelEncoder) is defined in our new setup cell\n",
    "final_predictions_numeric = np.argmax(agg_probs, axis=1)\n",
    "predicted_labels = le.inverse_transform(final_predictions_numeric)\n",
    "\n",
    "print(\"Loading sample submission file for correct formatting...\")\n",
    "test_sample_indices = sorted(X_test_long['sample_index'].unique())\n",
    "\n",
    "if len(predicted_labels) != len(test_sample_indices):\n",
    "    print(f\"ERROR: Prediction count mismatch!\")\n",
    "else:\n",
    "    print(\"Prediction count matches. Creating submission.\")\n",
    "    final_submission_df = pd.DataFrame({\n",
    "        'sample_index': test_sample_indices,\n",
    "        'label': predicted_labels \n",
    "    })\n",
    "    final_submission_df['sample_index'] = final_submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "\n",
    "    SUBMISSIONS_DIR = \"submissions\"\n",
    "    os.makedirs(SUBMISSIONS_DIR, exist_ok=True)\n",
    "    submission_filename = submission_filename_base \n",
    "    submission_filepath = os.path.join(SUBMISSIONS_DIR, submission_filename)\n",
    "    final_submission_df.to_csv(submission_filepath, index=False)\n",
    "\n",
    "    print(f\"\\nSuccessfully saved to {submission_filepath}!\")\n",
    "    print(\"This file is correctly formatted for Kaggle:\")\n",
    "    print(final_submission_df.head())\n",
    "\n",
    "# Clean up\n",
    "del all_fold_probabilities, final_test_features, final_test_ds, test_loader\n",
    "del X_test_engineered, X_test_final_scaled, X_test_final_windowed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
