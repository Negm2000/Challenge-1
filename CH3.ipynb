{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5222aca",
   "metadata": {
    "id": "markdown-notebook-intro"
   },
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset ðŸ´â€â˜ ï¸ (v5 - Attention + W80/S20)**\n",
    "\n",
    "This notebook tests a new hypothesis:\n",
    "\n",
    "1.  **Remove Static Features:** We have proven these hurt performance (0.918 vs 0.935). This notebook reverts to a Time-Series-only model.\n",
    "2.  **Add Attention Layer:** We will replace the \"last hidden state\" logic with an attention mechanism. The model will now learn to weigh all 80 timesteps to build a context vector for classification.\n",
    "3.  **Keep Winning Window:** We will use the *proven*, high-scoring window parameters (`WINDOW_SIZE = 80`, `STRIDE = 20`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e25d9",
   "metadata": {
    "id": "markdown-libraries"
   },
   "source": [
    "## âš™ï¸ **1. Setup & Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f67f7de3",
   "metadata": {
    "id": "code-libraries"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GPU (RTX 3070, here we come!) ---\n",
      "PyTorch version: 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 123 # Using the seed from your winning CH1 notebook\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "import time\n",
    "from scipy.stats import mode # For submission aggregation\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F # --- NEW: For Attention --- \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Sklearn Imports ---\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Ray[tune] & Optuna Imports ---\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from functools import partial\n",
    "\n",
    "# --- Setup Directories & Device ---\n",
    "logs_dir = \"tensorboard\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU (RTX 3070, here we come!) ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0184001",
   "metadata": {
    "id": "markdown-load-reshape"
   },
   "source": [
    "## ðŸ”„ **2. Data Loading & Reshaping (REVERTED)**\n",
    "\n",
    "Back to the simple, winning pipeline:\n",
    "1.  Define *only* time-series features.\n",
    "2.  Pivot the data to get a 3D tensor of shape `(num_samples, num_timesteps, num_features)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83c7115f",
   "metadata": {
    "id": "code-load-reshape"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 35 time-series features: ['joint_00', 'joint_01', 'joint_02']... to ['pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Loading and reshaping time-series training data...\n",
      "Loading and reshaping time-series test data...\n",
      "\n",
      "X_train_full_ts shape: (661, 160, 35)\n",
      "y_train_labels_str shape: (661,)\n",
      "X_test_ts shape: (1324, 160, 35)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define File Paths and Features ---\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "# Define our features (REVERTED to TS-only)\n",
    "JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(31)]\n",
    "PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "\n",
    "TS_FEATURES = JOINT_FEATURES + PAIN_FEATURES # Time-Series Features\n",
    "N_TS_FEATURES = len(TS_FEATURES)\n",
    "N_TIMESTEPS = 160 # Fixed from our earlier debugging\n",
    "\n",
    "print(f\"Using {N_TS_FEATURES} time-series features: {TS_FEATURES[:3]}... to {TS_FEATURES[-3:]}\")\n",
    "\n",
    "# --- 2. Create the Reshaping Functions ---\n",
    "\n",
    "def reshape_timeseries_data(df, features_list, n_timesteps):\n",
    "    \"\"\"\n",
    "    Pivots the long-format dataframe into a 3D NumPy array.\n",
    "    Shape: (n_samples, n_timesteps, n_features)\n",
    "    \"\"\"\n",
    "    df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "    data_2d = df_pivot.values\n",
    "    n_samples = data_2d.shape[0]\n",
    "    data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "    return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "# --- 3. Load and Reshape Data ---\n",
    "print(\"Loading and reshaping time-series training data...\")\n",
    "X_train_long = pd.read_csv(X_TRAIN_PATH)\n",
    "X_train_full_ts = reshape_timeseries_data(X_train_long, TS_FEATURES, N_TIMESTEPS)\n",
    "\n",
    "print(\"Loading and reshaping time-series test data...\")\n",
    "X_test_long = pd.read_csv(X_TEST_PATH)\n",
    "X_test_ts = reshape_timeseries_data(X_test_long, TS_FEATURES, N_TIMESTEPS)\n",
    "\n",
    "# --- REVERTED: No static data loading ---\n",
    "\n",
    "# Load labels\n",
    "y_train_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "y_train_full_df = y_train_df.sort_values(by='sample_index')\n",
    "y_train_labels_str = y_train_full_df['label'].values\n",
    "\n",
    "print(f\"\\nX_train_full_ts shape: {X_train_full_ts.shape}\")\n",
    "print(f\"y_train_labels_str shape: {y_train_labels_str.shape}\")\n",
    "print(f\"X_test_ts shape: {X_test_ts.shape}\")\n",
    "\n",
    "del X_train_long, X_test_long, y_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d77fc",
   "metadata": {
    "id": "markdown-preprocessing"
   },
   "source": [
    "## ðŸš§ **3. Preprocessing: Window, Split & Scale (REVERTED)**\n",
    "\n",
    "1.  **Encode Labels:** Convert `no_pain`, `low_pain`, `high_pain` to `0`, `1`, `2`.\n",
    "2.  **Create Sliding Windows:** Use the winning `W=80, S=20` parameters.\n",
    "3.  **Split Data:** Use `StratifiedShuffleSplit` on the new *windowed* data.\n",
    "4.  **Scale Features:** Use *one* `StandardScaler` for the time-series data. Fit *only* on the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355a626",
   "metadata": {
    "id": "code-preprocessing"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels encoded. 3 classes: {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
      "\n",
      "Creating sliding windows (W=160, S=0)...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 55\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(new_X_ts), \n\u001b[0;32m     46\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(new_y), \n\u001b[0;32m     47\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(window_to_sample_idx)\n\u001b[0;32m     48\u001b[0m     )\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreating sliding windows (W=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWINDOW_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, S=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTRIDE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m (\n\u001b[0;32m     52\u001b[0m     X_ts_windowed, \n\u001b[0;32m     53\u001b[0m     y_windowed, \n\u001b[0;32m     54\u001b[0m     _ \u001b[38;5;66;03m# We don't need the index map for training\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sliding_windows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_full_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mWINDOW_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSTRIDE\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Our new sequence length is the window size\u001b[39;00m\n\u001b[0;32m     63\u001b[0m N_TIMESTEPS_WINDOWED \u001b[38;5;241m=\u001b[39m WINDOW_SIZE\n",
      "Cell \u001b[1;32mIn[54], line 41\u001b[0m, in \u001b[0;36mcreate_sliding_windows\u001b[1;34m(X_3d_ts, y, window_size, stride)\u001b[0m\n\u001b[0;32m     39\u001b[0m         new_X_ts\u001b[38;5;241m.\u001b[39mappend(window)\n\u001b[0;32m     40\u001b[0m         new_y\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m---> 41\u001b[0m         \u001b[43mwindow_to_sample_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Track original sample\u001b[39;00m\n\u001b[0;32m     42\u001b[0m         idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m stride\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(new_X_ts), \n\u001b[0;32m     46\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(new_y), \n\u001b[0;32m     47\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(window_to_sample_idx)\n\u001b[0;32m     48\u001b[0m )\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 1. Encode Labels ---\n",
    "LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "le = LabelEncoder()\n",
    "le.fit(list(LABEL_MAPPING.keys()))\n",
    "y_train_full = le.transform(y_train_labels_str)\n",
    "N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "print(f\"Labels encoded. {N_CLASSES} classes: {LABEL_MAPPING}\")\n",
    "\n",
    "# --- 2. Create Sliding Windows ---\n",
    "# --- Using W=80, S=20 from your confirmed winner ---\n",
    "WINDOW_SIZE = 80\n",
    "STRIDE = 20\n",
    "# ---\n",
    "\n",
    "# --- REVERTED: Simplified create_sliding_windows --- \n",
    "def create_sliding_windows(X_3d_ts, y, window_size, stride):\n",
    "    \"\"\"\n",
    "    Takes 3D time-series and 1D labels\n",
    "    and creates overlapping windows.\n",
    "    Returns:\n",
    "    - new_X_ts (4D): (n_windows, window_size, n_ts_features)\n",
    "    - new_y (1D): (n_windows,)\n",
    "    - window_to_sample_idx (1D): (n_windows,) mapping to original sample\n",
    "    \"\"\"\n",
    "    new_X_ts = []\n",
    "    new_y = []\n",
    "    window_to_sample_idx = []\n",
    "    \n",
    "    n_samples, n_timesteps, n_features = X_3d_ts.shape\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        sample_ts = X_3d_ts[i]\n",
    "        label = y[i]\n",
    "        \n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            window = sample_ts[idx : idx + window_size]\n",
    "            new_X_ts.append(window)\n",
    "            new_y.append(label)\n",
    "            window_to_sample_idx.append(i) # Track original sample\n",
    "            idx += stride\n",
    "            \n",
    "    return (\n",
    "        np.array(new_X_ts), \n",
    "        np.array(new_y), \n",
    "        np.array(window_to_sample_idx)\n",
    "    )\n",
    "\n",
    "print(f\"\\nCreating sliding windows (W={WINDOW_SIZE}, S={STRIDE})...\")\n",
    "(\n",
    "    X_ts_windowed, \n",
    "    y_windowed, \n",
    "    _ # We don't need the index map for training\n",
    ") = create_sliding_windows(\n",
    "    X_train_full_ts, \n",
    "    y_train_full, \n",
    "    WINDOW_SIZE, \n",
    "    STRIDE\n",
    ")\n",
    "\n",
    "# Our new sequence length is the window size\n",
    "N_TIMESTEPS_WINDOWED = WINDOW_SIZE\n",
    "\n",
    "print(f\"Data augmented with sliding windows:\")\n",
    "print(f\"  Original TS shape: {X_train_full_ts.shape}\")\n",
    "print(f\"  Windowed TS shape: {X_ts_windowed.shape}\")\n",
    "print(f\"  Windowed y shape: {y_windowed.shape}\")\n",
    "\n",
    "\n",
    "# --- 3. Create Validation Split (on windowed data) ---\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "\n",
    "for train_idx, val_idx in sss.split(X_ts_windowed, y_windowed):\n",
    "    X_ts_train_split = X_ts_windowed[train_idx]\n",
    "    y_train_split = y_windowed[train_idx]\n",
    "    \n",
    "    X_ts_val_split = X_ts_windowed[val_idx]\n",
    "    y_val_split = y_windowed[val_idx]\n",
    "\n",
    "print(f\"\\nData split into Train and Validation sets:\")\n",
    "print(f\"  X_ts_train_split:     {X_ts_train_split.shape}\")\n",
    "print(f\"  y_train_split:        {y_train_split.shape}\")\n",
    "print(f\"  X_ts_val_split:       {X_ts_val_split.shape}\")\n",
    "print(f\"  y_val_split:          {y_val_split.shape}\")\n",
    "\n",
    "# --- 4. Scale Features (The \"No-Cheating\" Rule) ---\n",
    "\n",
    "# --- REVERTED: Only one scaler for TS data ---\n",
    "scaler_ts = StandardScaler()\n",
    "ns, ts, f = X_ts_train_split.shape\n",
    "X_ts_train_2d = X_ts_train_split.reshape(ns * ts, f)\n",
    "print(f\"\\nFitting Time-Series Scaler on X_ts_train_2d shape: {X_ts_train_2d.shape}\")\n",
    "scaler_ts.fit(X_ts_train_2d)\n",
    "\n",
    "# Transform TS Train\n",
    "X_ts_train_scaled_2d = scaler_ts.transform(X_ts_train_2d)\n",
    "X_ts_train_scaled = X_ts_train_scaled_2d.reshape(ns, ts, f)\n",
    "\n",
    "# Transform TS Val\n",
    "ns_val, ts_val, f_val = X_ts_val_split.shape\n",
    "X_ts_val_2d = X_ts_val_split.reshape(ns_val * ts_val, f_val)\n",
    "X_ts_val_scaled_2d = scaler_ts.transform(X_ts_val_2d)\n",
    "X_ts_val_scaled = X_ts_val_scaled_2d.reshape(ns_val, ts_val, f_val)\n",
    "\n",
    "print(\"\\nScaling complete.\")\n",
    "print(f\"  X_ts_train_scaled:     {X_ts_train_scaled.shape}\")\n",
    "print(f\"  X_ts_val_scaled:       {X_ts_val_scaled.shape}\")\n",
    "\n",
    "del X_ts_train_2d, X_ts_val_2d, X_ts_train_scaled_2d, X_ts_val_scaled_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c883b33",
   "metadata": {
    "id": "markdown-dataloaders"
   },
   "source": [
    "## ðŸšš **4. PyTorch DataLoaders (REVERTED)**\n",
    "\n",
    "We now create a `TensorDataset` that holds **two** items:\n",
    "1.  Time-series features\n",
    "2.  Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8702f5a",
   "metadata": {
    "id": "code-dataloaders"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDatasets created.\n",
      "Example train_ds[0] shapes:\n",
      "  TS features:  torch.Size([80, 35])\n",
      "  Target:       torch.Size([])\n",
      "\n",
      "DataLoaders will be created inside the tuning loop.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Convert to Tensors ---\n",
    "# Train\n",
    "train_ts_features = torch.from_numpy(X_ts_train_scaled).float()\n",
    "train_targets = torch.from_numpy(y_train_split).long()\n",
    "\n",
    "# Validation\n",
    "val_ts_features = torch.from_numpy(X_ts_val_scaled).float()\n",
    "val_targets = torch.from_numpy(y_val_split).long()\n",
    "\n",
    "# --- 2. Create TensorDatasets ---\n",
    "train_ds = TensorDataset(train_ts_features, train_targets)\n",
    "val_ds = TensorDataset(val_ts_features, val_targets)\n",
    "\n",
    "print(f\"TensorDatasets created.\")\n",
    "print(f\"Example train_ds[0] shapes:\")\n",
    "print(f\"  TS features:  {train_ds[0][0].shape}\")\n",
    "print(f\"  Target:       {train_ds[0][1].shape}\")\n",
    "\n",
    "# --- 3. Define make_loader function (from Lecture 4) ---\n",
    "BATCH_SIZE = 128 # This will be our default, but Optuna can tune it\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    num_workers = 0 \n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=int(batch_size), \n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=None,\n",
    "    )\n",
    "\n",
    "# --- 4. Create DataLoaders ---\n",
    "print(\"\\nDataLoaders will be created inside the tuning loop.\")\n",
    "del X_ts_train_scaled, val_ts_features, train_ts_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16263592",
   "metadata": {
    "id": "markdown-model-training"
   },
   "source": [
    "## ðŸ› ï¸ **5. Model & Training Engine (MODIFIED FOR ATTENTION)**\n",
    "\n",
    "-   `RecurrentClassifier`: **Modified** to use an Attention layer. It no longer uses the last hidden state, but a weighted sum of *all* hidden states.\n",
    "-   `train_one_epoch` / `validate_one_epoch`: **Reverted** to handle 2-part batches `(ts_inputs, targets)`.\n",
    "-   `objective_function`: **Modified** to build the new Attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06613e58",
   "metadata": {
    "id": "code-recurrent-summary"
   },
   "outputs": [],
   "source": [
    "def recurrent_summary(model, ts_input_size):\n",
    "    \"\"\"\n",
    "    Custom summary function (REVERTED) for 1-input model.\n",
    "    \"\"\"\n",
    "    output_shapes = {}\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1 \n",
    "                if isinstance(output[1], tuple): \n",
    "                    shape2 = list(output[1][0].shape)\n",
    "                else:\n",
    "                    shape2 = list(output[1].shape)\n",
    "                shape2[1] = -1\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "        return hook\n",
    "\n",
    "    try:\n",
    "        device_summary = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device_summary = torch.device(\"cpu\")\n",
    "\n",
    "    dummy_input_ts = torch.randn(1, *ts_input_size).to(device_summary)\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        # --- NEW: Also capture attention layers ---\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(dummy_input_ts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4fc09",
   "metadata": {
    "id": "code-recurrent-classifier"
   },
   "outputs": [],
   "source": [
    "# --- MODIFIED --- RecurrentClassifier with Attention --- \n",
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    MODIFIED to use an Attention mechanism instead of the last hidden state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,         # N_TS_FEATURES\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "        \n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        if self.bidirectional:\n",
    "            rnn_output_size = hidden_size * 2\n",
    "        else:\n",
    "            rnn_output_size = hidden_size\n",
    "        \n",
    "        # --- NEW: Attention Layers ---\n",
    "        # Bahdanau-style attention\n",
    "        # We'll use the rnn_output_size as the attention dimension\n",
    "        self.attention_W = nn.Linear(rnn_output_size, rnn_output_size)\n",
    "        self.attention_v = nn.Linear(rnn_output_size, 1)\n",
    "        \n",
    "        # --- NEW: Classifier takes context vector as input ---\n",
    "        self.classifier = nn.Linear(rnn_output_size, num_classes)\n",
    "\n",
    "    def forward(self, x_ts): # --- REVERTED: one input ---\n",
    "        \"\"\"\n",
    "        x_ts shape: (batch_size, seq_length, input_size)\n",
    "        \"\"\"\n",
    "        # rnn_out shape: (batch_size, seq_length, rnn_output_size)\n",
    "        # hidden shape (for GRU): (num_layers * num_directions, batch_size, hidden_size)\n",
    "        rnn_out, hidden = self.rnn(x_ts)\n",
    "\n",
    "        # --- NEW: Attention Logic ---\n",
    "        # u shape: (batch_size, seq_length, rnn_output_size)\n",
    "        u = torch.tanh(self.attention_W(rnn_out))\n",
    "        # scores shape: (batch_size, seq_length, 1)\n",
    "        scores = self.attention_v(u)\n",
    "        # scores shape: (batch_size, seq_length)\n",
    "        scores = scores.squeeze(2)\n",
    "        \n",
    "        # weights shape: (batch_size, seq_length)\n",
    "        weights = F.softmax(scores, dim=1)\n",
    "        \n",
    "        # --- Context Vector Calculation ---\n",
    "        # weights shape: (batch_size, 1, seq_length)\n",
    "        weights = weights.unsqueeze(1)\n",
    "        \n",
    "        # context_vector shape: (batch_size, 1, rnn_output_size)\n",
    "        context_vector = torch.bmm(weights, rnn_out)\n",
    "        \n",
    "        # context_vector shape: (batch_size, rnn_output_size)\n",
    "        context_vector = context_vector.squeeze(1)\n",
    "        \n",
    "        # --- Classification ---\n",
    "        # Classify based on the context vector, not the last hidden state\n",
    "        logits = self.classifier(context_vector)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b67a46",
   "metadata": {
    "id": "code-training-loop-MODIFIED"
   },
   "outputs": [],
   "source": [
    "# --- REVERTED --- Training & Validation Loops ---\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # --- REVERTED: Unpack 2-part batch ---\n",
    "    for batch_idx, (ts_inputs, targets) in enumerate(train_loader):\n",
    "        ts_inputs, targets = ts_inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            # --- REVERTED: Pass one input to model ---\n",
    "            logits = model(ts_inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            \n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * ts_inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # --- REVERTED: Unpack 2-part batch ---\n",
    "        for (ts_inputs, targets) in val_loader:\n",
    "            ts_inputs, targets = ts_inputs.to(device), targets.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                # --- REVERTED: Pass one input to model ---\n",
    "                logits = model(ts_inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item() * ts_inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
    "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "\n",
    "# --- MODIFIED --- Objective Function (for Attention Model) ---\n",
    "def objective_function(config, train_ds, val_ds):\n",
    "    \"\"\"\n",
    "    This is the main function that Ray Tune will call for each trial.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Create DataLoaders with the tuned batch size ---\n",
    "    train_loader = make_loader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "    val_loader = make_loader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=False)\n",
    "    \n",
    "    # --- 2. Create Model --- \n",
    "    model = RecurrentClassifier(\n",
    "        input_size=N_TS_FEATURES, # Time-series features\n",
    "        # --- REVERTED: No static_input_size ---\n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        num_classes=N_CLASSES,\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        bidirectional=config[\"bidirectional\"],\n",
    "        rnn_type=config[\"rnn_type\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    if torch.__version__[0] >= \"2\":\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    # --- 3. Create Optimizer, Loss, Scaler ---\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "    # --- 4. The Training Loop ---\n",
    "    EPOCHS = 200 \n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, 0, config[\"l2_lambda\"]\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # --- Send Results to Ray Tune --- \n",
    "        tune.report({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_f1\": val_f1\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a903e3",
   "metadata": {
    "id": "markdown-optuna-search"
   },
   "source": [
    "## ðŸ§ª **6. Hyperparameter Search with Ray Tune & Optuna**\n",
    "\n",
    "This cell is unchanged from CH1. It will now automatically use the new model and data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8c96b",
   "metadata": {
    "id": "code-optuna-search"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-08 18:54:23</td></tr>\n",
       "<tr><td>Running for: </td><td>00:10:44.08        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.5/13.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=20<br>Bracket: Iter 80.000: 0.9894015467509714 | Iter 40.000: 0.9870962865194832 | Iter 20.000: 0.9778667651442038<br>Logical resource usage: 4.0/16 CPUs, 0.25/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th>bidirectional  </th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  l2_lambda</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th>rnn_type  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_f1</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_function_986dfe0c</td><td>TERMINATED</td><td>127.0.0.1:48140</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.452793</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">3.62168e-05</td><td style=\"text-align: right;\">0.000153837</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         65.6878</td><td style=\"text-align: right;\">  0.0325408 </td><td style=\"text-align: right;\">  0.995697</td><td style=\"text-align: right;\"> 0.0876138</td></tr>\n",
       "<tr><td>objective_function_ad8b749a</td><td>TERMINATED</td><td>127.0.0.1:21952</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.569923</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">7.1905e-07 </td><td style=\"text-align: right;\">1.67379e-05</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         32.6202</td><td style=\"text-align: right;\">  0.453067  </td><td style=\"text-align: right;\">  0.70966 </td><td style=\"text-align: right;\"> 0.432193 </td></tr>\n",
       "<tr><td>objective_function_d91804ed</td><td>TERMINATED</td><td>127.0.0.1:6708 </td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.470955</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.63502e-06</td><td style=\"text-align: right;\">0.00107001 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        227.023 </td><td style=\"text-align: right;\">  0.00906745</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.0565662</td></tr>\n",
       "<tr><td>objective_function_c0f52ea7</td><td>TERMINATED</td><td>127.0.0.1:29164</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.454506</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.34284e-07</td><td style=\"text-align: right;\">0.0044276  </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         74.3563</td><td style=\"text-align: right;\">  0.130776  </td><td style=\"text-align: right;\">  0.951424</td><td style=\"text-align: right;\"> 0.143686 </td></tr>\n",
       "<tr><td>objective_function_661e5469</td><td>TERMINATED</td><td>127.0.0.1:52908</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.347087</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000436213</td><td style=\"text-align: right;\">0.000894897</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         97.9643</td><td style=\"text-align: right;\">  0.0690838 </td><td style=\"text-align: right;\">  0.993747</td><td style=\"text-align: right;\"> 0.114196 </td></tr>\n",
       "<tr><td>objective_function_8430a86d</td><td>TERMINATED</td><td>127.0.0.1:41868</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.193181</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">3.28684e-07</td><td style=\"text-align: right;\">0.000146637</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        292.942 </td><td style=\"text-align: right;\">  0.0018363 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.0719095</td></tr>\n",
       "<tr><td>objective_function_a17c8899</td><td>TERMINATED</td><td>127.0.0.1:31588</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.443734</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">6.77458e-07</td><td style=\"text-align: right;\">0.00134108 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        116.431 </td><td style=\"text-align: right;\">  0.00164573</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.0738939</td></tr>\n",
       "<tr><td>objective_function_d7ca4996</td><td>TERMINATED</td><td>127.0.0.1:40904</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.122842</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">6.8781e-05 </td><td style=\"text-align: right;\">0.00765522 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         37.779 </td><td style=\"text-align: right;\">  0.0802065 </td><td style=\"text-align: right;\">  0.994519</td><td style=\"text-align: right;\"> 0.0625604</td></tr>\n",
       "<tr><td>objective_function_0be76d5e</td><td>TERMINATED</td><td>127.0.0.1:44728</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.53442 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">8.94372e-05</td><td style=\"text-align: right;\">0.00159791 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        135.483 </td><td style=\"text-align: right;\">  0.0508805 </td><td style=\"text-align: right;\">  0.996945</td><td style=\"text-align: right;\"> 0.0585889</td></tr>\n",
       "<tr><td>objective_function_d1f1a9a3</td><td>TERMINATED</td><td>127.0.0.1:11288</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.183084</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000272238</td><td style=\"text-align: right;\">0.000127026</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         78.1207</td><td style=\"text-align: right;\">  0.276122  </td><td style=\"text-align: right;\">  0.964027</td><td style=\"text-align: right;\"> 0.110635 </td></tr>\n",
       "<tr><td>objective_function_f37672c1</td><td>TERMINATED</td><td>127.0.0.1:24056</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.453994</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.15374e-05</td><td style=\"text-align: right;\">3.39847e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         82.9502</td><td style=\"text-align: right;\">  0.299281  </td><td style=\"text-align: right;\">  0.922565</td><td style=\"text-align: right;\"> 0.199533 </td></tr>\n",
       "<tr><td>objective_function_1a27c266</td><td>TERMINATED</td><td>127.0.0.1:3544 </td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.546735</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">4.19283e-05</td><td style=\"text-align: right;\">0.00492667 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         22.2462</td><td style=\"text-align: right;\">  0.0568539 </td><td style=\"text-align: right;\">  0.996488</td><td style=\"text-align: right;\"> 0.0586068</td></tr>\n",
       "<tr><td>objective_function_b48f3b55</td><td>TERMINATED</td><td>127.0.0.1:46752</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.336156</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">3.93687e-07</td><td style=\"text-align: right;\">0.000351153</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        220.47  </td><td style=\"text-align: right;\">  0.00107559</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.0667517</td></tr>\n",
       "<tr><td>objective_function_699a95a1</td><td>TERMINATED</td><td>127.0.0.1:19076</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.340594</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">2.66626e-06</td><td style=\"text-align: right;\">0.000769216</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         27.0426</td><td style=\"text-align: right;\">  0.0306088 </td><td style=\"text-align: right;\">  0.99453 </td><td style=\"text-align: right;\"> 0.0919862</td></tr>\n",
       "<tr><td>objective_function_0e3b2432</td><td>TERMINATED</td><td>127.0.0.1:2540 </td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.359476</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">3.90584e-06</td><td style=\"text-align: right;\">0.000868684</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         32.7623</td><td style=\"text-align: right;\">  0.0405343 </td><td style=\"text-align: right;\">  0.991762</td><td style=\"text-align: right;\"> 0.0767486</td></tr>\n",
       "<tr><td>objective_function_96940b6e</td><td>TERMINATED</td><td>127.0.0.1:30604</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.340542</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.95822e-06</td><td style=\"text-align: right;\">0.00106467 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         31.8076</td><td style=\"text-align: right;\">  0.0282689 </td><td style=\"text-align: right;\">  0.992222</td><td style=\"text-align: right;\"> 0.0791066</td></tr>\n",
       "<tr><td>objective_function_a48ad8fe</td><td>TERMINATED</td><td>127.0.0.1:40516</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.404493</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">3.07731e-06</td><td style=\"text-align: right;\">0.00189224 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        160.005 </td><td style=\"text-align: right;\">  0.0071312 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.0604236</td></tr>\n",
       "<tr><td>objective_function_d5b2f68b</td><td>TERMINATED</td><td>127.0.0.1:43964</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.394564</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.66537e-06</td><td style=\"text-align: right;\">0.00214002 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         81.3095</td><td style=\"text-align: right;\">  0.00352539</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.0370345</td></tr>\n",
       "<tr><td>objective_function_d91b8bec</td><td>TERMINATED</td><td>127.0.0.1:49688</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.41019 </td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">8.35218e-07</td><td style=\"text-align: right;\">0.000359916</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         23.1125</td><td style=\"text-align: right;\">  0.0745719 </td><td style=\"text-align: right;\">  0.975462</td><td style=\"text-align: right;\"> 0.0841934</td></tr>\n",
       "<tr><td>objective_function_c89ca2d3</td><td>TERMINATED</td><td>127.0.0.1:48692</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.487133</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">7.98005e-06</td><td style=\"text-align: right;\">0.00247503 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         76.4366</td><td style=\"text-align: right;\">  0.0100213 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.0249546</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-08 18:43:55,187 E 52416 45196] (gcs_server.exe) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-08 18:44:00,136 E 42624 51856] (raylet.exe) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "2025-11-08 18:54:23,605\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'c:/Users/Karim Negm/Documents/AN2DL/Challenge 1/ray_results/pirate_pain_optuna_search_v5_attention_w80_s20' in 0.0514s.\n",
      "2025-11-08 18:54:23,630\tINFO tune.py:1041 -- Total run time: 644.20 seconds (644.01 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Search Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define the Search Space for Optuna ---\n",
    "search_space = {\n",
    "    \"rnn_type\": tune.choice(['GRU', 'LSTM']),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),           # Widen the learning rate\n",
    "    \"batch_size\": tune.choice([64, 128, 256]),  \n",
    "    \"hidden_size\": tune.choice([128, 256, 384]),# Let's try bigger models\n",
    "    \"num_layers\": tune.choice([2, 3]),       # From CH1\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.6),     # Widen the dropout range\n",
    "    \"bidirectional\": tune.choice([True, False]),\n",
    "    \"l2_lambda\": tune.loguniform(1e-7, 1e-3)      # Widen the L2 range\n",
    "}\n",
    "\n",
    "# --- 2. Define the Optimizer (Optuna) and Scheduler (ASHA) ---\n",
    "optuna_search = OptunaSearch(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\",\n",
    "    grace_period=20,  # Min epochs a trial must run\n",
    "    reduction_factor=2  # How aggressively to stop trials\n",
    ")\n",
    "\n",
    "# --- 3. Initialize Ray ---\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "ray_logs_path = os.path.abspath(\"./ray_results\")\n",
    "os.makedirs(ray_logs_path, exist_ok=True)\n",
    "os.environ[\"RAY_TEMP_DIR\"] = ray_logs_path\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=16, # From CH1\n",
    "    num_gpus=1, \n",
    "    ignore_reinit_error=True\n",
    ")\n",
    "\n",
    "def short_trial_name(trial):\n",
    "    \"\"\"Creates a short, unique name for each trial folder.\"\"\"\n",
    "    return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "\n",
    "# --- 4. Run the Tuner ---\n",
    "print(\"Starting hyperparameter search...\")\n",
    "\n",
    "# --- REVERTED: Pass the 2-tensor datasets ---\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(objective_function, train_ds=train_ds, val_ds=val_ds),\n",
    "    \n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 0.25}, # From CH1\n",
    "    \n",
    "    config=search_space,\n",
    "    num_samples=20, # Number of different HPO trials to run\n",
    "    search_alg=optuna_search,\n",
    "    scheduler=scheduler,\n",
    "    name=\"pirate_pain_optuna_search_v5_attention_w80_s20\",\n",
    "\n",
    "    storage_path=ray_logs_path,\n",
    "    \n",
    "    trial_dirname_creator=short_trial_name,\n",
    "    \n",
    "    log_to_file=True,\n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "print(\"\\n--- Search Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f34508",
   "metadata": {
    "id": "get-best-results"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting best trial from analysis...\n",
      "Best validation F1 score: 0.9909\n",
      "Best hyperparameters found:\n",
      "{'rnn_type': 'GRU', 'lr': 0.0021400225285473223, 'batch_size': 256, 'hidden_size': 128, 'num_layers': 3, 'dropout_rate': 0.39456373006338397, 'bidirectional': True, 'l2_lambda': 1.6653691902591887e-06}\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Get Best Results ---\n",
    "print(\"Getting best trial from analysis...\")\n",
    "best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "if best_trial:\n",
    "    best_config = best_trial.config\n",
    "    best_val_f1 = best_trial.last_result[\"val_f1\"]\n",
    "    \n",
    "    print(f\"Best validation F1 score: {best_val_f1:.4f}\")\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(best_config)\n",
    "else:\n",
    "    print(\"ERROR: No trials completed successfully. Check the 'ray_results' folder for logs.\")\n",
    "    best_config = None # Handle the case where all trials failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b564a8e",
   "metadata": {
    "id": "markdown-submission-config"
   },
   "source": [
    "## ðŸ† **7. Final Model Configuration**\n",
    "\n",
    "This cell is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5802d6",
   "metadata": {
    "id": "code-submission-config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ† Final Configuration Set --- \n",
      "Best Val F1 from search: 0.9909\n",
      "{'rnn_type': 'GRU', 'lr': 0.0021400225285473223, 'batch_size': 256, 'hidden_size': 128, 'num_layers': 3, 'dropout_rate': 0.39456373006338397, 'bidirectional': True, 'l2_lambda': 1.6653691902591887e-06}\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# --- ðŸ† FINAL MODEL CONFIGURATION ðŸ† ---\n",
    "# ===================================================================\n",
    "FINAL_CONFIG = best_config\n",
    "FINAL_BEST_VAL_F1 = best_val_f1\n",
    "\n",
    "print(\"--- ðŸ† Final Configuration Set --- \")\n",
    "print(f\"Best Val F1 from search: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "print(FINAL_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03626778",
   "metadata": {},
   "source": [
    "### **Re-run to find Best Epoch (MODIFIED)**\n",
    "\n",
    "The `fit` function is now the **REVERTED** version that handles the 2-part data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc58f7",
   "metadata": {
    "id": "code-find-best-epoch"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finding best epoch number for the winning model ---\n",
      "--- Starting Training: final_check_v5_attention ---\n",
      "Will train for 200 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/200 | Train: Loss=0.6526, F1=0.7190 | Val: Loss=0.4962, F1=0.7917\n",
      "Epoch  10/200 | Train: Loss=0.0900, F1=0.9683 | Val: Loss=0.1535, F1=0.9545\n",
      "Epoch  20/200 | Train: Loss=0.0143, F1=0.9953 | Val: Loss=0.0423, F1=0.9850\n",
      "Epoch  30/200 | Train: Loss=0.0002, F1=1.0000 | Val: Loss=0.0188, F1=0.9939\n",
      "Epoch  40/200 | Train: Loss=0.0001, F1=1.0000 | Val: Loss=0.0202, F1=0.9939\n",
      "Epoch  50/200 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.0208, F1=0.9939\n",
      "\n",
      "Early stopping triggered after 58 epochs.\n",
      "Restoring best model from epoch 28 with val_f1 0.9939\n",
      "--- Finished Training: final_check_v5_attention ---\n",
      "\n",
      "--- ðŸ† Optimal Epochs Found: 28 ---\n",
      "Submission name will be: submission_GRU_H128_L3_BTrue_D0.3946_Attention_Window_w80_s20_Optuna_FINAL.csv\n"
     ]
    }
   ],
   "source": [
    "# --- We need the original 'fit' function back (REVERTED) ---\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    print(f\"Will train for {epochs} epochs with patience={patience} monitoring {evaluation_metric}\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # --- REVERTED: Use 2-part-batch train/val functions ---\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        # --- End modifications ---\n",
    "\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
    "            )\n",
    "\n",
    "        if verbose > 0 and (epoch % verbose == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                  f\"Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n",
    "                  f\"Val: Loss={val_loss:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    if restore_best_weights and patience > 0:\n",
    "        print(f\"Restoring best model from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    if patience == 0:\n",
    "        print(\"Training complete. Saving final model.\")\n",
    "        torch.save(model.state_dict(), model_path.replace(\"_best_model.pt\", \"_final_model.pt\"))\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    \n",
    "    print(f\"--- Finished Training: {experiment_name} ---\")\n",
    "    return model, training_history, best_epoch if 'best_epoch' in locals() else epochs\n",
    "\n",
    "# --- 1. Create DataLoaders for the best config ---\n",
    "best_batch_size = FINAL_CONFIG[\"batch_size\"]\n",
    "# --- REVERTED: Use the 2-tensor datasets ---\n",
    "train_loader_final_check = make_loader(train_ds, batch_size=best_batch_size, shuffle=True, drop_last=True)\n",
    "val_loader_final_check = make_loader(val_ds, batch_size=best_batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# --- 2. Instantiate Fresh Model --- \n",
    "# --- MODIFIED: Build Attention model ---\n",
    "final_check_model = RecurrentClassifier(\n",
    "    input_size=N_TS_FEATURES,\n",
    "    # --- REVERTED: No static_input_size ---\n",
    "    hidden_size=FINAL_CONFIG[\"hidden_size\"],\n",
    "    num_layers=FINAL_CONFIG[\"num_layers\"],\n",
    "    num_classes=N_CLASSES,\n",
    "    dropout_rate=FINAL_CONFIG[\"dropout_rate\"],\n",
    "    bidirectional=FINAL_CONFIG[\"bidirectional\"],\n",
    "    rnn_type=FINAL_CONFIG[\"rnn_type\"]\n",
    ").to(device)\n",
    "\n",
    "if torch.__version__[0] >= \"2\":\n",
    "    final_check_model = torch.compile(final_check_model)\n",
    "\n",
    "final_check_optimizer = torch.optim.AdamW(final_check_model.parameters(), lr=FINAL_CONFIG[\"lr\"], weight_decay=FINAL_CONFIG[\"l2_lambda\"])\n",
    "final_check_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "final_check_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- 3. Run Training with Early Stopping ---\n",
    "print(\"--- Finding best epoch number for the winning model ---\")\n",
    "_, _, FINAL_BEST_EPOCH = fit(\n",
    "    model=final_check_model,\n",
    "    train_loader=train_loader_final_check,\n",
    "    val_loader=val_loader_final_check,\n",
    "    epochs=200, # Max epochs\n",
    "    criterion=final_check_criterion,\n",
    "    optimizer=final_check_optimizer,\n",
    "    scaler=final_check_scaler,\n",
    "    device=device,\n",
    "    writer=None, # No need to log this one\n",
    "    verbose=10,\n",
    "    experiment_name=\"final_check_v5_attention\",\n",
    "    patience=30 # Use a reasonable patience\n",
    ")\n",
    "\n",
    "print(f\"\\n--- ðŸ† Optimal Epochs Found: {FINAL_BEST_EPOCH} ---\")\n",
    "\n",
    "# --- 4. Set variables for the submission cell ---\n",
    "FINAL_MODEL_TYPE = FINAL_CONFIG[\"rnn_type\"]\n",
    "FINAL_HIDDEN_SIZE = FINAL_CONFIG[\"hidden_size\"]\n",
    "FINAL_HIDDEN_LAYERS = FINAL_CONFIG[\"num_layers\"]\n",
    "FINAL_BIDIRECTIONAL = FINAL_CONFIG[\"bidirectional\"]\n",
    "FINAL_DROPOUT_RATE = FINAL_CONFIG[\"dropout_rate\"]\n",
    "FINAL_LEARNING_RATE = FINAL_CONFIG[\"lr\"]\n",
    "FINAL_L2_LAMBDA = FINAL_CONFIG[\"l2_lambda\"]\n",
    "FINAL_BATCH_SIZE = FINAL_CONFIG[\"batch_size\"]\n",
    "\n",
    "FINAL_EXPERIMENT_NAME = (\n",
    "    f\"{FINAL_MODEL_TYPE}_H{FINAL_HIDDEN_SIZE}_L{FINAL_HIDDEN_LAYERS}_B{FINAL_BIDIRECTIONAL}\"\n",
    "    f\"_D{FINAL_DROPOUT_RATE:.4f}_Attention_Window_w{WINDOW_SIZE}_s{STRIDE}_Optuna_FINAL\"\n",
    ")\n",
    "\n",
    "print(f\"Submission name will be: submission_{FINAL_EXPERIMENT_NAME}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f19754",
   "metadata": {
    "id": "markdown-submission"
   },
   "source": [
    "## ðŸ“¬ **8. Create Submission (REVERTED & MODIFIED)**\n",
    "\n",
    "This cell is back to the simpler logic, but modified for the Attention model.\n",
    "1.  Re-scale all TS data on the *full* training set.\n",
    "2.  Apply **sliding windows** to the *full* training set and the *full* test set.\n",
    "3.  Train a new Attention model on the *full windowed* training set.\n",
    "4.  Generate predictions on the *windowed test set*.\n",
    "5.  **Aggregate** the windowed predictions using a **majority vote**.\n",
    "6.  Save the final aggregated submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc486f2",
   "metadata": {
    "id": "code-submission"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing full dataset for final training ---\n",
      "Fitting FINAL TS Scaler on X_train_full_ts_2d shape: (105760, 35)\n",
      "Final scaling complete.\n",
      "--- Applying sliding windows (W=80, S=20) to final dataset ---\n",
      "Full train windowed TS shape: (3305, 80, 35)\n",
      "Test windowed TS shape: (6620, 80, 35)\n",
      "Test window-to-sample map shape: (6620,)\n",
      "Final DataLoaders created.\n",
      "\n",
      "--- Building FINAL model for submission: GRU_H128_L3_BTrue_D0.3946_Attention_Window_w80_s20_Optuna_FINAL ---\n",
      "Compiling final model...\n",
      "Training final model for 28 epochs on ALL data...\n",
      "Final Training Epoch   1/28 | Train: Loss=0.6145, F1=0.7279\n",
      "Final Training Epoch   5/28 | Train: Loss=0.1442, F1=0.9482\n",
      "Final Training Epoch  10/28 | Train: Loss=0.0487, F1=0.9839\n",
      "Final Training Epoch  15/28 | Train: Loss=0.0405, F1=0.9867\n",
      "Final Training Epoch  20/28 | Train: Loss=0.0259, F1=0.9938\n",
      "Final Training Epoch  25/28 | Train: Loss=0.0104, F1=0.9987\n",
      "Final Training Epoch  28/28 | Train: Loss=0.0208, F1=0.9948\n",
      "Final training complete.\n",
      "\n",
      "--- Generating predictions on test set (windowed) ---\n",
      "Generated 6620 predictions (from 6620 windows).\n",
      "Aggregating window predictions to sample predictions...\n",
      "Aggregated to 1324 final predictions.\n",
      "Loading sample submission file for correct formatting...\n",
      "Prediction count matches. Creating submission.\n",
      "\n",
      "Successfully saved to submissions\\submission_GRU_H128_L3_BTrue_D0.3946_Attention_Window_w80_s20_Optuna_FINAL.csv!\n",
      "This file is correctly formatted for Kaggle:\n",
      "  sample_index      label\n",
      "0          000    no_pain\n",
      "1          001    no_pain\n",
      "2          002    no_pain\n",
      "3          003  high_pain\n",
      "4          004    no_pain\n"
     ]
    }
   ],
   "source": [
    "# --- 1. & 2. Create Full Training Set & Loader (with windows) ---\n",
    "print(\"\\n--- Preparing full dataset for final training ---\")\n",
    "\n",
    "# --- REVERTED: Only TS Scaler ---\n",
    "scaler_final_ts = StandardScaler()\n",
    "ns, ts, f = X_train_full_ts.shape\n",
    "X_train_full_ts_2d = X_train_full_ts.reshape(ns * ts, f)\n",
    "print(f\"Fitting FINAL TS Scaler on X_train_full_ts_2d shape: {X_train_full_ts_2d.shape}\")\n",
    "scaler_final_ts.fit(X_train_full_ts_2d)\n",
    "\n",
    "# Scale final TS Train\n",
    "X_train_full_ts_scaled_2d = scaler_final_ts.transform(X_train_full_ts_2d)\n",
    "X_train_full_ts_scaled = X_train_full_ts_scaled_2d.reshape(ns, ts, f)\n",
    "\n",
    "# Scale final TS Test\n",
    "ns_test, ts_test, f_test = X_test_ts.shape\n",
    "X_test_ts_2d = X_test_ts.reshape(ns_test * ts_test, f_test)\n",
    "X_test_ts_scaled_2d = scaler_final_ts.transform(X_test_ts_2d)\n",
    "X_test_ts_scaled = X_test_ts_scaled_2d.reshape(ns_test, ts_test, f_test)\n",
    "\n",
    "print(\"Final scaling complete.\")\n",
    "print(f\"--- Applying sliding windows (W={WINDOW_SIZE}, S={STRIDE}) to final dataset ---\")\n",
    "\n",
    "# --- REVERTED: Apply windowing to the final training set (TS only) ---\n",
    "(\n",
    "    X_train_full_windowed, \n",
    "    y_train_full_windowed, \n",
    "    _\n",
    ") = create_sliding_windows(\n",
    "    X_train_full_ts_scaled,\n",
    "    y_train_full,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    stride=STRIDE\n",
    ")\n",
    "\n",
    "# --- REVERTED: Apply windowing to the final test set (TS only) ---\n",
    "(\n",
    "    X_test_final_windowed, \n",
    "    _, \n",
    "    test_window_to_sample_idx # CRITICAL: We need this to map preds back\n",
    ") = create_sliding_windows(\n",
    "    X_test_ts_scaled,\n",
    "    y=np.zeros(len(X_test_ts_scaled)), # Dummy 'y'\n",
    "    window_size=WINDOW_SIZE,\n",
    "    stride=STRIDE\n",
    ")\n",
    "\n",
    "print(f\"Full train windowed TS shape: {X_train_full_windowed.shape}\")\n",
    "print(f\"Test windowed TS shape: {X_test_final_windowed.shape}\")\n",
    "print(f\"Test window-to-sample map shape: {test_window_to_sample_idx.shape}\")\n",
    "\n",
    "# --- Create Tensors and DataLoaders from WINDOWED data ---\n",
    "full_train_features_ts = torch.from_numpy(X_train_full_windowed).float()\n",
    "full_train_targets = torch.from_numpy(y_train_full_windowed).long()\n",
    "\n",
    "final_test_features_ts = torch.from_numpy(X_test_final_windowed).float()\n",
    "\n",
    "# 2-tensor dataset for training\n",
    "full_train_ds = TensorDataset(full_train_features_ts, full_train_targets)\n",
    "# 1-tensor dataset for test (no labels)\n",
    "final_test_ds = TensorDataset(final_test_features_ts)\n",
    "\n",
    "def make_final_loader(ds, batch_size, shuffle, drop_last):\n",
    "    return DataLoader(\n",
    "        ds, batch_size=int(batch_size), shuffle=shuffle, drop_last=drop_last,\n",
    "        num_workers=0, pin_memory=True, pin_memory_device=\"cuda\", prefetch_factor=None\n",
    "    )\n",
    "\n",
    "full_train_loader = make_final_loader(full_train_ds, batch_size=FINAL_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = make_final_loader(final_test_ds, batch_size=FINAL_BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "print(\"Final DataLoaders created.\")\n",
    "\n",
    "# --- 3. Instantiate Fresh Model --- \n",
    "print(f\"\\n--- Building FINAL model for submission: {FINAL_EXPERIMENT_NAME} ---\")\n",
    "final_model = RecurrentClassifier(\n",
    "    input_size=N_TS_FEATURES,\n",
    "    # --- REVERTED: No static_input_size ---\n",
    "    hidden_size=FINAL_HIDDEN_SIZE,\n",
    "    num_layers=FINAL_HIDDEN_LAYERS,\n",
    "    num_classes=N_CLASSES,\n",
    "    dropout_rate=FINAL_DROPOUT_RATE,\n",
    "    bidirectional=FINAL_BIDIRECTIONAL,\n",
    "    rnn_type=FINAL_MODEL_TYPE\n",
    ").to(device)\n",
    "\n",
    "if torch.__version__[0] >= \"2\":\n",
    "    print(\"Compiling final model...\")\n",
    "    final_model = torch.compile(final_model)\n",
    "\n",
    "final_optimizer = torch.optim.AdamW(final_model.parameters(), lr=FINAL_LEARNING_RATE, weight_decay=FINAL_L2_LAMBDA)\n",
    "final_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# --- 4. Train on Full Dataset --- \n",
    "print(f\"Training final model for {FINAL_BEST_EPOCH} epochs on ALL data...\")\n",
    "\n",
    "final_model.train() \n",
    "for epoch in range(1, FINAL_BEST_EPOCH + 1):\n",
    "    # --- REVERTED: Use 2-part-batch train function --- \n",
    "    train_loss, train_f1 = train_one_epoch(\n",
    "        final_model, full_train_loader, final_check_criterion, final_optimizer, final_scaler, device, 0, FINAL_L2_LAMBDA\n",
    "    )\n",
    "    if epoch % 5 == 0 or epoch == 1 or epoch == FINAL_BEST_EPOCH:\n",
    "        print(f\"Final Training Epoch {epoch:3d}/{FINAL_BEST_EPOCH} | Train: Loss={train_loss:.4f}, F1={train_f1:.4f}\")\n",
    "\n",
    "print(\"Final training complete.\")\n",
    "\n",
    "# --- 5. Generate Predictions (on windows) ---\n",
    "print(\"\\n--- Generating predictions on test set (windowed) ---\")\n",
    "final_model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # --- REVERTED: Unpack 1-part batch ---\n",
    "    for (ts_inputs,) in test_loader: \n",
    "        ts_inputs = ts_inputs.to(device)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            # --- REVERTED: Pass one input ---\n",
    "            logits = final_model(ts_inputs)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_predictions.append(preds.cpu().numpy())\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "print(f\"Generated {len(all_predictions)} predictions (from {len(test_window_to_sample_idx)} windows).\")\n",
    "\n",
    "\n",
    "# --- 6. Aggregate Predictions (Majority Vote) ---\n",
    "print(\"Aggregating window predictions to sample predictions...\")\n",
    "\n",
    "df_preds = pd.DataFrame({\n",
    "    'original_index': test_window_to_sample_idx,\n",
    "    'prediction': all_predictions\n",
    "})\n",
    "\n",
    "# Group by the original sample index (0 to 1323) and find the most common prediction\n",
    "# mode(x)[0] gets the most frequent value\n",
    "agg_preds = df_preds.groupby('original_index')['prediction'].apply(lambda x: mode(x)[0]).values\n",
    "\n",
    "print(f\"Aggregated to {len(agg_preds)} final predictions.\")\n",
    "\n",
    "# Inverse transform these aggregated predictions to labels\n",
    "predicted_labels = le.inverse_transform(agg_preds)\n",
    "\n",
    "# --- 7. Save Submission File ---\n",
    "print(\"Loading sample submission file for correct formatting...\")\n",
    "X_test_long = pd.read_csv(X_TEST_PATH)\n",
    "test_sample_indices = sorted(X_test_long['sample_index'].unique())\n",
    "\n",
    "if len(predicted_labels) != len(test_sample_indices):\n",
    "    print(f\"ERROR: Prediction count mismatch! Predictions: {len(predicted_labels)}, Test Indices: {len(test_sample_indices)}\")\n",
    "else:\n",
    "    print(\"Prediction count matches. Creating submission.\")\n",
    "    \n",
    "    final_submission_df = pd.DataFrame({\n",
    "        'sample_index': test_sample_indices,\n",
    "        'label': predicted_labels \n",
    "    })\n",
    "    \n",
    "    final_submission_df['sample_index'] = final_submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "\n",
    "    SUBMISSIONS_DIR = \"submissions\"\n",
    "    os.makedirs(SUBMISSIONS_DIR, exist_ok=True)\n",
    "    \n",
    "    submission_filename = f\"submission_{FINAL_EXPERIMENT_NAME}.csv\"\n",
    "    submission_filepath = os.path.join(SUBMISSIONS_DIR, submission_filename)\n",
    "    \n",
    "    final_submission_df.to_csv(submission_filepath, index=False)\n",
    "\n",
    "    print(f\"\\nSuccessfully saved to {submission_filepath}!\")\n",
    "    print(\"This file is correctly formatted for Kaggle:\")\n",
    "    print(final_submission_df.head())\n",
    "\n",
    "del final_model, full_train_loader, test_loader\n",
    "del full_train_features_ts, final_test_features_ts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
