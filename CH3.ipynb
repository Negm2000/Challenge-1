{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è (v2 - Advanced)**\n",
    "\n",
    "This notebook has been modified to include two new experimental techniques:\n",
    "\n",
    "1.  **Static Feature Integration:** We now load, one-hot encode, and feed the static features (`n_legs`, `n_hands`, `n_eyes`) into the model.\n",
    "2.  **Sliding Window Augmentation:** We now use a sliding window over the 160-timestep samples to create more (but shorter) training examples.\n",
    "\n",
    "**Local Setup:**\n",
    "1.  Ensure you have a Conda environment with PyTorch (GPU), `pandas`, `sklearn`, `jupyterlab`, `ray[tune]`, and `optuna`.\n",
    "2.  Place the Kaggle CSVs (`pirate_pain_train.csv`, `pirate_pain_train_labels.csv`, `pirate_pain_test.csv`) in a folder named `data/` in the same directory as this notebook.\n",
    "3.  To run TensorBoard, open a separate terminal, `conda activate` your environment, `cd` to this folder, and run: `tensorboard --logdir=./tensorboard`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è **1. Setup & Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GPU (RTX 3070, here we come!) ---\n",
      "PyTorch version: 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "import time\n",
    "from scipy.stats import mode # --- NEW --- for aggregation\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# --- PyTorch Imports ---\\n\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Sklearn Imports ---\\n\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder # --- MODIFIED ---\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Ray[tune] & Optuna Imports ---\\n\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from functools import partial\n",
    "\n",
    "# --- Setup Directories & Device ---\\n\n",
    "logs_dir = \"tensorboard\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU (RTX 3070, here we come!) ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "# %matplotlib inline # Uncomment if running in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ **2. Data Loading & Reshaping (MODIFIED)**\n",
    " \n",
    "This is now a two-part process:\n",
    "1.  Define all features (time-series and static).\n",
    "2.  Create a function to pivot the time-series data into a 3D tensor: `(num_samples, num_timesteps, num_features)`.\n",
    "3.  Create a function to load and one-hot encode the static data into a 2D tensor: `(num_samples, num_static_features)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 35 time-series features: ['joint_00', 'joint_01', 'joint_02']... to ['pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Using 3 static features: ['n_legs', 'n_hands', 'n_eyes']\n",
      "Loading and reshaping time-series training data...\n",
      "Loading and reshaping time-series test data...\n",
      "Fitting new OneHotEncoder for static features...\n",
      "Transforming static features with OneHotEncoder...\n",
      "Transforming static features with OneHotEncoder...\n",
      "\n",
      "X_train_full_ts shape: (661, 160, 35)\n",
      "X_train_full_static shape: (661, 6)\n",
      "y_train_labels_str shape: (661,)\n",
      "X_test_ts shape: (1324, 160, 35)\n",
      "X_test_static shape: (1324, 6)\n",
      "Total static features after OHE: 6\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define File Paths and Features ---\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "# Define our features\n",
    "JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(31)]\n",
    "PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "STATIC_FEATURES = [\"n_legs\", \"n_hands\", \"n_eyes\"] # --- NEW ---\n",
    "\n",
    "TS_FEATURES = JOINT_FEATURES + PAIN_FEATURES # Time-Series Features\n",
    "N_TS_FEATURES = len(TS_FEATURES)\n",
    "N_TIMESTEPS = 160 # Fixed from our earlier debugging\n",
    "\n",
    "print(f\"Using {N_TS_FEATURES} time-series features: {TS_FEATURES[:3]}... to {TS_FEATURES[-3:]}\")\n",
    "print(f\"Using {len(STATIC_FEATURES)} static features: {STATIC_FEATURES}\")\n",
    "\n",
    "# --- 2. Create the Reshaping Functions ---\n",
    "\n",
    "def reshape_timeseries_data(df, features_list, n_timesteps):\n",
    "    \"\"\"\n",
    "    Pivots the long-format dataframe into a 3D NumPy array.\n",
    "    Shape: (n_samples, n_timesteps, n_features)\n",
    "    \"\"\"\n",
    "    df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "    data_2d = df_pivot.values\n",
    "    n_samples = data_2d.shape[0]\n",
    "    data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "    return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "def load_and_encode_static_data(csv_path, static_features, ohe_encoder=None, fit_encoder=False):\n",
    "    \"\"\"\n",
    "    Loads static features, takes the first row for each sample,\n",
    "    and One-Hot Encodes them.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Get just one row per sample_index\n",
    "    df_static = df.drop_duplicates(subset='sample_index', keep='first').set_index('sample_index')\n",
    "    df_static = df_static[static_features]\n",
    "    \n",
    "    # Handle string-based categorical data\n",
    "    df_static = df_static.astype(str)\n",
    "\n",
    "    if fit_encoder:\n",
    "        print(\"Fitting new OneHotEncoder for static features...\")\n",
    "        ohe_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        ohe_encoder.fit(df_static)\n",
    "    \n",
    "    print(\"Transforming static features with OneHotEncoder...\")\n",
    "    static_data_encoded = ohe_encoder.transform(df_static)\n",
    "    \n",
    "    if fit_encoder:\n",
    "        return static_data_encoded, ohe_encoder\n",
    "    else:\n",
    "        return static_data_encoded\n",
    "\n",
    "# --- 3. Load and Reshape Data ---\n",
    "print(\"Loading and reshaping time-series training data...\")\n",
    "X_train_long = pd.read_csv(X_TRAIN_PATH)\n",
    "X_train_full_ts = reshape_timeseries_data(X_train_long, TS_FEATURES, N_TIMESTEPS)\n",
    "\n",
    "print(\"Loading and reshaping time-series test data...\")\n",
    "X_test_long = pd.read_csv(X_TEST_PATH)\n",
    "X_test_ts = reshape_timeseries_data(X_test_long, TS_FEATURES, N_TIMESTEPS)\n",
    "\n",
    "# --- NEW: Load and Encode Static Data ---\n",
    "# Fit the encoder on the training data\n",
    "X_train_full_static, static_ohe_encoder = load_and_encode_static_data(\n",
    "    X_TRAIN_PATH, \n",
    "    STATIC_FEATURES, \n",
    "    fit_encoder=True\n",
    ")\n",
    "\n",
    "# Use the *same* encoder to transform the test data\n",
    "X_test_static = load_and_encode_static_data(\n",
    "    X_TEST_PATH, \n",
    "    STATIC_FEATURES, \n",
    "    ohe_encoder=static_ohe_encoder, \n",
    "    fit_encoder=False\n",
    ")\n",
    "\n",
    "N_STATIC_FEATURES = X_train_full_static.shape[1]\n",
    "\n",
    "# Load labels\n",
    "y_train_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "y_train_full_df = y_train_df.sort_values(by='sample_index')\n",
    "y_train_labels_str = y_train_full_df['label'].values\n",
    "\n",
    "print(f\"\\nX_train_full_ts shape: {X_train_full_ts.shape}\")\n",
    "print(f\"X_train_full_static shape: {X_train_full_static.shape}\")\n",
    "print(f\"y_train_labels_str shape: {y_train_labels_str.shape}\")\n",
    "print(f\"X_test_ts shape: {X_test_ts.shape}\")\n",
    "print(f\"X_test_static shape: {X_test_static.shape}\")\n",
    "print(f\"Total static features after OHE: {N_STATIC_FEATURES}\")\n",
    "\n",
    "del X_train_long, X_test_long, y_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöß **3. Preprocessing: Window, Split & Scale (MODIFIED)**\n",
    " \n",
    "1.  **Encode Labels:** Convert `no_pain`, `low_pain`, `high_pain` to `0`, `1`, `2`.\n",
    "2.  **Create Sliding Windows:** Use a new function to \"chop\" the 160-timestep data into smaller, overlapping windows. This augments our data.\n",
    "3.  **Split Data:** Use `StratifiedShuffleSplit` on the new *windowed* data.\n",
    "4.  **Scale Features:** Use *two* `StandardScaler`s: one for time-series, one for static. Fit *only* on the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels encoded. 3 classes: {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
      "\n",
      "Creating sliding windows (W=100, S=20)...\n",
      "Data augmented with sliding windows:\n",
      "  Original TS shape: (661, 160, 35)\n",
      "  Windowed TS shape: (2644, 100, 35)\n",
      "  Windowed Static shape: (2644, 6)\n",
      "  Windowed y shape: (2644,)\n",
      "\n",
      "Data split into Train and Validation sets:\n",
      "  X_ts_train_split:       (2115, 100, 35)\n",
      "  X_static_train_split: (2115, 6)\n",
      "  y_train_split:          (2115,)\n",
      "  X_ts_val_split:         (529, 100, 35)\n",
      "  X_static_val_split:   (529, 6)\n",
      "  y_val_split:          (529,)\n",
      "\n",
      "Fitting Time-Series Scaler on X_ts_train_2d shape: (211500, 35)\n",
      "Fitting Static Scaler on X_static_train_split shape: (2115, 6)\n",
      "\n",
      "Scaling complete.\n",
      "  X_ts_train_scaled:       (2115, 100, 35)\n",
      "  X_static_train_scaled: (2115, 6)\n",
      "  X_ts_val_scaled:         (529, 100, 35)\n",
      "  X_static_val_scaled:   (529, 6)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Encode Labels ---\n",
    "LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "le = LabelEncoder()\n",
    "le.fit(list(LABEL_MAPPING.keys()))\n",
    "y_train_full = le.transform(y_train_labels_str)\n",
    "N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "print(f\"Labels encoded. {N_CLASSES} classes: {LABEL_MAPPING}\")\n",
    "\n",
    "# --- 2. Create Sliding Windows ---\n",
    "# --- NEW --- Define Window Parameters ---\n",
    "WINDOW_SIZE = 100 # Experiment with this (e.g., 80, 100, 120)\n",
    "STRIDE = 20       # Experiment with this (e.g., 10, 20)\n",
    "# ---\n",
    "\n",
    "def create_sliding_windows(X_3d_ts, X_2d_static, y, window_size, stride):\n",
    "    \"\"\"\n",
    "    Takes 3D time-series, 2D static data, and 1D labels\n",
    "    and creates overlapping windows.\n",
    "    Returns:\n",
    "    - new_X_ts (4D): (n_windows, window_size, n_ts_features)\n",
    "    - new_X_static (2D): (n_windows, n_static_features)\n",
    "    - new_y (1D): (n_windows,)\n",
    "    - window_to_sample_idx (1D): (n_windows,) mapping to original sample\n",
    "    \"\"\"\n",
    "    new_X_ts = []\n",
    "    new_X_static = []\n",
    "    new_y = []\n",
    "    window_to_sample_idx = []\n",
    "    \n",
    "    n_samples, n_timesteps, n_features = X_3d_ts.shape\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        sample_ts = X_3d_ts[i]\n",
    "        sample_static = X_2d_static[i]\n",
    "        label = y[i]\n",
    "        \n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            window = sample_ts[idx : idx + window_size]\n",
    "            new_X_ts.append(window)\n",
    "            new_X_static.append(sample_static) # Static features are repeated\n",
    "            new_y.append(label)\n",
    "            window_to_sample_idx.append(i) # Track original sample\n",
    "            idx += stride\n",
    "            \n",
    "    return (\n",
    "        np.array(new_X_ts), \n",
    "        np.array(new_X_static), \n",
    "        np.array(new_y), \n",
    "        np.array(window_to_sample_idx)\n",
    "    )\n",
    "\n",
    "print(f\"\\nCreating sliding windows (W={WINDOW_SIZE}, S={STRIDE})...\")\n",
    "(\n",
    "    X_ts_windowed, \n",
    "    X_static_windowed, \n",
    "    y_windowed, \n",
    "    _ # We don't need the index map for training\n",
    ") = create_sliding_windows(\n",
    "    X_train_full_ts, \n",
    "    X_train_full_static, \n",
    "    y_train_full, \n",
    "    WINDOW_SIZE, \n",
    "    STRIDE\n",
    ")\n",
    "\n",
    "# Our new sequence length is the window size\n",
    "N_TIMESTEPS_WINDOWED = WINDOW_SIZE\n",
    "\n",
    "print(f\"Data augmented with sliding windows:\")\n",
    "print(f\"  Original TS shape: {X_train_full_ts.shape}\")\n",
    "print(f\"  Windowed TS shape: {X_ts_windowed.shape}\")\n",
    "print(f\"  Windowed Static shape: {X_static_windowed.shape}\")\n",
    "print(f\"  Windowed y shape: {y_windowed.shape}\")\n",
    "\n",
    "\n",
    "# --- 3. Create Validation Split (on windowed data) ---\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# We split the new, larger windowed arrays\n",
    "for train_idx, val_idx in sss.split(X_ts_windowed, y_windowed):\n",
    "    X_ts_train_split = X_ts_windowed[train_idx]\n",
    "    X_static_train_split = X_static_windowed[train_idx]\n",
    "    y_train_split = y_windowed[train_idx]\n",
    "    \n",
    "    X_ts_val_split = X_ts_windowed[val_idx]\n",
    "    X_static_val_split = X_static_windowed[val_idx]\n",
    "    y_val_split = y_windowed[val_idx]\n",
    "\n",
    "print(f\"\\nData split into Train and Validation sets:\")\n",
    "print(f\"  X_ts_train_split:       {X_ts_train_split.shape}\")\n",
    "print(f\"  X_static_train_split: {X_static_train_split.shape}\")\n",
    "print(f\"  y_train_split:          {y_train_split.shape}\")\n",
    "print(f\"  X_ts_val_split:         {X_ts_val_split.shape}\")\n",
    "print(f\"  X_static_val_split:   {X_static_val_split.shape}\")\n",
    "print(f\"  y_val_split:          {y_val_split.shape}\")\n",
    "\n",
    "# --- 4. Scale Features (The \"No-Cheating\" Rule) ---\n",
    "\n",
    "# --- Scaler 1: Time-Series ---\n",
    "scaler_ts = StandardScaler()\n",
    "ns, ts, f = X_ts_train_split.shape\n",
    "X_ts_train_2d = X_ts_train_split.reshape(ns * ts, f)\n",
    "print(f\"\\nFitting Time-Series Scaler on X_ts_train_2d shape: {X_ts_train_2d.shape}\")\n",
    "scaler_ts.fit(X_ts_train_2d)\n",
    "\n",
    "# Transform TS Train\n",
    "X_ts_train_scaled_2d = scaler_ts.transform(X_ts_train_2d)\n",
    "X_ts_train_scaled = X_ts_train_scaled_2d.reshape(ns, ts, f)\n",
    "\n",
    "# Transform TS Val\n",
    "ns_val, ts_val, f_val = X_ts_val_split.shape\n",
    "X_ts_val_2d = X_ts_val_split.reshape(ns_val * ts_val, f_val)\n",
    "X_ts_val_scaled_2d = scaler_ts.transform(X_ts_val_2d)\n",
    "X_ts_val_scaled = X_ts_val_scaled_2d.reshape(ns_val, ts_val, f_val)\n",
    "\n",
    "# --- Scaler 2: Static ---\n",
    "# Note: Scaling OHE features is fine, it just centers them around 0\n",
    "scaler_static = StandardScaler()\n",
    "print(f\"Fitting Static Scaler on X_static_train_split shape: {X_static_train_split.shape}\")\n",
    "scaler_static.fit(X_static_train_split)\n",
    "\n",
    "# Transform Static Train\n",
    "X_static_train_scaled = scaler_static.transform(X_static_train_split)\n",
    "# Transform Static Val\n",
    "X_static_val_scaled = scaler_static.transform(X_static_val_split)\n",
    "\n",
    "print(\"\\nScaling complete.\")\n",
    "print(f\"  X_ts_train_scaled:       {X_ts_train_scaled.shape}\")\n",
    "print(f\"  X_static_train_scaled: {X_static_train_scaled.shape}\")\n",
    "print(f\"  X_ts_val_scaled:         {X_ts_val_scaled.shape}\")\n",
    "print(f\"  X_static_val_scaled:   {X_static_val_scaled.shape}\")\n",
    "\n",
    "del X_ts_train_2d, X_ts_val_2d, X_ts_train_scaled_2d, X_ts_val_scaled_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöö **4. PyTorch DataLoaders (MODIFIED)**\n",
    " \n",
    "We now create a `TensorDataset` that holds **three** items:\n",
    "1.  Time-series features\n",
    "2.  Static features\n",
    "3.  Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDatasets created.\n",
      "Example train_ds[0] shapes:\n",
      "  TS features:  torch.Size([100, 35])\n",
      "  Static features: torch.Size([6])\n",
      "  Target:         torch.Size([])\n",
      "\n",
      "DataLoaders will be created inside the tuning loop.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Convert to Tensors ---\n",
    "# Train\n",
    "train_ts_features = torch.from_numpy(X_ts_train_scaled).float()\n",
    "train_static_features = torch.from_numpy(X_static_train_scaled).float()\n",
    "train_targets = torch.from_numpy(y_train_split).long()\n",
    "\n",
    "# Validation\n",
    "val_ts_features = torch.from_numpy(X_ts_val_scaled).float()\n",
    "val_static_features = torch.from_numpy(X_static_val_scaled).float()\n",
    "val_targets = torch.from_numpy(y_val_split).long()\n",
    "\n",
    "# Test (we'll process this in the submission cell)\n",
    "\n",
    "# --- 2. Create TensorDatasets ---\n",
    "train_ds = TensorDataset(train_ts_features, train_static_features, train_targets)\n",
    "val_ds = TensorDataset(val_ts_features, val_static_features, val_targets)\n",
    "# test_ds will be created in the final submission cell\n",
    "\n",
    "print(f\"TensorDatasets created.\")\n",
    "print(f\"Example train_ds[0] shapes:\")\n",
    "print(f\"  TS features:  {train_ds[0][0].shape}\")\n",
    "print(f\"  Static features: {train_ds[0][1].shape}\")\n",
    "print(f\"  Target:         {train_ds[0][2].shape}\")\n",
    "\n",
    "# --- 3. Define make_loader function (from Lecture 4) ---\n",
    "BATCH_SIZE = 128 # This will be our default, but Optuna can tune it\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    num_workers = 0 \n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=int(batch_size),\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=None,\n",
    "    )\n",
    "\n",
    "# --- 4. Create DataLoaders ---\n",
    "# We will create these *inside* the objective function now,\n",
    "# as the batch size is a hyperparameter we want to tune.\n",
    "print(\"\\nDataLoaders will be created inside the tuning loop.\")\n",
    "del X_ts_train_scaled, X_static_train_scaled, val_ts_features, val_static_features\n",
    "del train_ts_features, train_static_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **5. Model & Training Engine (MODIFIED)**\n",
    " \n",
    "-   `RecurrentClassifier`: **Modified** to accept `static_input_size` and a second input `x_static`. It concatenates the RNN output with the static features before classifying.\n",
    "-   `train_one_epoch` / `validate_one_epoch`: **Modified** to handle 3-part batches `(ts_inputs, static_inputs, targets)`.\n",
    "-   `objective_function`: **Modified** to pass `static_input_size` to the model and handle the new batch structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_summary(model, ts_input_size, static_input_size):\n",
    "    \"\"\"\n",
    "    Custom summary function (MODIFIED) for 2-input model.\n",
    "    \"\"\"\n",
    "    output_shapes = {}\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1\n",
    "                if isinstance(output[1], tuple):\n",
    "                    shape2 = list(output[1][0].shape)\n",
    "                else:\n",
    "                    shape2 = list(output[1].shape)\n",
    "                shape2[1] = -1\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "        return hook\n",
    "\n",
    "    try:\n",
    "        device_summary = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device_summary = torch.device(\"cpu\")\n",
    "\n",
    "    # --- MODIFIED: Create two dummy inputs ---\n",
    "    dummy_input_ts = torch.randn(1, *ts_input_size).to(device_summary)\n",
    "    dummy_input_static = torch.randn(1, static_input_size).to(device_summary)\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # --- MODIFIED: Pass both inputs ---\n",
    "            model(dummy_input_ts, dummy_input_static)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODIFIED --- RecurrentClassifier ---\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic RNN classifier (RNN, LSTM, GRU) from Lecture 4.\n",
    "    MODIFIED to accept static features.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,         # N_TS_FEATURES\n",
    "            static_input_size,  # N_STATIC_FEATURES\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "        \n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        if self.bidirectional:\n",
    "            rnn_output_size = hidden_size * 2\n",
    "        else:\n",
    "            rnn_output_size = hidden_size\n",
    "        \n",
    "        # --- NEW: Classifier input is RNN output + static features ---\n",
    "        classifier_input_size = rnn_output_size + static_input_size\n",
    "\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x_ts, x_static): # --- MODIFIED: two inputs ---\n",
    "        \"\"\"\n",
    "        x_ts shape: (batch_size, seq_length, input_size)\n",
    "        x_static shape: (batch_size, static_input_size)\n",
    "        \"\"\"\n",
    "        rnn_out, hidden = self.rnn(x_ts)\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0]\n",
    "\n",
    "        if self.bidirectional:\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
    "        else:\n",
    "            hidden_to_classify = hidden[-1]\n",
    "        \n",
    "        # --- NEW: Concatenate RNN output with static features ---\n",
    "        combined_features = torch.cat([hidden_to_classify, x_static], dim=1)\n",
    "\n",
    "        logits = self.classifier(combined_features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODIFIED --- Training & Validation Loops ---\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # --- MODIFIED: Unpack 3-part batch ---\n",
    "    for batch_idx, (ts_inputs, static_inputs, targets) in enumerate(train_loader):\n",
    "        # --- MODIFIED: Move all parts to device ---\n",
    "        ts_inputs, static_inputs, targets = ts_inputs.to(device), static_inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            # --- MODIFIED: Pass both inputs to model ---\n",
    "            logits = model(ts_inputs, static_inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            \n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * ts_inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # --- MODIFIED: Unpack 3-part batch ---\n",
    "        for (ts_inputs, static_inputs, targets) in val_loader:\n",
    "            # --- MODIFIED: Move all parts to device ---\n",
    "            ts_inputs, static_inputs, targets = ts_inputs.to(device), static_inputs.to(device), targets.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                # --- MODIFIED: Pass both inputs to model ---\n",
    "                logits = model(ts_inputs, static_inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item() * ts_inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
    "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "\n",
    "# --- MODIFIED --- Objective Function ---\n",
    "def objective_function(config, train_ds, val_ds):\n",
    "    \"\"\"\n",
    "    This is the main function that Ray Tune will call for each trial.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Create DataLoaders with the tuned batch size ---\n",
    "    train_loader = make_loader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "    val_loader = make_loader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=False)\n",
    "    \n",
    "    # --- 2. Create Model --- \n",
    "    model = RecurrentClassifier(\n",
    "        input_size=N_TS_FEATURES,          # Time-series features\n",
    "        static_input_size=N_STATIC_FEATURES, # --- NEW --- Static features\n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        num_classes=N_CLASSES,\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        bidirectional=config[\"bidirectional\"],\n",
    "        rnn_type=config[\"rnn_type\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    if torch.__version__[0] >= \"2\":\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    # --- 3. Create Optimizer, Loss, Scaler ---\\n\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "    # --- 4. The Training Loop ---\n",
    "    EPOCHS = 200 \n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # --- MODIFIED: train/validate functions are new versions ---\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, 0, config[\"l2_lambda\"]\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # --- Send Results to Ray Tune --- \n",
    "        tune.report({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_f1\": val_f1\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ **6. Hyperparameter Search with Ray Tune & Optuna**\n",
    " \n",
    "This cell is unchanged. It will now automatically use the new model and data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-08 14:16:34</td></tr>\n",
       "<tr><td>Running for: </td><td>00:14:14.31        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.1/13.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=20<br>Bracket: Iter 80.000: 0.9875665288212603 | Iter 40.000: 0.9808993453810615 | Iter 20.000: 0.9585723807043616<br>Logical resource usage: 4.0/16 CPUs, 0.25/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th>bidirectional  </th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  l2_lambda</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th>rnn_type  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_f1</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_function_3c8f5a6b</td><td>TERMINATED</td><td>127.0.0.1:39648</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.449863</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.55699e-06</td><td style=\"text-align: right;\">0.0071723  </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        208.026 </td><td style=\"text-align: right;\">  0.105711  </td><td style=\"text-align: right;\">  0.998047</td><td style=\"text-align: right;\">0.448443  </td></tr>\n",
       "<tr><td>objective_function_32ae99ef</td><td>TERMINATED</td><td>127.0.0.1:43288</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.317327</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">2.82998e-05</td><td style=\"text-align: right;\">1.4419e-05 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         15.5797</td><td style=\"text-align: right;\">  0.717849  </td><td style=\"text-align: right;\">  0.706303</td><td style=\"text-align: right;\">0.662027  </td></tr>\n",
       "<tr><td>objective_function_814b89bf</td><td>TERMINATED</td><td>127.0.0.1:49432</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.354524</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">9.29181e-06</td><td style=\"text-align: right;\">0.00338544 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        332.602 </td><td style=\"text-align: right;\">  0.101406  </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">0.105049  </td></tr>\n",
       "<tr><td>objective_function_2726046a</td><td>TERMINATED</td><td>127.0.0.1:24964</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.300067</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">4.84556e-06</td><td style=\"text-align: right;\">0.00434132 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        151.939 </td><td style=\"text-align: right;\">  0.0309734 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">0.0449008 </td></tr>\n",
       "<tr><td>objective_function_be825108</td><td>TERMINATED</td><td>127.0.0.1:20568</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.372461</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">4.60463e-06</td><td style=\"text-align: right;\">0.00165158 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        314.232 </td><td style=\"text-align: right;\">  0.0137545 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">0.0696831 </td></tr>\n",
       "<tr><td>objective_function_86beef32</td><td>TERMINATED</td><td>127.0.0.1:45312</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.260787</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">1.05134e-07</td><td style=\"text-align: right;\">0.00137486 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        542.429 </td><td style=\"text-align: right;\">  0.00125076</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">0.0767007 </td></tr>\n",
       "<tr><td>objective_function_e7047a9b</td><td>TERMINATED</td><td>127.0.0.1:37228</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.20574 </td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.16829e-06</td><td style=\"text-align: right;\">0.00352773 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        298.956 </td><td style=\"text-align: right;\">  0.00886131</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">0.0354344 </td></tr>\n",
       "<tr><td>objective_function_e8a19773</td><td>TERMINATED</td><td>127.0.0.1:44724</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.125128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.03541e-07</td><td style=\"text-align: right;\">6.88449e-05</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         26.1692</td><td style=\"text-align: right;\">  0.397567  </td><td style=\"text-align: right;\">  0.81841 </td><td style=\"text-align: right;\">0.419326  </td></tr>\n",
       "<tr><td>objective_function_bc2a2d81</td><td>TERMINATED</td><td>127.0.0.1:35040</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.505653</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">9.09173e-06</td><td style=\"text-align: right;\">0.00151572 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         73.1486</td><td style=\"text-align: right;\">  0.00556539</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">0.0211121 </td></tr>\n",
       "<tr><td>objective_function_27337408</td><td>TERMINATED</td><td>127.0.0.1:34416</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.594296</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.34443e-06</td><td style=\"text-align: right;\">0.0015202  </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        357.348 </td><td style=\"text-align: right;\">  0.00488792</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">0.0534718 </td></tr>\n",
       "<tr><td>objective_function_6a5c0544</td><td>TERMINATED</td><td>127.0.0.1:10860</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.526442</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.03369e-07</td><td style=\"text-align: right;\">0.000390585</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         64.6863</td><td style=\"text-align: right;\">  0.0483621 </td><td style=\"text-align: right;\">  0.985753</td><td style=\"text-align: right;\">0.129637  </td></tr>\n",
       "<tr><td>objective_function_1c246844</td><td>TERMINATED</td><td>127.0.0.1:22864</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.280027</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.61199e-07</td><td style=\"text-align: right;\">0.000170446</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         27.8213</td><td style=\"text-align: right;\">  0.150484  </td><td style=\"text-align: right;\">  0.945142</td><td style=\"text-align: right;\">0.156273  </td></tr>\n",
       "<tr><td>objective_function_49088158</td><td>TERMINATED</td><td>127.0.0.1:3616 </td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.156487</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.000682461</td><td style=\"text-align: right;\">0.000785418</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         23.2029</td><td style=\"text-align: right;\">  0.246366  </td><td style=\"text-align: right;\">  0.941801</td><td style=\"text-align: right;\">0.228973  </td></tr>\n",
       "<tr><td>objective_function_5327cf8b</td><td>TERMINATED</td><td>127.0.0.1:14620</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.342604</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.539e-06  </td><td style=\"text-align: right;\">0.00353815 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         32.1306</td><td style=\"text-align: right;\">  0.175464  </td><td style=\"text-align: right;\">  0.939712</td><td style=\"text-align: right;\">0.224288  </td></tr>\n",
       "<tr><td>objective_function_b27c2628</td><td>TERMINATED</td><td>127.0.0.1:49148</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.578796</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">8.90789e-05</td><td style=\"text-align: right;\">0.000882527</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         83.0415</td><td style=\"text-align: right;\">  0.0417922 </td><td style=\"text-align: right;\">  0.995583</td><td style=\"text-align: right;\">0.0327949 </td></tr>\n",
       "<tr><td>objective_function_c8ff9c0f</td><td>TERMINATED</td><td>127.0.0.1:26776</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.575028</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">8.44875e-05</td><td style=\"text-align: right;\">0.0012865  </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         40.6084</td><td style=\"text-align: right;\">  0.0750887 </td><td style=\"text-align: right;\">  0.985266</td><td style=\"text-align: right;\">0.0753483 </td></tr>\n",
       "<tr><td>objective_function_bef0d086</td><td>TERMINATED</td><td>127.0.0.1:40872</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.578344</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">9.96326e-05</td><td style=\"text-align: right;\">0.00114597 </td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         97.5507</td><td style=\"text-align: right;\">  0.0356876 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">0.00626702</td></tr>\n",
       "<tr><td>objective_function_9f2ba93d</td><td>TERMINATED</td><td>127.0.0.1:36928</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.181526</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">8.53347e-07</td><td style=\"text-align: right;\">0.000219303</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         42.4654</td><td style=\"text-align: right;\">  0.118156  </td><td style=\"text-align: right;\">  0.965665</td><td style=\"text-align: right;\">0.14929   </td></tr>\n",
       "<tr><td>objective_function_56901d7a</td><td>TERMINATED</td><td>127.0.0.1:45376</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.197059</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">8.96311e-07</td><td style=\"text-align: right;\">0.00881107 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         57.7276</td><td style=\"text-align: right;\">  0.110765  </td><td style=\"text-align: right;\">  0.974398</td><td style=\"text-align: right;\">0.135299  </td></tr>\n",
       "<tr><td>objective_function_aca5e2e3</td><td>TERMINATED</td><td>127.0.0.1:48072</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.202966</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">5.93818e-07</td><td style=\"text-align: right;\">0.000266708</td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         21.603 </td><td style=\"text-align: right;\">  0.14019   </td><td style=\"text-align: right;\">  0.952832</td><td style=\"text-align: right;\">0.13728   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-08 14:02:39,750 E 12724 2408] (gcs_server.exe) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-08 14:02:43,615 E 45472 22436] (raylet.exe) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "2025-11-08 14:16:34,733\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'c:/Users/Karim Negm/Documents/AN2DL/Challenge 1/ray_results/pirate_pain_optuna_search_v2' in 0.0428s.\n",
      "2025-11-08 14:16:34,755\tINFO tune.py:1041 -- Total run time: 854.36 seconds (854.26 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Search Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define the Search Space for Optuna ---\\n\n",
    "search_space = {\n",
    "    \"rnn_type\": tune.choice(['GRU', 'LSTM']),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"batch_size\": tune.choice([64, 128]),  \n",
    "    \"hidden_size\": tune.choice([64, 128, 256, 384]),\n",
    "    \"num_layers\": tune.choice([2, 3]),\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.6),\n",
    "    \"bidirectional\": tune.choice([True, False]),\n",
    "    \"l2_lambda\": tune.loguniform(1e-7, 1e-3)\n",
    "}\n",
    "\n",
    "# --- 2. Define the Optimizer (Optuna) and Scheduler (ASHA) ---\n",
    "optuna_search = OptunaSearch(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\",\n",
    "    grace_period=20,  # Min epochs a trial must run\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "# --- 3. Initialize Ray ---\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "ray_logs_path = os.path.abspath(\"./ray_results\")\n",
    "os.makedirs(ray_logs_path, exist_ok=True)\n",
    "os.environ[\"RAY_TEMP_DIR\"] = ray_logs_path\n",
    "os.environ[\"RAY_RAYLET_START_WAIT_TIME_S\"] = \"120\"\n",
    "ray.init(\n",
    "    num_cpus=16, \n",
    "    num_gpus=1, \n",
    "    ignore_reinit_error=True\n",
    ")\n",
    "\n",
    "def short_trial_name(trial):\n",
    "    return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "\n",
    "# --- 4. Run the Tuner ---\n",
    "print(\"Starting hyperparameter search (1 trial at a time)...\")\n",
    "\n",
    "# --- MODIFIED: Pass the new 3-tensor datasets ---\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(objective_function, train_ds=train_ds, val_ds=val_ds),\n",
    "    \n",
    "    resources_per_trial={\n",
    "        \"cpu\": 4, \n",
    "        \"gpu\": 0.25\n",
    "    },\n",
    "    \n",
    "    config=search_space,\n",
    "    num_samples=20, # Number of different HPO trials to run\n",
    "    search_alg=optuna_search,\n",
    "    scheduler=scheduler,\n",
    "    name=\"pirate_pain_optuna_search_v2\",\n",
    "\n",
    "    storage_path=ray_logs_path,\n",
    "    trial_dirname_creator=short_trial_name,\n",
    "    log_to_file=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n--- Search Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting best trial from analysis...\n",
      "Best validation F1 score: 0.9981\n",
      "Best hyperparameters found:\n",
      "{'rnn_type': 'GRU', 'lr': 0.0011459748962260494, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 3, 'dropout_rate': 0.5783440469483345, 'bidirectional': True, 'l2_lambda': 9.963258042875386e-05}\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Get Best Results ---\n",
    "print(\"Getting best trial from analysis...\")\n",
    "best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "if best_trial:\n",
    "    best_config = best_trial.config\n",
    "    best_val_f1 = best_trial.last_result[\"val_f1\"]\n",
    "    \n",
    "    print(f\"Best validation F1 score: {best_val_f1:.4f}\")\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(best_config)\n",
    "else:\n",
    "    print(\"ERROR: No trials completed successfully. Check the 'ray_results' folder for logs.\")\n",
    "    # --- FAKE CONFIG FOR TESTING ---\n",
    "    # best_config = search_space = {\n",
    "    #     \"rnn_type\": 'GRU', \"lr\": 1e-3, \"batch_size\": 128, \"hidden_size\": 128,\n",
    "    #     \"num_layers\": 2, \"dropout_rate\": 0.2, \"bidirectional\": False, \"l2_lambda\": 1e-5\n",
    "    # }\n",
    "    # best_val_f1 = 0.8 # Dummy value\n",
    "    # print(\"USING FAKE CONFIG FOR DEMONSTRATION\")\n",
    "    # --- END FAKE CONFIG ---\n",
    "    best_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ **7. Final Model Configuration**\n",
    " \n",
    "This cell is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèÜ Final Configuration Set --- \n",
      "Best Val F1 from search: 0.9981\n",
      "{'rnn_type': 'GRU', 'lr': 0.0011459748962260494, 'batch_size': 128, 'hidden_size': 64, 'num_layers': 3, 'dropout_rate': 0.5783440469483345, 'bidirectional': True, 'l2_lambda': 9.963258042875386e-05}\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# --- üèÜ FINAL MODEL CONFIGURATION üèÜ ---\n",
    "# ===================================================================\n",
    "# --- 1. Get Best Config from Analysis --- \n",
    "FINAL_CONFIG = best_config\n",
    "FINAL_BEST_VAL_F1 = best_val_f1\n",
    "\n",
    "print(\"--- üèÜ Final Configuration Set --- \")\n",
    "print(f\"Best Val F1 from search: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "print(FINAL_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Re-run to find Best Epoch (MODIFIED)**\n",
    " \n",
    "The `fit` function is now the **MODIFIED** version that handles the new data and model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finding best epoch number for the winning model ---\n",
      "\n",
      "--- Starting Training: final_check_v2 ---\n",
      "Will train for 200 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/200 | Train: Loss=0.7254, F1=0.7007 | Val: Loss=0.5959, F1=0.7319\n",
      "Epoch  10/200 | Train: Loss=0.1256, F1=0.9553 | Val: Loss=0.2008, F1=0.9441\n",
      "Epoch  20/200 | Train: Loss=0.0430, F1=0.9843 | Val: Loss=0.0872, F1=0.9697\n",
      "Epoch  30/200 | Train: Loss=0.0105, F1=0.9971 | Val: Loss=0.0352, F1=0.9848\n",
      "Epoch  40/200 | Train: Loss=0.0243, F1=0.9893 | Val: Loss=0.0558, F1=0.9848\n",
      "Epoch  50/200 | Train: Loss=0.0066, F1=0.9976 | Val: Loss=0.0245, F1=0.9904\n",
      "Epoch  60/200 | Train: Loss=0.0076, F1=0.9966 | Val: Loss=0.0307, F1=0.9924\n",
      "Epoch  70/200 | Train: Loss=0.0004, F1=1.0000 | Val: Loss=0.0220, F1=0.9924\n",
      "Epoch  80/200 | Train: Loss=0.0003, F1=1.0000 | Val: Loss=0.0284, F1=0.9943\n",
      "\n",
      "Early stopping triggered after 84 epochs.\n",
      "Restoring best model from epoch 54 with val_f1 0.9962\n",
      "--- Finished Training: final_check_v2 ---\n",
      "\n",
      "--- üèÜ Optimal Epochs Found: 54 ---\n",
      "Submission name will be: submission_GRU_H64_L3_BTrue_D0.5783_Static_Window_w100_s20_Optuna_FINAL.csv\n"
     ]
    }
   ],
   "source": [
    "# --- We need the original 'fit' function back (MODIFIED) ---\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    print(f\"Will train for {epochs} epochs with patience={patience} monitoring {evaluation_metric}\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # --- MODIFIED: Use new 3-part-batch train/val functions ---\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        # --- End modifications ---\n",
    "\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
    "            )\n",
    "\n",
    "        if verbose > 0 and (epoch % verbose == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                  f\"Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n",
    "                  f\"Val: Loss={val_loss:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    if restore_best_weights and patience > 0:\n",
    "        print(f\"Restoring best model from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    if patience == 0:\n",
    "        print(\"Training complete. Saving final model.\")\n",
    "        torch.save(model.state_dict(), model_path.replace(\"_best_model.pt\", \"_final_model.pt\"))\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    \n",
    "    print(f\"--- Finished Training: {experiment_name} ---\")\n",
    "    return model, training_history, best_epoch if 'best_epoch' in locals() else epochs\n",
    "\n",
    "# --- 1. Create DataLoaders for the best config ---\n",
    "best_batch_size = FINAL_CONFIG[\"batch_size\"]\n",
    "# --- MODIFIED: Use the 3-tensor datasets ---\n",
    "train_loader_final_check = make_loader(train_ds, batch_size=best_batch_size, shuffle=True, drop_last=True)\n",
    "val_loader_final_check = make_loader(val_ds, batch_size=best_batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# --- 2. Instantiate Fresh Model ---\n",
    "final_check_model = RecurrentClassifier(\n",
    "    input_size=N_TS_FEATURES,\n",
    "    static_input_size=N_STATIC_FEATURES, # --- NEW ---\n",
    "    hidden_size=FINAL_CONFIG[\"hidden_size\"],\n",
    "    num_layers=FINAL_CONFIG[\"num_layers\"],\n",
    "    num_classes=N_CLASSES,\n",
    "    dropout_rate=FINAL_CONFIG[\"dropout_rate\"],\n",
    "    bidirectional=FINAL_CONFIG[\"bidirectional\"],\n",
    "    rnn_type=FINAL_CONFIG[\"rnn_type\"]\n",
    ").to(device)\n",
    "\n",
    "if torch.__version__[0] >= \"2\":\n",
    "    final_check_model = torch.compile(final_check_model)\n",
    "\n",
    "final_check_optimizer = torch.optim.AdamW(final_check_model.parameters(), lr=FINAL_CONFIG[\"lr\"], weight_decay=FINAL_CONFIG[\"l2_lambda\"])\n",
    "final_check_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "final_check_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- 3. Run Training with Early Stopping ---\n",
    "print(\"--- Finding best epoch number for the winning model ---\\n\")\n",
    "_, _, FINAL_BEST_EPOCH = fit(\n",
    "    model=final_check_model,\n",
    "    train_loader=train_loader_final_check,\n",
    "    val_loader=val_loader_final_check,\n",
    "    epochs=200, # Max epochs\n",
    "    criterion=final_check_criterion,\n",
    "    optimizer=final_check_optimizer,\n",
    "    scaler=final_check_scaler,\n",
    "    device=device,\n",
    "    writer=None,\n",
    "    verbose=10,\n",
    "    experiment_name=\"final_check_v2\",\n",
    "    patience=30\n",
    ")\n",
    "\n",
    "print(f\"\\n--- üèÜ Optimal Epochs Found: {FINAL_BEST_EPOCH} ---\")\n",
    "\n",
    "# --- 4. Set variables for the submission cell ---\n",
    "FINAL_MODEL_TYPE = FINAL_CONFIG[\"rnn_type\"]\n",
    "FINAL_HIDDEN_SIZE = FINAL_CONFIG[\"hidden_size\"]\n",
    "FINAL_HIDDEN_LAYERS = FINAL_CONFIG[\"num_layers\"]\n",
    "FINAL_BIDIRECTIONAL = FINAL_CONFIG[\"bidirectional\"]\n",
    "FINAL_DROPOUT_RATE = FINAL_CONFIG[\"dropout_rate\"]\n",
    "FINAL_LEARNING_RATE = FINAL_CONFIG[\"lr\"]\n",
    "FINAL_L2_LAMBDA = FINAL_CONFIG[\"l2_lambda\"]\n",
    "FINAL_BATCH_SIZE = FINAL_CONFIG[\"batch_size\"]\n",
    "\n",
    "FINAL_EXPERIMENT_NAME = (\n",
    "    f\"{FINAL_MODEL_TYPE}_H{FINAL_HIDDEN_SIZE}_L{FINAL_HIDDEN_LAYERS}_B{FINAL_BIDIRECTIONAL}\"\n",
    "    f\"_D{FINAL_DROPOUT_RATE:.4f}_Static_Window_w{WINDOW_SIZE}_s{STRIDE}_Optuna_FINAL\"\n",
    ")\n",
    "\n",
    "print(f\"Submission name will be: submission_{FINAL_EXPERIMENT_NAME}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¨ **8. Create Submission (HEAVILY MODIFIED)**\n",
    " \n",
    "This cell is completely new to handle the complex submission logic.\n",
    "1.  Re-scale all data (TS and Static) on the *full* training set.\n",
    "2.  Apply **sliding windows** to the *full* training set and the *full* test set.\n",
    "3.  Train a new model on the *full windowed* training set.\n",
    "4.  Generate predictions on the *windowed test set*.\n",
    "5.  **Aggregate** the windowed predictions (e.g., 4 predictions for sample `000`) into a *single* prediction using a **majority vote**.\n",
    "6.  Save the final aggregated submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing full dataset for final training ---\n",
      "Fitting FINAL TS Scaler on X_train_full_ts_2d shape: (105760, 35)\n",
      "Fitting FINAL Static Scaler on X_train_full_static shape: (661, 6)\n",
      "Final scaling complete.\n",
      "--- Applying sliding windows to final dataset ---\n",
      "Full train windowed TS shape: (2644, 100, 35)\n",
      "Full train windowed Static shape: (2644, 6)\n",
      "Test windowed TS shape: (5296, 100, 35)\n",
      "Test windowed Static shape: (5296, 6)\n",
      "Test window-to-sample map shape: (5296,)\n",
      "Final DataLoaders created.\n",
      "\n",
      "--- Building FINAL model for submission: GRU_H64_L3_BTrue_D0.5783_Static_Window_w100_s20_Optuna_FINAL ---\n",
      "Compiling final model...\n",
      "Training final model for 54 epochs on ALL data...\n",
      "Final Training Epoch   1/54 | Train: Loss=0.7753, F1=0.7137\n",
      "Final Training Epoch   5/54 | Train: Loss=0.2956, F1=0.9096\n",
      "Final Training Epoch  10/54 | Train: Loss=0.1522, F1=0.9675\n",
      "Final Training Epoch  15/54 | Train: Loss=0.1381, F1=0.9731\n",
      "Final Training Epoch  20/54 | Train: Loss=0.1032, F1=0.9843\n",
      "Final Training Epoch  25/54 | Train: Loss=0.0855, F1=0.9886\n",
      "Final Training Epoch  30/54 | Train: Loss=0.0598, F1=0.9973\n",
      "Final Training Epoch  35/54 | Train: Loss=0.0847, F1=0.9862\n",
      "Final Training Epoch  40/54 | Train: Loss=0.0516, F1=0.9984\n",
      "Final Training Epoch  45/54 | Train: Loss=0.0471, F1=0.9980\n",
      "Final Training Epoch  50/54 | Train: Loss=0.0670, F1=0.9933\n",
      "Final Training Epoch  54/54 | Train: Loss=0.0509, F1=0.9953\n",
      "Final training complete.\n",
      "\n",
      "--- Generating predictions on test set (windowed) ---\n",
      "Generated 5296 predictions (from 5296 windows).\n",
      "Aggregating window predictions to sample predictions...\n",
      "Aggregated to 1324 final predictions.\n",
      "Loading sample submission file for correct formatting...\n",
      "Prediction count matches. Creating submission.\n",
      "\n",
      "Successfully saved to submissions\\submission_GRU_H64_L3_BTrue_D0.5783_Static_Window_w100_s20_Optuna_FINAL.csv!\n",
      "This file is correctly formatted for Kaggle:\n",
      "  sample_index      label\n",
      "0          000    no_pain\n",
      "1          001    no_pain\n",
      "2          002    no_pain\n",
      "3          003  high_pain\n",
      "4          004    no_pain\n"
     ]
    }
   ],
   "source": [
    "# --- 1. & 2. Create Full Training Set & Loader (with windows) ---\n",
    "print(\"\\n--- Preparing full dataset for final training ---\")\n",
    "\n",
    "# --- Scaler 1: Time-Series ---\n",
    "scaler_final_ts = StandardScaler()\n",
    "ns, ts, f = X_train_full_ts.shape\n",
    "X_train_full_ts_2d = X_train_full_ts.reshape(ns * ts, f)\n",
    "print(f\"Fitting FINAL TS Scaler on X_train_full_ts_2d shape: {X_train_full_ts_2d.shape}\")\n",
    "scaler_final_ts.fit(X_train_full_ts_2d)\n",
    "\n",
    "# Scale final TS Train\n",
    "X_train_full_ts_scaled_2d = scaler_final_ts.transform(X_train_full_ts_2d)\n",
    "X_train_full_ts_scaled = X_train_full_ts_scaled_2d.reshape(ns, ts, f)\n",
    "\n",
    "# Scale final TS Test\n",
    "ns_test, ts_test, f_test = X_test_ts.shape\n",
    "X_test_ts_2d = X_test_ts.reshape(ns_test * ts_test, f_test)\n",
    "X_test_ts_scaled_2d = scaler_final_ts.transform(X_test_ts_2d)\n",
    "X_test_ts_scaled = X_test_ts_scaled_2d.reshape(ns_test, ts_test, f_test)\n",
    "\n",
    "# --- Scaler 2: Static ---\n",
    "scaler_final_static = StandardScaler()\n",
    "print(f\"Fitting FINAL Static Scaler on X_train_full_static shape: {X_train_full_static.shape}\")\n",
    "scaler_final_static.fit(X_train_full_static)\n",
    "\n",
    "# Scale final Static Train\n",
    "X_train_full_static_scaled = scaler_final_static.transform(X_train_full_static)\n",
    "# Scale final Static Test\n",
    "X_test_static_scaled = scaler_final_static.transform(X_test_static)\n",
    "\n",
    "print(\"Final scaling complete.\")\n",
    "print(\"--- Applying sliding windows to final dataset ---\")\n",
    "\n",
    "# --- Apply windowing to the final training set ---\n",
    "(\n",
    "    X_train_full_windowed, \n",
    "    X_static_full_windowed, \n",
    "    y_train_full_windowed, \n",
    "    _\n",
    ") = create_sliding_windows(\n",
    "    X_train_full_ts_scaled,\n",
    "    X_train_full_static_scaled,\n",
    "    y_train_full,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    stride=STRIDE\n",
    ")\n",
    "\n",
    "# --- Apply windowing to the final test set ---\n",
    "(\n",
    "    X_test_final_windowed, \n",
    "    X_static_test_windowed, \n",
    "    _, \n",
    "    test_window_to_sample_idx # CRITICAL: We need this to map preds back\n",
    ") = create_sliding_windows(\n",
    "    X_test_ts_scaled,\n",
    "    X_test_static_scaled,\n",
    "    y=np.zeros(len(X_test_ts_scaled)), # Dummy 'y'\n",
    "    window_size=WINDOW_SIZE,\n",
    "    stride=STRIDE\n",
    ")\n",
    "\n",
    "print(f\"Full train windowed TS shape: {X_train_full_windowed.shape}\")\n",
    "print(f\"Full train windowed Static shape: {X_static_full_windowed.shape}\")\n",
    "print(f\"Test windowed TS shape: {X_test_final_windowed.shape}\")\n",
    "print(f\"Test windowed Static shape: {X_static_test_windowed.shape}\")\n",
    "print(f\"Test window-to-sample map shape: {test_window_to_sample_idx.shape}\")\n",
    "\n",
    "# --- Create Tensors and DataLoaders from WINDOWED data ---\n",
    "full_train_features_ts = torch.from_numpy(X_train_full_windowed).float()\n",
    "full_train_features_static = torch.from_numpy(X_static_full_windowed).float()\n",
    "full_train_targets = torch.from_numpy(y_train_full_windowed).long()\n",
    "\n",
    "final_test_features_ts = torch.from_numpy(X_test_final_windowed).float()\n",
    "final_test_features_static = torch.from_numpy(X_static_test_windowed).float()\n",
    "\n",
    "# 3-tensor dataset for training\n",
    "full_train_ds = TensorDataset(full_train_features_ts, full_train_features_static, full_train_targets)\n",
    "# 2-tensor dataset for test (no labels)\n",
    "final_test_ds = TensorDataset(final_test_features_ts, final_test_features_static)\n",
    "\n",
    "# --- NEW: Define the loader function here ---\n",
    "def make_final_loader(ds, batch_size, shuffle, drop_last):\n",
    "    return DataLoader(\n",
    "        ds, batch_size=int(batch_size), shuffle=shuffle, drop_last=drop_last,\n",
    "        num_workers=0, pin_memory=True, pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\", prefetch_factor=None\n",
    "    )\n",
    "\n",
    "full_train_loader = make_final_loader(full_train_ds, batch_size=FINAL_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = make_final_loader(final_test_ds, batch_size=FINAL_BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "print(\"Final DataLoaders created.\")\n",
    "\n",
    "# --- 3. Instantiate Fresh Model ---\n",
    "print(f\"\\n--- Building FINAL model for submission: {FINAL_EXPERIMENT_NAME} ---\")\n",
    "final_model = RecurrentClassifier(\n",
    "    input_size=N_TS_FEATURES,\n",
    "    static_input_size=N_STATIC_FEATURES,\n",
    "    hidden_size=FINAL_HIDDEN_SIZE,\n",
    "    num_layers=FINAL_HIDDEN_LAYERS,\n",
    "    num_classes=N_CLASSES,\n",
    "    dropout_rate=FINAL_DROPOUT_RATE,\n",
    "    bidirectional=FINAL_BIDIRECTIONAL,\n",
    "    rnn_type=FINAL_MODEL_TYPE\n",
    ").to(device)\n",
    "\n",
    "if torch.__version__[0] >= \"2\":\n",
    "    print(\"Compiling final model...\")\n",
    "    final_model = torch.compile(final_model)\n",
    "\n",
    "final_optimizer = torch.optim.AdamW(final_model.parameters(), lr=FINAL_LEARNING_RATE, weight_decay=FINAL_L2_LAMBDA)\n",
    "final_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# --- 4. Train on Full Dataset ---\n",
    "print(f\"Training final model for {FINAL_BEST_EPOCH} epochs on ALL data...\")\n",
    "\n",
    "final_model.train() \n",
    "for epoch in range(1, FINAL_BEST_EPOCH + 1):\n",
    "    train_loss, train_f1 = train_one_epoch(\n",
    "        final_model, full_train_loader, final_check_criterion, final_optimizer, final_scaler, device, 0, FINAL_L2_LAMBDA\n",
    "    )\n",
    "    if epoch % 5 == 0 or epoch == 1 or epoch == FINAL_BEST_EPOCH:\n",
    "        print(f\"Final Training Epoch {epoch:3d}/{FINAL_BEST_EPOCH} | Train: Loss={train_loss:.4f}, F1={train_f1:.4f}\")\n",
    "\n",
    "print(\"Final training complete.\")\n",
    "\n",
    "# --- 5. Generate Predictions (on windows) ---\n",
    "print(\"\\n--- Generating predictions on test set (windowed) ---\")\n",
    "final_model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # --- MODIFIED: Unpack 2-part batch ---\n",
    "    for (ts_inputs, static_inputs) in test_loader: \n",
    "        ts_inputs, static_inputs = ts_inputs.to(device), static_inputs.to(device)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            # --- MODIFIED: Pass both inputs ---\n",
    "            logits = final_model(ts_inputs, static_inputs)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_predictions.append(preds.cpu().numpy())\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "print(f\"Generated {len(all_predictions)} predictions (from {len(test_window_to_sample_idx)} windows).\")\n",
    "\n",
    "\n",
    "# --- 6. NEW: Aggregate Predictions (Majority Vote) ---\n",
    "print(\"Aggregating window predictions to sample predictions...\")\n",
    "\n",
    "df_preds = pd.DataFrame({\n",
    "    'original_index': test_window_to_sample_idx,\n",
    "    'prediction': all_predictions\n",
    "})\n",
    "\n",
    "# Group by the original sample index (0 to 1323) and find the most common prediction\n",
    "# mode(x)[0] gets the most frequent value\n",
    "agg_preds = df_preds.groupby('original_index')['prediction'].apply(lambda x: mode(x)[0]).values\n",
    "\n",
    "print(f\"Aggregated to {len(agg_preds)} final predictions.\")\n",
    "\n",
    "# Inverse transform these aggregated predictions to labels\n",
    "predicted_labels = le.inverse_transform(agg_preds)\n",
    "\n",
    "# --- 7. Save Submission File ---\n",
    "print(\"Loading sample submission file for correct formatting...\")\n",
    "X_test_long = pd.read_csv(X_TEST_PATH)\n",
    "test_sample_indices = sorted(X_test_long['sample_index'].unique())\n",
    "\n",
    "if len(predicted_labels) != len(test_sample_indices):\n",
    "    print(f\"ERROR: Prediction count mismatch! Predictions: {len(predicted_labels)}, Test Indices: {len(test_sample_indices)}\")\n",
    "else:\n",
    "    print(\"Prediction count matches. Creating submission.\")\n",
    "    \n",
    "    final_submission_df = pd.DataFrame({\n",
    "        'sample_index': test_sample_indices,\n",
    "        'label': predicted_labels \n",
    "    })\n",
    "    \n",
    "    final_submission_df['sample_index'] = final_submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "\n",
    "    SUBMISSIONS_DIR = \"submissions\"\n",
    "    os.makedirs(SUBMISSIONS_DIR, exist_ok=True)\n",
    "    \n",
    "    submission_filename = f\"submission_{FINAL_EXPERIMENT_NAME}.csv\"\n",
    "    submission_filepath = os.path.join(SUBMISSIONS_DIR, submission_filename)\n",
    "    \n",
    "    final_submission_df.to_csv(submission_filepath, index=False)\n",
    "\n",
    "    print(f\"\\nSuccessfully saved to {submission_filepath}!\")\n",
    "    print(\"This file is correctly formatted for Kaggle:\")\n",
    "    print(final_submission_df.head())\n",
    "\n",
    "del final_model, full_train_loader, test_loader\n",
    "del full_train_features_ts, full_train_features_static, final_test_features_ts, final_test_features_static"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
