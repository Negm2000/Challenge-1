{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-cell-md-v17",
   "metadata": {},
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è (v17: Final Version - Mean Aggregation)**\n",
    "\n",
    "This notebook represents the definitive final strategy. It **reverts the submission logic back to the original, more robust `mean` windowing aggregation**, as the `Top-K` strategy proved to be ineffective on the leaderboard. \n",
    "\n",
    "**üî• Summary of Final Strategy:**\n",
    "\n",
    "1.  **‚úÖ `Mean` Aggregation Restored:** The final prediction is made by averaging the probabilities from **all** sliding windows. This is a more robust strategy that considers the full context of the time series.\n",
    "2.  **Stable, Aligned HPO:** The HPO process is correctly aligned with the training pipeline (augmentation, weighted sampling) and is stable on Windows (`num_workers=0`, short trial names).\n",
    "3.  **Hold-Out Test Set (10%):** A 10% test set is used for a final, unbiased evaluation before submission.\n",
    "4.  **Full Suite of Balancing Techniques:**\n",
    "    - **Feature Cleaning:** `joint_30` is removed.\n",
    "    - **Data Augmentation:** Noise injection for minority classes.\n",
    "    - **WeightedRandomSampler:** Balanced batches during training.\n",
    "    - **Focal Loss:** Advanced loss function.\n",
    "5.  **Optimized Performance:** Uses the `cudagraphs` backend for fast, stable training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03291d-v17",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Setup & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba22088-v17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 1234\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "import time\n",
    "import gc\n",
    "\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from functools import partial\n",
    "\n",
    "logs_dir = \"tensorboard\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8582ef-v17",
   "metadata": {},
   "source": [
    "## üîÑ 2. Data Loading & Pre-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44203b00-v17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 1. Loading Data ---\")\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "\n",
    "features_long_df = pd.read_csv(X_TRAIN_PATH)\n",
    "labels_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "X_test_long_df = pd.read_csv(X_TEST_PATH)\n",
    "\n",
    "if 'joint_30' in features_long_df.columns:\n",
    "    features_long_df = features_long_df.drop(columns=['joint_30'])\n",
    "    X_test_long_df = X_test_long_df.drop(columns=['joint_30'])\n",
    "    print(\"Removed zero-variance feature: 'joint_30'\")\n",
    "\n",
    "N_TIMESTEPS = 160\n",
    "JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(30)]\n",
    "PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "TIME_FEATURE = ['time']\n",
    "FEATURES = JOINT_FEATURES + PAIN_FEATURES + TIME_FEATURE\n",
    "LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "def reshape_data(df, features_list, n_timesteps):\n",
    "    df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "    data_2d = df_pivot.values\n",
    "    n_samples = data_2d.shape[0]\n",
    "    data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "    return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "X_train_full = reshape_data(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())], FEATURES, N_TIMESTEPS)\n",
    "y_train_full_df = labels_df.sort_values(by='sample_index')\n",
    "le = LabelEncoder().fit(list(LABEL_MAPPING.keys()))\n",
    "y_train_full = le.transform(y_train_full_df['label'])\n",
    "\n",
    "print(\"\\n--- 2. Engineering 'is_pirate' Feature ---\")\n",
    "static_cols = ['sample_index', 'n_legs', 'n_hands', 'n_eyes']\n",
    "static_df = features_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "pirate_filter = (static_df['n_legs'] == 'one+peg_leg') | (static_df['n_hands'] == 'one+hook_hand') | (static_df['n_eyes'] == 'one+eye_patch')\n",
    "pirate_indices = static_df[pirate_filter].index\n",
    "sample_indices_ordered = sorted(features_long_df[features_long_df['sample_index'].isin(labels_df['sample_index'].unique())]['sample_index'].unique())\n",
    "is_pirate_map = np.array([1 if idx in pirate_indices else 0 for idx in sample_indices_ordered])\n",
    "pirate_feature_broadcast = np.tile(is_pirate_map.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "X_train_full_engineered = np.concatenate([X_train_full, pirate_feature_broadcast], axis=2)\n",
    "\n",
    "print(\"\\n--- 3. Creating 90/10 Train/Test Split ---\")\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=SEED)\n",
    "for train_indices, test_indices in splitter.split(X_train_full_engineered, y_train_full):\n",
    "    X_main_train, y_main_train = X_train_full_engineered[train_indices], y_train_full[train_indices]\n",
    "    X_holdout_test, y_holdout_test = X_train_full_engineered[test_indices], y_train_full[test_indices]\n",
    "\n",
    "print(f\"Main training set shape: {X_main_train.shape}\")\n",
    "print(f\"Hold-out test set shape: {X_holdout_test.shape}\")\n",
    "\n",
    "print(\"\\n--- 4. Calculating Alpha Weights for Focal Loss (from main training set) ---\")\n",
    "class_counts = np.bincount(y_main_train)\n",
    "class_weights_tensor = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n",
    "alpha_tensor = (class_weights_tensor / class_weights_tensor.sum()).to(device)\n",
    "print(f\"Class counts (0, 1, 2) in main training set: {class_counts}\")\n",
    "print(f\"Calculated alpha weights: {alpha_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f8bc4-v17",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 3. Helper Functions & Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c2d0e-v17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets].to(focal_loss.device)\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "        if self.reduction == 'mean': return focal_loss.mean()\n",
    "        elif self.reduction == 'sum': return focal_loss.sum()\n",
    "        else: return focal_loss\n",
    "\n",
    "def create_sliding_windows(X_3d, y=None, window_size=100, stride=20):\n",
    "    new_X, new_y, window_indices = [], [], []\n",
    "    n_samples, n_timesteps, _ = X_3d.shape\n",
    "    for i in range(n_samples):\n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            new_X.append(X_3d[i, idx:idx+window_size, :])\n",
    "            window_indices.append(i)\n",
    "            if y is not None: new_y.append(y[i])\n",
    "            idx += stride\n",
    "    if y is not None: return np.array(new_X), np.array(new_y), np.array(window_indices)\n",
    "    return np.array(new_X), np.array(window_indices)\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last, sampler=None, num_workers=4):\n",
    "    use_shuffle = shuffle if sampler is None else False\n",
    "    return DataLoader(ds, batch_size=int(batch_size), shuffle=use_shuffle, drop_last=drop_last, \n",
    "                      num_workers=num_workers, pin_memory=True, persistent_workers=(num_workers > 0), sampler=sampler)\n",
    "\n",
    "def augment_minority_classes(X_w, y_w, continuous_feature_count, noise_level=0.01, aug_factor=1):\n",
    "    X_aug_list, y_aug_list = [X_w], [y_w]\n",
    "    minority_indices = np.where((y_w == 1) | (y_w == 2))[0]\n",
    "    if len(minority_indices) == 0: return X_w, y_w\n",
    "    for _ in range(aug_factor):\n",
    "        X_to_augment = X_w[minority_indices]\n",
    "        noise = np.random.normal(0, noise_level, X_to_augment.shape)\n",
    "        X_augmented = X_to_augment.copy()\n",
    "        X_augmented[:, :, :continuous_feature_count] += noise[:, :, :continuous_feature_count]\n",
    "        X_aug_list.append(X_augmented)\n",
    "        y_aug_list.append(y_w[minority_indices])\n",
    "    return np.concatenate(X_aug_list, axis=0), np.concatenate(y_aug_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faed930-v17",
   "metadata": {},
   "source": [
    "## üß† 4. Model & Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3cadd5-v17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, rnn_outputs):\n",
    "        energy = torch.tanh(self.attn(rnn_outputs))\n",
    "        attn_scores = self.v(energy).squeeze(2)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(1), rnn_outputs).squeeze(1)\n",
    "        return context_vector\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes,\n",
    "                 conv_out_channels, conv_kernel_size, bidirectional,\n",
    "                 dropout_rate, feature_dropout_rate, rnn_type='GRU'):\n",
    "        super().__init__()\n",
    "        self.rnn_type, self.num_layers, self.hidden_size, self.bidirectional = \\\n",
    "            rnn_type, num_layers, hidden_size, bidirectional\n",
    "        \n",
    "        rnn_hidden_dim = hidden_size * 2 if bidirectional else hidden_size\n",
    "\n",
    "        self.pain_embed_dim, self.pirate_embed_dim = 4, 4\n",
    "        self.pain_embeddings = nn.ModuleList([nn.Embedding(3, self.pain_embed_dim) for _ in range(4)])\n",
    "        self.pirate_embedding = nn.Embedding(2, self.pirate_embed_dim)\n",
    "        \n",
    "        num_continuous_features = 31 # 30 joints + 1 time\n",
    "        total_embedding_dim = (4 * self.pain_embed_dim) + self.pirate_embed_dim\n",
    "        conv_input_size = num_continuous_features + total_embedding_dim\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=conv_input_size, out_channels=conv_out_channels,\n",
    "                                kernel_size=conv_kernel_size, padding='same')\n",
    "        self.conv_activation = nn.ReLU()\n",
    "        self.feature_dropout = nn.Dropout(feature_dropout_rate)\n",
    "\n",
    "        rnn_class = nn.GRU if rnn_type == 'GRU' else nn.LSTM\n",
    "        self.rnn = rnn_class(\n",
    "            input_size=conv_out_channels, hidden_size=hidden_size,\n",
    "            num_layers=num_layers, batch_first=True, bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        self.attention = Attention(rnn_hidden_dim)\n",
    "        self.classifier = nn.Linear(rnn_hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_continuous = x[:, :, :31]\n",
    "        x_categorical = x[:, :, 31:].long()\n",
    "        \n",
    "        embedded_cats = [self.pain_embeddings[i](x_categorical[:, :, i]) for i in range(4)] \\\n",
    "                      + [self.pirate_embedding(x_categorical[:, :, 4])]\n",
    "        x_combined = torch.cat([x_continuous] + embedded_cats, dim=2)\n",
    "        x_permuted = x_combined.permute(0, 2, 1)\n",
    "        x_conv = self.conv_activation(self.conv1d(x_permuted))\n",
    "        x_conv_permuted = x_conv.permute(0, 2, 1)\n",
    "        x_dropped = self.feature_dropout(x_conv_permuted)\n",
    "        rnn_outputs, _ = self.rnn(x_dropped)\n",
    "        context_vector = self.attention(rnn_outputs)\n",
    "        return self.classifier(context_vector)\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    num_samples = len(loader.sampler) if loader.sampler else len(loader.dataset)\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / num_samples, f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, all_preds, all_targets = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            all_preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset.tensors[0]), f1_score(np.concatenate(all_targets), np.concatenate(all_preds), average='weighted')\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scheduler, scaler, device, patience, experiment_name):\n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "    best_f1 = -1; patience_counter = 0\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, scaler, scheduler, device)\n",
    "        val_loss, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        if epoch % 10 == 0: print(f\"Epoch {epoch:3d}/{epochs} | Val F1: {val_f1:.4f} | Train Loss: {train_loss:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1, patience_counter = val_f1, 0\n",
    "            torch.save(model._orig_mod.state_dict() if hasattr(model, '_orig_mod') else model.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience: print(f\"Early stopping at epoch {epoch}. Best F1: {best_f1:.4f}\"); break\n",
    "    print(f\"--- Finished Training --- Best F1: {best_f1:.4f}\")\n",
    "    uncompiled_model = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "    uncompiled_model.load_state_dict(torch.load(model_path))\n",
    "    return uncompiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f911c-v17",
   "metadata": {},
   "source": [
    "## üß™ 5. Phase 1: Aligned & Stable Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-func-cell-v17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(config, X_train_w_ref, y_train_w_ref, X_val_w_ref, y_val_w_ref, alpha_tensor, continuous_indices_reordered):\n",
    "    X_train_w = ray.get(X_train_w_ref)\n",
    "    y_train_w = ray.get(y_train_w_ref)\n",
    "    X_val_w = ray.get(X_val_w_ref)\n",
    "    y_val_w = ray.get(y_val_w_ref)\n",
    "    \n",
    "    EPOCHS = 100\n",
    "    \n",
    "    X_train_w_aug, y_train_w_aug = augment_minority_classes(\n",
    "        X_train_w, y_train_w, continuous_feature_count=len(continuous_indices_reordered), aug_factor=1\n",
    "    )\n",
    "    \n",
    "    class_counts = np.bincount(y_train_w_aug)\n",
    "    class_weights = 1. / (class_counts + 1e-9)\n",
    "    sample_weights = np.array([class_weights[t] for t in y_train_w_aug])\n",
    "    sampler = WeightedRandomSampler(torch.from_numpy(sample_weights).double(), num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    train_ds = TensorDataset(torch.from_numpy(X_train_w_aug).float(), torch.from_numpy(y_train_w_aug).long())\n",
    "    val_ds = TensorDataset(torch.from_numpy(X_val_w).float(), torch.from_numpy(y_val_w).long())\n",
    "    \n",
    "    train_loader = make_loader(train_ds, config[\"batch_size\"], shuffle=False, drop_last=True, sampler=sampler, num_workers=0)\n",
    "    val_loader = make_loader(val_ds, config[\"batch_size\"], False, False, num_workers=0)\n",
    "\n",
    "    model_config = {k: v for k, v in config.items() if k not in ['lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "    model = RecurrentClassifier(**model_config, num_classes=N_CLASSES).to(device)\n",
    "    model = torch.compile(model, backend=\"cudagraphs\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=config[\"lr\"], epochs=EPOCHS, steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = FocalLoss(alpha=alpha_tensor, gamma=config['focal_loss_gamma'])\n",
    "\n",
    "    best_val_f1 = -1.0; patience_counter = 0; hpo_patience = 20\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, _ = train_one_epoch(model, train_loader, criterion, optimizer, scaler, scheduler, device)\n",
    "        _, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        tune.report({\"val_f1\": val_f1, \"train_loss\": train_loss})\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1; patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= hpo_patience: break\n",
    "    del model, train_loader, val_loader\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42797c7-v17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Preparing data for HPO ---\")\n",
    "WINDOW_SIZE = 10\n",
    "STRIDE = 2\n",
    "\n",
    "continuous_indices_orig = list(range(30)) + [34]\n",
    "categorical_indices_orig = list(range(30, 34)) + [35]\n",
    "X_main_train_reordered = np.concatenate([\n",
    "    X_main_train[:, :, continuous_indices_orig],\n",
    "    X_main_train[:, :, categorical_indices_orig]], axis=2)\n",
    "\n",
    "hpo_splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "for hpo_train_idx, hpo_val_idx in hpo_splitter.split(X_main_train_reordered, y_main_train):\n",
    "    X_hpo_train, y_hpo_train = X_main_train_reordered[hpo_train_idx], y_main_train[hpo_train_idx]\n",
    "    X_hpo_val, y_hpo_val = X_main_train_reordered[hpo_val_idx], y_main_train[hpo_val_idx]\n",
    "\n",
    "continuous_indices_reordered = list(range(31))\n",
    "preprocessor_hpo = ColumnTransformer([('scaler', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "ns, ts, f = X_hpo_train.shape\n",
    "X_hpo_train_scaled = preprocessor_hpo.fit_transform(X_hpo_train.reshape(ns*ts, f)).reshape(ns, ts, f)\n",
    "ns_val, ts_val, f_val = X_hpo_val.shape\n",
    "X_hpo_val_scaled = preprocessor_hpo.transform(X_hpo_val.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "\n",
    "X_train_w, y_train_w, _ = create_sliding_windows(X_hpo_train_scaled, y_hpo_train, WINDOW_SIZE, STRIDE)\n",
    "X_val_w, y_val_w, _ = create_sliding_windows(X_hpo_val_scaled, y_hpo_val, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "search_space = {\n",
    "    \"rnn_type\": tune.choice(['GRU', 'LSTM']),\n",
    "    \"focal_loss_gamma\": tune.uniform(0.5, 3.0),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"batch_size\": tune.choice([64, 128]),\n",
    "    \"hidden_size\": tune.choice([256, 384, 512]), \n",
    "    \"num_layers\": tune.choice([2, 3]),\n",
    "    \"dropout_rate\": tune.uniform(0, 0.5), \n",
    "    \"feature_dropout_rate\": tune.uniform(0, 0.5),\n",
    "    \"bidirectional\": tune.choice([True, False]), \n",
    "    \"l2_lambda\": tune.loguniform(1e-8, 1e-5),\n",
    "    \"conv_out_channels\": tune.choice([128]), \n",
    "    \"conv_kernel_size\": tune.choice([5])\n",
    "}\n",
    "\n",
    "if ray.is_initialized(): ray.shutdown()\n",
    "ray.init(num_cpus=os.cpu_count(), num_gpus=1, ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "X_train_w_ref = ray.put(X_train_w)\n",
    "y_train_w_ref = ray.put(y_train_w)\n",
    "X_val_w_ref = ray.put(X_val_w)\n",
    "y_val_w_ref = ray.put(y_val_w)\n",
    "\n",
    "def short_trial_name(trial):\n",
    "    return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "print(\"--- Starting Aligned & Stable HPO ---\")\n",
    "analysis = tune.run(\n",
    "    partial(objective_function, \n",
    "            X_train_w_ref=X_train_w_ref, y_train_w_ref=y_train_w_ref,\n",
    "            X_val_w_ref=X_val_w_ref, y_val_w_ref=y_val_w_ref,\n",
    "            alpha_tensor=alpha_tensor,\n",
    "            continuous_indices_reordered=continuous_indices_reordered),\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 0.25},\n",
    "    config=search_space, \n",
    "    num_samples=60,\n",
    "    search_alg=OptunaSearch(metric=\"val_f1\", mode=\"max\"),\n",
    "    scheduler=ASHAScheduler(metric=\"val_f1\", mode=\"max\", grace_period=15, reduction_factor=2),\n",
    "    name=\"pirate_pain_aligned_hpo_v17\",\n",
    "    verbose=1,\n",
    "    trial_dirname_creator=short_trial_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbf60f-v17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Loading HPO Search Results ---\")\n",
    "\n",
    "try:\n",
    "    best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "    if best_trial:\n",
    "        FINAL_CONFIG = best_trial.config\n",
    "        FINAL_BEST_VAL_F1 = best_trial.last_result.get(\"val_f1\", 0.0) \n",
    "        print(f\"Best validation F1 score from HPO: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "        print(\"Best hyperparameters found:\")\n",
    "        print(FINAL_CONFIG)\n",
    "    else: raise ValueError(\"No successful trials found.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nWARNING: Could not load HPO analysis. The error was: {e}\")\n",
    "    print(\"\\n--- USING FALLBACK CONFIGURATION ---\")\n",
    "    FINAL_CONFIG = {'rnn_type': 'GRU', 'focal_loss_gamma': 2.8, 'lr': 0.0015, 'batch_size': 128, 'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.35, 'feature_dropout_rate': 0.33, 'bidirectional': False, 'l2_lambda': 4.2e-06, 'conv_out_channels': 128, 'conv_kernel_size': 5}\n",
    "    FINAL_BEST_VAL_F1 = 0.93 # Fallback score reflects the harder, more realistic HPO task\n",
    "    print(\"Best hyperparameters (fallback):\\n\", FINAL_CONFIG)\n",
    "\n",
    "FINAL_CONFIG['window_size'] = WINDOW_SIZE\n",
    "FINAL_CONFIG['stride'] = STRIDE\n",
    "\n",
    "del X_train_w_ref, y_train_w_ref, X_val_w_ref, y_val_w_ref, X_hpo_train, y_hpo_train, X_hpo_val, y_hpo_val, X_hpo_train_scaled, X_hpo_val_scaled\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c0bb8-v17",
   "metadata": {},
   "source": [
    "## üèÜ 6. Phase 2: K-Fold Ensemble Training on 90% Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13308e8c-v17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- üèÜ Final Configuration Set for K-Fold Training --- \")\n",
    "N_SPLITS = 5\n",
    "FINAL_EXPERIMENT_NAME = f\"StableFinalMean-{FINAL_CONFIG['rnn_type']}_H{FINAL_CONFIG['hidden_size']}_v17\"\n",
    "submission_filename_base = f\"submission_{FINAL_EXPERIMENT_NAME}.csv\"\n",
    "print(f\"Submission name will be: {submission_filename_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda55dd-v17",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "fold_val_f1_list = []\n",
    "continuous_indices_reordered = list(range(31))\n",
    "EPOCHS = 350\n",
    "PATIENCE = 100\n",
    "\n",
    "X_main_train_reordered = np.concatenate([\n",
    "    X_main_train[:, :, continuous_indices_orig],\n",
    "    X_main_train[:, :, categorical_indices_orig]], axis=2)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_main_train_reordered, y_main_train)):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} --- ({fold_name}) ---\")\n",
    "    \n",
    "    X_train_fold, y_train_fold = X_main_train_reordered[train_idx], y_main_train[train_idx]\n",
    "    X_val_fold, y_val_fold = X_main_train_reordered[val_idx], y_main_train[val_idx]\n",
    "\n",
    "    preprocessor_fold = ColumnTransformer([('s', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "    ns, ts, f = X_train_fold.shape\n",
    "    X_train_scaled = preprocessor_fold.fit_transform(X_train_fold.reshape(ns*ts, f)).reshape(ns, ts, f)\n",
    "    ns_val, ts_val, f_val = X_val_fold.shape\n",
    "    X_val_scaled = preprocessor_fold.transform(X_val_fold.reshape(ns_val*ts_val, f_val)).reshape(ns_val, ts_val, f_val)\n",
    "    \n",
    "    X_train_w, y_train_w, _ = create_sliding_windows(X_train_scaled, y_train_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    X_val_w, y_val_w, _ = create_sliding_windows(X_val_scaled, y_val_fold, FINAL_CONFIG['window_size'], FINAL_CONFIG['stride'])\n",
    "    \n",
    "    X_train_w_aug, y_train_w_aug = augment_minority_classes(X_train_w, y_train_w, continuous_feature_count=len(continuous_indices_reordered), aug_factor=1)\n",
    "\n",
    "    class_counts = np.bincount(y_train_w_aug)\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = np.array([class_weights[t] for t in y_train_w_aug])\n",
    "    sampler = WeightedRandomSampler(torch.from_numpy(sample_weights).double(), num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    train_ds = TensorDataset(torch.from_numpy(X_train_w_aug).float(), torch.from_numpy(y_train_w_aug).long())\n",
    "    val_ds = TensorDataset(torch.from_numpy(X_val_w).float(), torch.from_numpy(y_val_w).long())\n",
    "    train_loader = make_loader(train_ds, FINAL_CONFIG['batch_size'], shuffle=False, drop_last=True, sampler=sampler, num_workers=4)\n",
    "    val_loader = make_loader(val_ds, FINAL_CONFIG['batch_size'], shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "    model_config_kfold = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "    model_fold = RecurrentClassifier(**model_config_kfold, num_classes=N_CLASSES).to(device)\n",
    "    model_fold = torch.compile(model_fold, backend=\"cudagraphs\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model_fold.parameters(), lr=FINAL_CONFIG['lr'], weight_decay=FINAL_CONFIG['l2_lambda'])\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=FINAL_CONFIG['lr'], epochs=EPOCHS, steps_per_epoch=len(train_loader))\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    criterion = FocalLoss(alpha=alpha_tensor, gamma=FINAL_CONFIG['focal_loss_gamma'])\n",
    "\n",
    "    model_fold_uncompiled = fit(model_fold, train_loader, val_loader, EPOCHS, criterion, optimizer, scheduler, scaler, device, PATIENCE, fold_name)\n",
    "    \n",
    "    _, val_f1 = validate_one_epoch(model_fold_uncompiled, val_loader, criterion, device)\n",
    "    fold_val_f1_list.append(val_f1)\n",
    "    print(f\"Fold {fold+1} Final Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n--- üèÜ K-Fold Training Complete --- Average CV F1: {np.mean(fold_val_f1_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-eval-cell-md-v17",
   "metadata": {},
   "source": [
    "## üìä 7. Final Model Evaluation on Hold-Out Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-eval-cell-code-v17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Evaluating ensembled model on the 10% hold-out test set ---\")\n",
    "\n",
    "X_holdout_test_reordered = np.concatenate([\n",
    "    X_holdout_test[:, :, continuous_indices_orig],\n",
    "    X_holdout_test[:, :, categorical_indices_orig]], axis=2)\n",
    "\n",
    "final_preprocessor = ColumnTransformer([('scaler', StandardScaler(), continuous_indices_reordered)], remainder='passthrough')\n",
    "ns, ts, f = X_main_train_reordered.shape\n",
    "final_preprocessor.fit(X_main_train_reordered.reshape(ns * ts, f))\n",
    "\n",
    "ns_test, ts_test, f_test = X_holdout_test_reordered.shape\n",
    "X_holdout_scaled = final_preprocessor.transform(X_holdout_test_reordered.reshape(ns_test * ts_test, f_test)).reshape(ns_test, ts_test, f_test)\n",
    "X_holdout_w, holdout_window_indices = create_sliding_windows(X_holdout_scaled, y=None, window_size=FINAL_CONFIG['window_size'], stride=FINAL_CONFIG['stride'])\n",
    "holdout_loader = make_loader(TensorDataset(torch.from_numpy(X_holdout_w).float()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "model_config_final = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "all_fold_probs = []\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    model = RecurrentClassifier(**model_config_final, num_classes=N_CLASSES).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in holdout_loader:\n",
    "            probs = torch.softmax(model(inputs.to(device)), dim=1)\n",
    "            fold_preds.append(probs.cpu().numpy())\n",
    "    all_fold_probs.append(np.concatenate(fold_preds))\n",
    "\n",
    "# --- REVERTED TO MEAN: Reverting to the original, more robust mean aggregation ---\n",
    "print(\"\\n--- Aggregating predictions using Mean Aggregation ---\")\n",
    "mean_probs = np.mean(all_fold_probs, axis=0)\n",
    "df_probs = pd.DataFrame(mean_probs, columns=[f\"prob_{c}\" for c in range(N_CLASSES)])\n",
    "df_probs['original_index'] = holdout_window_indices\n",
    "\n",
    "agg_probs = df_probs.groupby('original_index')[[f\"prob_{c}\" for c in range(N_CLASSES)]].mean().values\n",
    "final_holdout_preds = np.argmax(agg_probs, axis=1)\n",
    "\n",
    "print(f\"\\n--- Classification Report on {len(y_holdout_test)}-Sample Hold-Out Set ---\")\n",
    "print(classification_report(y_holdout_test, final_holdout_preds, target_names=le.classes_))\n",
    "\n",
    "print(\"--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_holdout_test, final_holdout_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix on Hold-Out Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9d380-v17",
   "metadata": {},
   "source": [
    "## üì¨ 8. Phase 3: Ensemble Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31871e7-v17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Preparing competition test dataset for submission ---\")\n",
    "\n",
    "X_test_full = reshape_data(X_test_long_df, FEATURES, N_TIMESTEPS)\n",
    "static_df_test = X_test_long_df[static_cols].drop_duplicates().set_index('sample_index')\n",
    "pirate_filter_test = (static_df_test['n_legs'] == 'one+peg_leg') | (static_df_test['n_hands'] == 'one+hook_hand') | (static_df_test['n_eyes'] == 'one+eye_patch')\n",
    "pirate_indices_test = static_df_test[pirate_filter_test].index\n",
    "sample_indices_test_ordered = sorted(X_test_long_df['sample_index'].unique())\n",
    "is_pirate_map_test = np.array([1 if idx in pirate_indices_test else 0 for idx in sample_indices_test_ordered])\n",
    "pirate_feature_broadcast_test = np.tile(is_pirate_map_test.reshape(-1, 1, 1), (1, N_TIMESTEPS, 1))\n",
    "X_test_full_engineered = np.concatenate([X_test_full, pirate_feature_broadcast_test], axis=2)\n",
    "\n",
    "X_test_reordered = np.concatenate([\n",
    "    X_test_full_engineered[:, :, continuous_indices_orig],\n",
    "    X_test_full_engineered[:, :, categorical_indices_orig]], axis=2)\n",
    "\n",
    "ns_test, ts_test, f_test = X_test_reordered.shape\n",
    "X_test_scaled = final_preprocessor.transform(X_test_reordered.reshape(ns_test * ts_test, f_test)).reshape(ns_test, ts_test, f_test)\n",
    "\n",
    "X_test_w, test_window_indices = create_sliding_windows(X_test_scaled, y=None, window_size=FINAL_CONFIG['window_size'], stride=FINAL_CONFIG['stride'])\n",
    "submission_loader = make_loader(TensorDataset(torch.from_numpy(X_test_w).float()), FINAL_CONFIG['batch_size'], False, False)\n",
    "\n",
    "all_fold_probs_submission = []\n",
    "model_config_final = {k: v for k, v in FINAL_CONFIG.items() if k not in ['window_size', 'stride', 'lr', 'batch_size', 'l2_lambda', 'focal_loss_gamma']}\n",
    "\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_name = f\"{FINAL_EXPERIMENT_NAME}_fold_{fold+1}\"\n",
    "    model_path = f\"models/{fold_name}_best_model.pt\"\n",
    "    model = RecurrentClassifier(**model_config_final, num_classes=N_CLASSES).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in submission_loader:\n",
    "            probs = torch.softmax(model(inputs.to(device)), dim=1)\n",
    "            fold_preds.append(probs.cpu().numpy())\n",
    "    all_fold_probs_submission.append(np.concatenate(fold_preds))\n",
    "\n",
    "# --- REVERTED TO MEAN: Using the robust mean aggregation for the final submission ---\n",
    "mean_probs_sub = np.mean(all_fold_probs_submission, axis=0)\n",
    "df_probs_sub = pd.DataFrame(mean_probs_sub, columns=[f\"prob_{c}\" for c in range(N_CLASSES)])\n",
    "df_probs_sub['original_index'] = test_window_indices\n",
    "\n",
    "agg_probs_sub = df_probs_sub.groupby('original_index')[[f\"prob_{c}\" for c in range(N_CLASSES)]].mean().values\n",
    "final_predictions = le.inverse_transform(np.argmax(agg_probs_sub, axis=1))\n",
    "\n",
    "submission_df = pd.DataFrame({'sample_index': sorted(X_test_long_df['sample_index'].unique()), 'label': final_predictions})\n",
    "submission_df['sample_index'] = submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "submission_filepath = os.path.join(\"submissions\", submission_filename_base)\n",
    "submission_df.to_csv(submission_filepath, index=False)\n",
    "print(f\"\\nSuccessfully saved to {submission_filepath}!\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}