{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "markdown-notebook-intro"
   },
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è**\n",
    "\n",
    "This notebook is a modified version of the Lecture 4 (Timeseries Classification) code, adapted for the Kaggle challenge.\n",
    "\n",
    "**Local Setup:**\n",
    "1.  Ensure you have a Conda environment with PyTorch (GPU), `pandas`, `sklearn`, and `jupyterlab`.\n",
    "2.  Place the Kaggle CSVs (`pirate_pain_train.csv`, `pirate_pain_train_labels.csv`, `pirate_pain_test.csv`) in a folder named `data/` in the same directory as this notebook.\n",
    "3.  To run TensorBoard, open a separate terminal, `conda activate` your environment, `cd` to this folder, and run: `tensorboard --logdir=./tensorboard`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "markdown-libraries"
   },
   "source": [
    "## ‚öôÔ∏è **1. Setup & Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "code-libraries"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GPU (RTX 3070, here we come!) ---\n",
      "PyTorch version: 2.5.1\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Sklearn Imports ---\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Setup Directories & Device ---\n",
    "logs_dir = \"tensorboard\"\n",
    "!mkdir -p models\n",
    "!mkdir -p $logs_dir\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    # This is a performance optimization for when input sizes don't vary\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU (RTX 3070, here we come!) ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "markdown-load-reshape"
   },
   "source": [
    "## üîÑ **2. Data Loading & Reshaping**\n",
    "\n",
    "This is the most critical new step. The data is in a \"long\" format (one row per timestep), where each `sample_index` is one complete time series. We must:\n",
    "1.  Define the features we want to use.\n",
    "2.  Pivot the data to get a 3D tensor of shape `(num_samples, num_timesteps, num_features)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "code-load-reshape"
   },
   "outputs": [],
   "source": [
    "# --- 1. Define File Paths and Features ---\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "# Define our time-series features\n",
    "# We'll ignore static features (n_legs, etc.) for our baseline model\n",
    "JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(31)]\n",
    "PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "FEATURES = JOINT_FEATURES + PAIN_FEATURES\n",
    "\n",
    "N_FEATURES = len(FEATURES)\n",
    "N_TIMESTEPS = 180 # Given in the problem description\n",
    "\n",
    "print(f\"Using {N_FEATURES} features: {FEATURES[:3]}... to {FEATURES[-3:]}\")\n",
    "\n",
    "# --- 2. Create the Reshaping Function ---\n",
    "def reshape_data(df, features_list, n_timesteps):\n",
    "    \"\"\"\n",
    "    Pivots the long-format dataframe into a 3D NumPy array.\n",
    "    Shape: (n_samples, n_timesteps, n_features)\n",
    "    \"\"\"\n",
    "    # Use pivot to create a (n_samples, n_features * n_timesteps) table\n",
    "    # This works because the (sample_index, time) pairs are unique\n",
    "    df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "    \n",
    "    # The resulting columns are multi-level (e.g., ('joint_00', 0), ('joint_00', 1)...)\n",
    "    # We don't need to sort them; pivot already preserves order.\n",
    "    \n",
    "    # Get the raw values, which is a 2D array\n",
    "    # Shape: (n_samples, n_features * n_timesteps)\n",
    "    data_2d = df_pivot.values\n",
    "    \n",
    "    # Get the number of samples\n",
    "    n_samples = data_2d.shape[0]\n",
    "    \n",
    "    # Reshape to (n_samples, n_features, n_timesteps)\n",
    "    # We shape it this way first because of how .values flattens the pivot table\n",
    "    data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "    \n",
    "    # **CRITICAL STEP**: Transpose to match RNN expected input\n",
    "    # We want (n_samples, n_timesteps, n_features) for batch_first=True\n",
    "    return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "# --- 3. Load and Reshape Data ---\n",
    "print(\"Loading and reshaping training data...\")\n",
    "X_train_long = pd.read_csv(X_TRAIN_PATH)\n",
    "X_train_full = reshape_data(X_train_long[X_train_long['sample_index'].isin(X_train_long['sample_index'].unique())], FEATURES, N_TIMESTEPS)\n",
    "\n",
    "print(\"Loading and reshaping test data...\")\n",
    "X_test_long = pd.read_csv(X_TEST_PATH)\n",
    "X_test = reshape_data(X_test_long, FEATURES, N_TIMESTEPS)\n",
    "\n",
    "# Load labels\n",
    "y_train_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "# We need to sort by sample_index to match X_train_full, which is sorted by pivot\n",
    "y_train_full_df = y_train_df.sort_values(by='sample_index')\n",
    "y_train_labels_str = y_train_full_df['target'].values\n",
    "\n",
    "print(f\"X_train_full shape: {X_train_full.shape}\")\n",
    "print(f\"y_train_labels_str shape: {y_train_labels_str.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Clean up large dataframes to save memory\n",
    "del X_train_long, X_test_long, y_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "markdown-preprocessing"
   },
   "source": [
    "## üöß **3. Preprocessing: Split & Scale**\n",
    "\n",
    "1.  **Encode Labels:** Convert `no_pain`, `low_pain`, `high_pain` to `0`, `1`, `2`.\n",
    "2.  **Split Data:** Use `StratifiedShuffleSplit` to create a single 80/20 train/validation split. This ensures both sets have the same class proportions.\n",
    "3.  **Scale Features:** Use `StandardScaler`. We `fit` it *only* on the training data and `transform` all sets (train, val, and test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "code-preprocessing"
   },
   "outputs": [],
   "source": [
    "# --- 1. Encode Labels ---\n",
    "LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "le = LabelEncoder()\n",
    "le.fit(list(LABEL_MAPPING.keys()))\n",
    "y_train_full = le.transform(y_train_labels_str)\n",
    "N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "print(f\"Labels encoded. {N_CLASSES} classes: {LABEL_MAPPING}\")\n",
    "\n",
    "# --- 2. Create Validation Split ---\n",
    "# We use StratifiedShuffleSplit to maintain class balance\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "\n",
    "for train_idx, val_idx in sss.split(X_train_full, y_train_full):\n",
    "    X_train_split = X_train_full[train_idx]\n",
    "    y_train_split = y_train_full[train_idx]\n",
    "    X_val_split = X_train_full[val_idx]\n",
    "    y_val_split = y_train_full[val_idx]\n",
    "\n",
    "print(f\"Data split into Train and Validation sets:\")\n",
    "print(f\"  X_train_split: {X_train_split.shape}\")\n",
    "print(f\"  y_train_split: {y_train_split.shape}\")\n",
    "print(f\"  X_val_split:   {X_val_split.shape}\")\n",
    "print(f\"  y_val_split:   {y_val_split.shape}\")\n",
    "\n",
    "# --- 3. Scale Features (The \"No-Cheating\" Rule) ---\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# To fit the scaler, we must reshape our 3D data to 2D\n",
    "# (samples, timesteps, features) -> (samples * timesteps, features)\n",
    "# We must do this for all sets before transforming\n",
    "\n",
    "# Reshape train and FIT the scaler\n",
    "ns, ts, f = X_train_split.shape\n",
    "X_train_2d = X_train_split.reshape(ns * ts, f)\n",
    "print(f\"Fitting Scaler on X_train_2d shape: {X_train_2d.shape}\")\n",
    "scaler.fit(X_train_2d)\n",
    "\n",
    "# TRANSFORM train and reshape back to 3D\n",
    "X_train_scaled_2d = scaler.transform(X_train_2d)\n",
    "X_train_scaled = X_train_scaled_2d.reshape(ns, ts, f)\n",
    "\n",
    "# TRANSFORM val and reshape back to 3D\n",
    "ns_val, ts_val, f_val = X_val_split.shape\n",
    "X_val_2d = X_val_split.reshape(ns_val * ts_val, f_val)\n",
    "X_val_scaled_2d = scaler.transform(X_val_2d)\n",
    "X_val_scaled = X_val_scaled_2d.reshape(ns_val, ts_val, f_val)\n",
    "\n",
    "# TRANSFORM test and reshape back to 3D (for final submission)\n",
    "ns_test, ts_test, f_test = X_test.shape\n",
    "X_test_2d = X_test.reshape(ns_test * ts_test, f_test)\n",
    "X_test_scaled_2d = scaler.transform(X_test_2d)\n",
    "X_test_scaled = X_test_scaled_2d.reshape(ns_test, ts_test, f_test)\n",
    "\n",
    "print(\"Scaling complete.\")\n",
    "print(f\"  X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"  X_val_scaled:   {X_val_scaled.shape}\")\n",
    "print(f\"  X_test_scaled:  {X_test_scaled.shape}\")\n",
    "\n",
    "# Clean up 2D arrays\n",
    "del X_train_2d, X_val_2d, X_test_2d, X_train_scaled_2d, X_val_scaled_2d, X_test_scaled_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "markdown-dataloaders"
   },
   "source": [
    "## üöö **4. PyTorch DataLoaders**\n",
    "\n",
    "This section is identical to Lecture 4. We wrap our NumPy arrays in `TensorDataset` and `DataLoader` to efficiently feed batches to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "code-dataloaders"
   },
   "outputs": [],
   "source": [
    "# --- 1. Convert to Tensors ---\n",
    "# We use .float() for features and .long() for classification labels\n",
    "train_features = torch.from_numpy(X_train_scaled).float()\n",
    "train_targets = torch.from_numpy(y_train_split).long()\n",
    "\n",
    "val_features = torch.from_numpy(X_val_scaled).float()\n",
    "val_targets = torch.from_numpy(y_val_split).long()\n",
    "\n",
    "test_features = torch.from_numpy(X_test_scaled).float()\n",
    "\n",
    "# --- 2. Create TensorDatasets ---\n",
    "train_ds = TensorDataset(train_features, train_targets)\n",
    "val_ds = TensorDataset(val_features, val_targets)\n",
    "test_ds = TensorDataset(test_features) # Test set has no labels\n",
    "\n",
    "# --- 3. Define make_loader function (from Lecture 4) ---\n",
    "BATCH_SIZE = 128 # You can tune this\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    # Determine optimal number of worker processes for data loading\n",
    "    # Using more workers (e.g., 4 or 8) is great for a local RTX 3070\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = min(cpu_cores // 2, 8)\n",
    "    num_workers = max(2, num_workers)\n",
    "\n",
    "    print(f\"Creating DataLoader with batch_size={batch_size} and num_workers={num_workers}\")\n",
    "\n",
    "    # Create DataLoader with performance optimizations\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=4 if num_workers > 0 else None,  # Load batches ahead\n",
    "    )\n",
    "\n",
    "# --- 4. Create DataLoaders ---\n",
    "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "# Test loader will be made in the submission step\n",
    "\n",
    "# Check one batch\n",
    "for xb, yb in train_loader:\n",
    "    print(\"\\n--- Checking one batch --- \")\n",
    "    print(\"Features batch shape:\", xb.shape) # (BATCH_SIZE, 180, 35)\n",
    "    print(\"Labels batch shape:\", yb.shape)   # (BATCH_SIZE)\n",
    "    break # Stop after getting one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "markdown-model-training"
   },
   "source": [
    "## üõ†Ô∏è **5. Model & Training Engine (From Lecture 4)**\n",
    "\n",
    "This is the \"treasure map.\" These classes and functions are copied directly from Lecture 4, as they are perfect for this problem.\n",
    "\n",
    "-   `RecurrentClassifier`: Our flexible model (RNN, LSTM, GRU).\n",
    "-   `fit`: The main training loop with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "code-recurrent-summary"
   },
   "outputs": [],
   "source": [
    "def recurrent_summary(model, input_size):\n",
    "    \"\"\"\n",
    "    Custom summary function that emulates torchinfo's output while correctly\n",
    "    counting parameters for RNN/GRU/LSTM layers.\n",
    "    \"\"\"\n",
    "\n",
    "    output_shapes = {}\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1  # Replace batch dimension with -1\n",
    "\n",
    "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
    "                    shape2 = list(output[1][0].shape)\n",
    "                else:  # RNN/GRU case: h_n only\n",
    "                    shape2 = list(output[1].shape)\n",
    "                shape2[1] = -1\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "        return hook\n",
    "\n",
    "    try:\n",
    "        device_summary = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device_summary = torch.device(\"cpu\")\n",
    "\n",
    "    dummy_input = torch.randn(1, *input_size).to(device_summary)\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(dummy_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "code-recurrent-classifier"
   },
   "outputs": [],
   "source": [
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic RNN classifier (RNN, LSTM, GRU) from Lecture 4.\n",
    "    Uses the last hidden state for classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',        # 'RNN', 'LSTM', or 'GRU'\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "\n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "\n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "\n",
    "        # Dropout is only applied between layers (if num_layers > 1)\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        if self.bidirectional:\n",
    "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
    "        else:\n",
    "            classifier_input_size = hidden_size\n",
    "\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_length, input_size)\n",
    "        \"\"\"\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0]\n",
    "\n",
    "        if self.bidirectional:\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
    "        else:\n",
    "            hidden_to_classify = hidden[-1]\n",
    "\n",
    "        logits = self.classifier(hidden_to_classify)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "code-training-loop"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
    "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if param.numel() > 0:\n",
    "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
    "            if param.grad is not None:\n",
    "                if param.grad.numel() > 0:\n",
    "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
    "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    print(f\"Will train for {epochs} epochs with patience={patience} monitoring {evaluation_metric}\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
    "            )\n",
    "\n",
    "        if verbose > 0 and (epoch % verbose == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                  f\"Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n",
    "                  f\"Val: Loss={val_loss:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    if restore_best_weights and patience > 0:\n",
    "        print(f\"Restoring best model from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    if patience == 0:\n",
    "        print(\"Training complete. Saving final model.\")\n",
    "        torch.save(model.state_dict(), model_path.replace(\"_best_model.pt\", \"_final_model.pt\"))\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    \n",
    "    print(f\"--- Finished Training: {experiment_name} ---\")\n",
    "    return model, training_history, best_epoch if 'best_epoch' in locals() else epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "markdown-experiment-1"
   },
   "source": [
    "## üß™ **6. Experiment 1: Baseline Model (GRU)**\n",
    "\n",
    "Let's start with a strong baseline from Lecture 4: a 2-layer GRU. We'll train it on our split data and see what F1 score we get on the validation set. This is our starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "code-experiment-1"
   },
   "outputs": [],
   "source": [
    "# --- 1. Define Hyperparameters ---\n",
    "\n",
    "# Architecture\n",
    "INPUT_SIZE = N_FEATURES      # 35 features\n",
    "SEQ_LEN = N_TIMESTEPS        # 180 timesteps\n",
    "HIDDEN_SIZE = 128          # You can tune this\n",
    "HIDDEN_LAYERS = 2          # You can tune this\n",
    "MODEL_TYPE = 'GRU'         # 'RNN', 'LSTM', or 'GRU'\n",
    "BIDIRECTIONAL = False      # You can tune this\n",
    "\n",
    "# Training\n",
    "LEARNING_RATE = 1e-3       # You can tune this\n",
    "EPOCHS = 200               # Max epochs\n",
    "PATIENCE = 30              # Early stopping patience\n",
    "\n",
    "# Regularisation\n",
    "DROPOUT_RATE = 0.3         # You can tune this\n",
    "L2_LAMBDA = 0              # You can tune this\n",
    "\n",
    "# --- 2. Create Model, Loss, Optimizer ---\n",
    "print(f\"\\n--- Building {MODEL_TYPE} model (Bidirectional={BIDIRECTIONAL}) ---\")\n",
    "\n",
    "baseline_model = RecurrentClassifier(\n",
    "    input_size=INPUT_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=HIDDEN_LAYERS,\n",
    "    num_classes=N_CLASSES,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    rnn_type=MODEL_TYPE\n",
    ").to(device)\n",
    "\n",
    "# Compile the model (for PyTorch 2.0+ on your 3070)\n",
    "if torch.__version__[0] >= \"2\":\n",
    "    print(\"Compiling model with torch.compile() for a speed boost...\")\n",
    "    baseline_model = torch.compile(baseline_model)\n",
    "\n",
    "recurrent_summary(baseline_model, input_size=(SEQ_LEN, INPUT_SIZE))\n",
    "\n",
    "# Loss function (CrossEntropy handles softmax internally)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (AdamW is Adam with proper L2 weight decay)\n",
    "optimizer = torch.optim.AdamW(baseline_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "\n",
    "# Mixed precision scaler for faster training on RTX cards\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# TensorBoard Writer\n",
    "experiment_name = f\"{MODEL_TYPE}_H{HIDDEN_SIZE}_L{HIDDEN_LAYERS}_B{BIDIRECTIONAL}_D{DROPOUT_RATE}\"\n",
    "writer = SummaryWriter(f\"{logs_dir}/{experiment_name}\")\n",
    "\n",
    "# --- 3. Run Training ---\n",
    "baseline_model, history, best_epoch = fit(\n",
    "    model=baseline_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    writer=writer,\n",
    "    verbose=5, # Print update every 5 epochs\n",
    "    experiment_name=experiment_name,\n",
    "    patience=PATIENCE\n",
    ")\n",
    "\n",
    "# --- 4. Plot History ---\n",
    "print(\"\\n--- Plotting Training History ---\")\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot Loss\n",
    "ax1.plot(history['train_loss'], label='Training loss', alpha=0.8, color='blue', linestyle='--')\n",
    "ax1.plot(history['val_loss'], label='Validation loss', alpha=0.9, color='blue')\n",
    "ax1.set_title('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot F1 Score\n",
    "ax2.plot(history['train_f1'], label='Training F1', alpha=0.8, color='orange', linestyle='--')\n",
    "ax2.plot(history['val_f1'], label='Validation F1', alpha=0.9, color='orange')\n",
    "ax2.set_title('F1 Score')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "markdown-submission"
   },
   "source": [
    "## üì¨ **7. Create Submission**\n",
    "\n",
    "Now we follow the plan:\n",
    "1.  Create a **new, full training set** (train + val).\n",
    "2.  Re-scale the data and create a `full_train_loader`.\n",
    "3.  Instantiate a **fresh copy** of our best model (with the same hyperparameters).\n",
    "4.  Train it on the **full dataset** for the `best_epoch` we found in the previous step (this prevents overfitting).\n",
    "5.  Create a `test_loader`.\n",
    "6.  Generate predictions on the test set.\n",
    "7.  Save to `submission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "code-submission"
   },
   "outputs": [],
   "source": [
    "# --- 1. & 2. Create Full Training Set & Loader ---\n",
    "print(\"\\n--- Preparing full dataset for final training ---\")\n",
    "# We already have the full data: X_train_full, y_train_full\n",
    "\n",
    "# We must re-scale the *entire* training set\n",
    "scaler_final = StandardScaler()\n",
    "\n",
    "ns, ts, f = X_train_full.shape\n",
    "X_train_full_2d = X_train_full.reshape(ns * ts, f)\n",
    "\n",
    "print(f\"Fitting FINAL Scaler on X_train_full_2d shape: {X_train_full_2d.shape}\")\n",
    "scaler_final.fit(X_train_full_2d)\n",
    "\n",
    "# Transform full train set\n",
    "X_train_full_scaled_2d = scaler_final.transform(X_train_full_2d)\n",
    "X_train_full_scaled = X_train_full_scaled_2d.reshape(ns, ts, f)\n",
    "\n",
    "# Transform test set *using this new final scaler*\n",
    "# This is the correct way, as our model will be trained on this new scaling\n",
    "ns_test, ts_test, f_test = X_test.shape\n",
    "X_test_2d = X_test.reshape(ns_test * ts_test, f_test)\n",
    "X_test_final_scaled_2d = scaler_final.transform(X_test_2d)\n",
    "X_test_final_scaled = X_test_final_scaled_2d.reshape(ns_test, ts_test, f_test)\n",
    "\n",
    "print(\"Final scaling complete.\")\n",
    "\n",
    "# Create final DataLoaders\n",
    "full_train_features = torch.from_numpy(X_train_full_scaled).float()\n",
    "full_train_targets = torch.from_numpy(y_train_full).long()\n",
    "final_test_features = torch.from_numpy(X_test_final_scaled).float()\n",
    "\n",
    "full_train_ds = TensorDataset(full_train_features, full_train_targets)\n",
    "final_test_ds = TensorDataset(final_test_features)\n",
    "\n",
    "full_train_loader = make_loader(full_train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = make_loader(final_test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "# --- 3. Instantiate Fresh Model ---\n",
    "print(f\"\\n--- Building FINAL {MODEL_TYPE} model for submission ---\")\n",
    "final_model = RecurrentClassifier(\n",
    "    input_size=INPUT_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=HIDDEN_LAYERS,\n",
    "    num_classes=N_CLASSES,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    rnn_type=MODEL_TYPE\n",
    ").to(device)\n",
    "\n",
    "if torch.__version__[0] >= \"2\":\n",
    "    print(\"Compiling final model...\")\n",
    "    final_model = torch.compile(final_model)\n",
    "\n",
    "final_optimizer = torch.optim.AdamW(final_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "final_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# --- 4. Train on Full Dataset ---\n",
    "# We train for the 'best_epoch' found during validation.\n",
    "# This is a common strategy to get the best of both worlds:\n",
    "# - We use all data (train+val) for more robust training.\n",
    "# - We use the 'best_epoch' to prevent overfitting.\n",
    "print(f\"Training final model for {best_epoch} epochs on ALL data...\")\n",
    "\n",
    "final_model.train() # Set to train mode\n",
    "for epoch in range(1, best_epoch + 1):\n",
    "    # We only need the training loop now\n",
    "    train_loss, train_f1 = train_one_epoch(\n",
    "        final_model, full_train_loader, criterion, final_optimizer, final_scaler, device, L2_LAMBDA\n",
    "    )\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Final Training Epoch {epoch:3d}/{best_epoch} | Train: Loss={train_loss:.4f}, F1={train_f1:.4f}\")\n",
    "\n",
    "print(\"Final training complete.\")\n",
    "\n",
    "# --- 5. & 6. Generate Predictions ---\n",
    "print(\"\\n--- Generating predictions on test set ---\")\n",
    "final_model.eval() # Set to evaluation mode\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (inputs,) in test_loader: # Note the comma: test_loader only yields inputs\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = final_model(inputs)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_predictions.append(preds.cpu().numpy())\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "# Convert numeric predictions back to string labels\n",
    "predicted_labels = le.inverse_transform(all_predictions)\n",
    "\n",
    "print(f\"Generated {len(predicted_labels)} predictions.\")\n",
    "\n",
    "# --- 7. Save Submission File ---\n",
    "submission_df = pd.read_csv(SUBMISSION_PATH)\n",
    "\n",
    "# Make sure the sample_index in submission matches our predictions\n",
    "# We must load the original test set to get the unique sample_index order\n",
    "# that our `reshape_data` function used.\n",
    "X_test_long = pd.read_csv(X_TEST_PATH)\n",
    "test_sample_indices = X_test_long['sample_index'].unique()\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_index': test_sample_indices,\n",
    "    'target': predicted_labels\n",
    "})\n",
    "\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully saved submission.csv!\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# Clean up memory\n",
    "del final_model, full_train_loader, test_loader, full_train_features, final_test_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
