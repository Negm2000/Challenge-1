{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5222aca",
   "metadata": {
    "id": "markdown-notebook-intro"
   },
   "source": [
    "# **Kaggle Challenge: Pirate Pain Dataset üè¥‚Äç‚ò†Ô∏è**\n",
    "\n",
    "This notebook is a modified version of the Lecture 4 (Timeseries Classification) code, adapted for the Kaggle challenge.\n",
    "\n",
    "**Local Setup:**\n",
    "1.  Ensure you have a Conda environment with PyTorch (GPU), `pandas`, `sklearn`, `jupyterlab`, `ray[tune]`, and `optuna`.\n",
    "2.  Place the Kaggle CSVs (`pirate_pain_train.csv`, `pirate_pain_train_labels.csv`, `pirate_pain_test.csv`) in a folder named `data/` in the same directory as this notebook.\n",
    "3.  To run TensorBoard, open a separate terminal, `conda activate` your environment, `cd` to this folder, and run: `tensorboard --logdir=./tensorboard`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e25d9",
   "metadata": {
    "id": "markdown-libraries"
   },
   "source": [
    "## ‚öôÔ∏è **1. Setup & Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f67f7de3",
   "metadata": {
    "id": "code-libraries"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GPU (RTX 3070, here we come!) ---\n",
      "PyTorch version: 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import copy\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# --- PyTorch Imports ---\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Sklearn Imports ---\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Ray[tune] & Optuna Imports ---\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from functools import partial\n",
    "\n",
    "# --- Setup Directories & Device ---\n",
    "logs_dir = \"tensorboard\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n--- Using GPU (RTX 3070, here we come!) ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\n--- Using CPU ---\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0184001",
   "metadata": {
    "id": "markdown-load-reshape"
   },
   "source": [
    "## üîÑ **2. Data Loading & Reshaping**\n",
    "\n",
    "This is the most critical new step. The data is in a \"long\" format (one row per timestep), where each `sample_index` is one complete time series. We must:\n",
    "1.  Define the features we want to use.\n",
    "2.  Pivot the data to get a 3D tensor of shape `(num_samples, num_timesteps, num_features)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "83c7115f",
   "metadata": {
    "id": "code-load-reshape"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 35 features: ['joint_00', 'joint_01', 'joint_02']... to ['pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
      "Loading and reshaping training data...\n",
      "Loading and reshaping test data...\n",
      "X_train_full shape: (661, 160, 35)\n",
      "y_train_labels_str shape: (661,)\n",
      "X_test shape: (1324, 160, 35)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define File Paths and Features ---\n",
    "DATA_DIR = \"data\"\n",
    "X_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train.csv\")\n",
    "Y_TRAIN_PATH = os.path.join(DATA_DIR, \"pirate_pain_train_labels.csv\")\n",
    "X_TEST_PATH = os.path.join(DATA_DIR, \"pirate_pain_test.csv\")\n",
    "SUBMISSION_PATH = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "# Define our time-series features\n",
    "# We'll ignore static features (n_legs, etc.) for our baseline model\n",
    "JOINT_FEATURES = [f\"joint_{i:02d}\" for i in range(31)]\n",
    "PAIN_FEATURES = [f\"pain_survey_{i}\" for i in range(1, 5)]\n",
    "FEATURES = JOINT_FEATURES + PAIN_FEATURES\n",
    "\n",
    "N_FEATURES = len(FEATURES)\n",
    "N_TIMESTEPS = 160 # Fixed from our earlier debugging\n",
    "\n",
    "print(f\"Using {N_FEATURES} features: {FEATURES[:3]}... to {FEATURES[-3:]}\")\n",
    "\n",
    "# --- 2. Create the Reshaping Function ---\n",
    "def reshape_data(df, features_list, n_timesteps):\n",
    "    \"\"\"\n",
    "    Pivots the long-format dataframe into a 3D NumPy array.\n",
    "    Shape: (n_samples, n_timesteps, n_features)\n",
    "    \"\"\"\n",
    "    df_pivot = df.pivot(index='sample_index', columns='time', values=features_list)\n",
    "    data_2d = df_pivot.values\n",
    "    n_samples = data_2d.shape[0]\n",
    "    data_3d = data_2d.reshape(n_samples, len(features_list), n_timesteps)\n",
    "    return data_3d.transpose(0, 2, 1)\n",
    "\n",
    "def create_sliding_windows(X_3d, y=None, window_size=100, stride=20):\n",
    "    \"\"\"\n",
    "    Takes 3D data (n_samples, n_timesteps, n_features)\n",
    "    and creates overlapping windows.\n",
    "    \n",
    "    Returns:\n",
    "    - new_X: (n_windows, window_size, n_features)\n",
    "    - new_y (if y is provided): (n_windows,)\n",
    "    - window_indices: (n_windows,) array tracking which original sample\n",
    "                      (e.g., 0, 1, 2...) each window came from.\n",
    "    \"\"\"\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    # This new array tracks which original sample each window came from.\n",
    "    window_indices = [] \n",
    "    \n",
    "    n_samples, n_timesteps, n_features = X_3d.shape\n",
    "    \n",
    "    # Iterate over each original sample\n",
    "    for i in range(n_samples):\n",
    "        sample = X_3d[i] # Shape (160, 35)\n",
    "        \n",
    "        # Slide a window over this sample\n",
    "        idx = 0\n",
    "        while (idx + window_size) <= n_timesteps:\n",
    "            window = sample[idx : idx + window_size]\n",
    "            new_X.append(window)\n",
    "            window_indices.append(i) # Track the original sample index (0, 1, 2...)\n",
    "            \n",
    "            if y is not None:\n",
    "                new_y.append(y[i]) # The label is the same for all windows\n",
    "                \n",
    "            idx += stride\n",
    "            \n",
    "    if y is not None:\n",
    "        # Return new X, new y, and the index mapping\n",
    "        return np.array(new_X), np.array(new_y), np.array(window_indices)\n",
    "    else:\n",
    "        # Return new X and the index mapping\n",
    "        return np.array(new_X), np.array(window_indices)\n",
    "\n",
    "# --- 3. Load and Reshape Data ---\n",
    "print(\"Loading and reshaping training data...\")\n",
    "X_train_long = pd.read_csv(X_TRAIN_PATH)\n",
    "X_train_full = reshape_data(X_train_long[X_train_long['sample_index'].isin(X_train_long['sample_index'].unique())], FEATURES, N_TIMESTEPS)\n",
    "\n",
    "print(\"Loading and reshaping test data...\")\n",
    "X_test_long = pd.read_csv(X_TEST_PATH)\n",
    "X_test = reshape_data(X_test_long, FEATURES, N_TIMESTEPS)\n",
    "\n",
    "# Load labels\n",
    "y_train_df = pd.read_csv(Y_TRAIN_PATH)\n",
    "y_train_full_df = y_train_df.sort_values(by='sample_index')\n",
    "y_train_labels_str = y_train_full_df['label'].values # Fixed from our debugging\n",
    "\n",
    "print(f\"X_train_full shape: {X_train_full.shape}\")\n",
    "print(f\"y_train_labels_str shape: {y_train_labels_str.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "del X_train_long, X_test_long, y_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d77fc",
   "metadata": {
    "id": "markdown-preprocessing"
   },
   "source": [
    "## üöß **3. Preprocessing: Split & Scale**\n",
    "\n",
    "1.  **Encode Labels:** Convert `no_pain`, `low_pain`, `high_pain` to `0`, `1`, `2`.\n",
    "2.  **Split Data:** Use `StratifiedShuffleSplit` to create a single 80/20 train/validation split. This ensures both sets have the same class proportions.\n",
    "3.  **Scale Features:** Use `StandardScaler`. We `fit` it *only* on the training data and `transform` all sets (train, val, and test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2355a626",
   "metadata": {
    "id": "code-preprocessing"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels encoded. 3 classes: {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
      "--- Applying sliding window augmentation ---\n",
      "Original X shape: (661, 160, 35)\n",
      "Windowed X shape: (3305, 80, 35)\n",
      "Original y shape: (661,)\n",
      "Windowed y shape: (3305,)\n",
      "\n",
      "--- Splitting windowed data ---\n",
      "  X_train_split: (2644, 80, 35)\n",
      "  y_train_split: (2644,)\n",
      "  X_val_split:   (661, 80, 35)\n",
      "  y_val_split:   (661,)\n",
      "Fitting Scaler on X_train_2d shape: (211520, 35)\n",
      "Scaling complete.\n",
      "  X_train_scaled: (2644, 80, 35)\n",
      "  X_val_scaled:   (661, 80, 35)\n",
      "  X_test_scaled:  (1324, 160, 35)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Encode Labels ---\n",
    "LABEL_MAPPING = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "le = LabelEncoder()\n",
    "le.fit(list(LABEL_MAPPING.keys()))\n",
    "y_train_full = le.transform(y_train_labels_str)\n",
    "N_CLASSES = len(LABEL_MAPPING)\n",
    "\n",
    "print(f\"Labels encoded. {N_CLASSES} classes: {LABEL_MAPPING}\")\n",
    "\n",
    "# 1. DEFINE YOUR WINDOW PARAMETERS\n",
    "NEW_WINDOW_SIZE = 80 # Example: 80 timesteps\n",
    "NEW_STRIDE = 20       # Example: 20 timesteps\n",
    "\n",
    "print(\"--- Applying sliding window augmentation ---\")\n",
    "# 3. APPLY THE NEW WINDOWING FUNCTION\n",
    "X_train_windowed, y_train_windowed, _ = create_sliding_windows(\n",
    "    X_train_full, \n",
    "    y_train_full, \n",
    "    window_size=NEW_WINDOW_SIZE, \n",
    "    stride=NEW_STRIDE\n",
    ")\n",
    "\n",
    "print(f\"Original X shape: {X_train_full.shape}\")\n",
    "print(f\"Windowed X shape: {X_train_windowed.shape}\")\n",
    "print(f\"Original y shape: {y_train_full.shape}\")\n",
    "print(f\"Windowed y shape: {y_train_windowed.shape}\")\n",
    "\n",
    "# (You would also need to apply this to X_test for submission)\n",
    "# (But NOT to y_train_full for the split)\n",
    "\n",
    "# 4. USE THE *WINDOWED* DATA FOR YOUR SPLIT\n",
    "print(\"\\n--- Splitting windowed data ---\")\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# IMPORTANT: You split the *new*, *larger* X and y arrays\n",
    "for train_idx, val_idx in sss.split(X_train_windowed, y_train_windowed):\n",
    "    X_train_split = X_train_windowed[train_idx]\n",
    "    y_train_split = y_train_windowed[train_idx]\n",
    "    X_val_split = X_train_windowed[val_idx]\n",
    "    y_val_split = y_train_windowed[val_idx]\n",
    "\n",
    "print(f\"  X_train_split: {X_train_split.shape}\")\n",
    "print(f\"  y_train_split: {y_train_split.shape}\")\n",
    "print(f\"  X_val_split:   {X_val_split.shape}\")\n",
    "print(f\"  y_val_split:   {y_val_split.shape}\")\n",
    "# --- 3. Scale Features (The \"No-Cheating\" Rule) ---\n",
    "scaler = StandardScaler()\n",
    "ns, ts, f = X_train_split.shape\n",
    "X_train_2d = X_train_split.reshape(ns * ts, f)\n",
    "print(f\"Fitting Scaler on X_train_2d shape: {X_train_2d.shape}\")\n",
    "scaler.fit(X_train_2d)\n",
    "\n",
    "X_train_scaled_2d = scaler.transform(X_train_2d)\n",
    "X_train_scaled = X_train_scaled_2d.reshape(ns, ts, f)\n",
    "\n",
    "ns_val, ts_val, f_val = X_val_split.shape\n",
    "X_val_2d = X_val_split.reshape(ns_val * ts_val, f_val)\n",
    "X_val_scaled_2d = scaler.transform(X_val_2d)\n",
    "X_val_scaled = X_val_scaled_2d.reshape(ns_val, ts_val, f_val)\n",
    "\n",
    "ns_test, ts_test, f_test = X_test.shape\n",
    "X_test_2d = X_test.reshape(ns_test * ts_test, f_test)\n",
    "X_test_scaled_2d = scaler.transform(X_test_2d)\n",
    "X_test_scaled = X_test_scaled_2d.reshape(ns_test, ts_test, f_test)\n",
    "\n",
    "print(\"Scaling complete.\")\n",
    "print(f\"  X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"  X_val_scaled:   {X_val_scaled.shape}\")\n",
    "print(f\"  X_test_scaled:  {X_test_scaled.shape}\")\n",
    "\n",
    "del X_train_2d, X_val_2d, X_test_2d, X_train_scaled_2d, X_val_scaled_2d, X_test_scaled_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c883b33",
   "metadata": {
    "id": "markdown-dataloaders"
   },
   "source": [
    "## üöö **4. PyTorch DataLoaders**\n",
    "\n",
    "This section is identical to Lecture 4. We wrap our NumPy arrays in `TensorDataset` and `DataLoader` to efficiently feed batches to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b8702f5a",
   "metadata": {
    "id": "code-dataloaders"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders will be created inside the tuning loop.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Convert to Tensors ---\n",
    "train_features = torch.from_numpy(X_train_scaled).float()\n",
    "train_targets = torch.from_numpy(y_train_split).long()\n",
    "\n",
    "val_features = torch.from_numpy(X_val_scaled).float()\n",
    "val_targets = torch.from_numpy(y_val_split).long()\n",
    "\n",
    "test_features = torch.from_numpy(X_test_scaled).float()\n",
    "\n",
    "# --- 2. Create TensorDatasets ---\n",
    "train_ds = TensorDataset(train_features, train_targets)\n",
    "val_ds = TensorDataset(val_features, val_targets)\n",
    "test_ds = TensorDataset(test_features) # Test set has no labels\n",
    "\n",
    "# --- 3. Define make_loader function (from Lecture 4) ---\n",
    "BATCH_SIZE = 128 # This will be our default, but Optuna can tune it\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    # Set num_workers=0 for Windows-friendly loading (from our debugging)\n",
    "    num_workers = 0 \n",
    "    \n",
    "    # Create DataLoader with performance optimizations\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=int(batch_size), # Ensure batch_size is an int for the DataLoader\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=None,\n",
    "    )\n",
    "\n",
    "# --- 4. Create DataLoaders ---\n",
    "# We will create these *inside* the objective function now,\n",
    "# as the batch size is a hyperparameter we want to tune.\n",
    "print(\"DataLoaders will be created inside the tuning loop.\")\n",
    "del X_train_scaled, X_val_scaled, test_features, train_features, val_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16263592",
   "metadata": {
    "id": "markdown-model-training"
   },
   "source": [
    "## üõ†Ô∏è **5. Model & Training Engine (From Lecture 4)**\n",
    "\n",
    "These are the core components from Lecture 4, slightly modified for Ray Tune.\n",
    "\n",
    "-   `RecurrentClassifier`: Our flexible model (RNN, LSTM, GRU).\n",
    "-   `fit`: The main training loop. **This is now modified** to accept a `config` dict and report results back to `ray.tune` instead of running for a fixed number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "06613e58",
   "metadata": {
    "id": "code-recurrent-summary"
   },
   "outputs": [],
   "source": [
    "def recurrent_summary(model, input_size):\n",
    "    \"\"\"\n",
    "    Custom summary function that emulates torchinfo's output while correctly\n",
    "    counting parameters for RNN/GRU/LSTM layers.\n",
    "    \"\"\"\n",
    "\n",
    "    output_shapes = {}\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1  # Replace batch dimension with -1\n",
    "\n",
    "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
    "                    shape2 = list(output[1][0].shape)\n",
    "                else:  # RNN/GRU case: h_n only\n",
    "                    shape2 = list(output[1].shape)\n",
    "                shape2[1] = -1\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "        return hook\n",
    "\n",
    "    try:\n",
    "        device_summary = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device_summary = torch.device(\"cpu\")\n",
    "\n",
    "    dummy_input = torch.randn(1, *input_size).to(device_summary)\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM)):\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(dummy_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ac4fc09",
   "metadata": {
    "id": "code-recurrent-classifier"
   },
   "outputs": [],
   "source": [
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic RNN classifier (RNN, LSTM, GRU) from Lecture 4.\n",
    "    Uses the last hidden state for classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',        # 'RNN', 'LSTM', or 'GRU'\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "\n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "\n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "\n",
    "        # Dropout is only applied between layers (if num_layers > 1)\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,       # Input shape: (batch, seq_len, features)\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        if self.bidirectional:\n",
    "            classifier_input_size = hidden_size * 2 # Concat fwd + bwd\n",
    "        else:\n",
    "            classifier_input_size = hidden_size\n",
    "\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_length, input_size)\n",
    "        \"\"\"\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0]\n",
    "\n",
    "        if self.bidirectional:\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
    "        else:\n",
    "            hidden_to_classify = hidden[-1]\n",
    "\n",
    "        logits = self.classifier(hidden_to_classify)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17b67a46",
   "metadata": {
    "id": "code-training-loop-MODIFIED"
   },
   "outputs": [],
   "source": [
    "# This cell is MODIFIED to work with Ray Tune.\n",
    "# The fit function is now part of the 'objective_function'\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
    "    # This function is not strictly needed for tune, but good to keep\n",
    "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "\n",
    "# The 'fit' function is now part of the objective function below\n",
    "# We create a NEW objective_function for Ray Tune\n",
    "\n",
    "def objective_function(config, train_ds, val_ds):\n",
    "    \"\"\"\n",
    "    This is the main function that Ray Tune will call for each trial.\n",
    "    'config' is a dictionary of hyperparameters from Optuna.\n",
    "    'train_ds' and 'val_ds' are our TensorDatasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Create DataLoaders with the tuned batch size ---\n",
    "    # We create them here because batch_size is a hyperparameter\n",
    "    train_loader = make_loader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "    val_loader = make_loader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=False)\n",
    "    \n",
    "    # --- 2. Create Model --- \n",
    "    model = RecurrentClassifier(\n",
    "        input_size=N_FEATURES,\n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        num_classes=N_CLASSES,\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        bidirectional=config[\"bidirectional\"],\n",
    "        rnn_type=config[\"rnn_type\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    if torch.__version__[0] >= \"2\":\n",
    "        model = torch.compile(model)\n",
    "    \n",
    "    # --- 3. Create Optimizer, Loss, Scaler ---\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"l2_lambda\"])\n",
    "    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "    # --- 4. The Training Loop (adapted from fit) ---\n",
    "    # We loop for a fixed number of epochs (e.g., 200) and let Ray's\n",
    "    # ASHA scheduler handle early stopping of bad trials.\n",
    "    EPOCHS = 200 \n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, 0, config[\"l2_lambda\"]\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # --- Send Results to Ray Tune --- \n",
    "        # This is the most important part.\n",
    "        # Ray Tune will use this 'val_f1' score to make decisions.\n",
    "        # FIX: Pass metrics as a single dictionary\n",
    "        tune.report({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_f1\": val_f1\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a903e3",
   "metadata": {
    "id": "markdown-optuna-search"
   },
   "source": [
    "## üß™ **6. Hyperparameter Search with Ray Tune & Optuna**\n",
    "\n",
    "This cell replaces our old manual \"Experiment 1\".\n",
    "\n",
    "1.  **`search_space`**: We define the hyperparameters Optuna is allowed to search.\n",
    "2.  **`OptunaSearch`**: We tell Ray Tune to use Optuna's smart search algorithm.\n",
    "3.  **`ASHAScheduler`**: We tell Ray Tune to automatically stop trials that are not performing well (to save time).\n",
    "4.  **`tune.run`**: This starts the parallel search. It will run `NUM_SAMPLES` (e.g., 20) different experiments, using `gpus_per_trial` to run them on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "41d8c96b",
   "metadata": {
    "id": "code-optuna-search"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-08 15:00:39</td></tr>\n",
       "<tr><td>Running for: </td><td>00:11:38.72        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.1/13.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=20<br>Bracket: Iter 80.000: 0.9850307304620033 | Iter 40.000: 0.9791739054832561 | Iter 20.000: 0.9539065649661369<br>Logical resource usage: 4.0/16 CPUs, 0.25/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th>bidirectional  </th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  l2_lambda</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th>rnn_type  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_f1</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_function_f14856cc</td><td>TERMINATED</td><td>127.0.0.1:30156</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.197461</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.60861e-05</td><td style=\"text-align: right;\">6.34824e-05</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         67.5903</td><td style=\"text-align: right;\"> 0.176599   </td><td style=\"text-align: right;\">  0.949192</td><td style=\"text-align: right;\"> 0.127393 </td></tr>\n",
       "<tr><td>objective_function_f169db90</td><td>TERMINATED</td><td>127.0.0.1:34416</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.10323 </td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.08283e-05</td><td style=\"text-align: right;\">0.00031591 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        216.986 </td><td style=\"text-align: right;\"> 0.0415726  </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.0783661</td></tr>\n",
       "<tr><td>objective_function_b1d11c86</td><td>TERMINATED</td><td>127.0.0.1:48160</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.124886</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">9.07201e-07</td><td style=\"text-align: right;\">0.00104391 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        385.408 </td><td style=\"text-align: right;\"> 0.00531774 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.0705994</td></tr>\n",
       "<tr><td>objective_function_2d4c9246</td><td>TERMINATED</td><td>127.0.0.1:49928</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.434178</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">3.44393e-06</td><td style=\"text-align: right;\">4.25182e-05</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         57.0431</td><td style=\"text-align: right;\"> 0.270809   </td><td style=\"text-align: right;\">  0.890112</td><td style=\"text-align: right;\"> 0.248498 </td></tr>\n",
       "<tr><td>objective_function_c5b176ba</td><td>TERMINATED</td><td>127.0.0.1:3192 </td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.119342</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000797479</td><td style=\"text-align: right;\">1.43133e-05</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         63.8124</td><td style=\"text-align: right;\"> 2.99978    </td><td style=\"text-align: right;\">  0.739021</td><td style=\"text-align: right;\"> 0.514179 </td></tr>\n",
       "<tr><td>objective_function_db57d655</td><td>TERMINATED</td><td>127.0.0.1:35972</td><td style=\"text-align: right;\">          64</td><td>True           </td><td style=\"text-align: right;\">      0.177049</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000610535</td><td style=\"text-align: right;\">0.000562012</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        139.939 </td><td style=\"text-align: right;\"> 0.223743   </td><td style=\"text-align: right;\">  0.955876</td><td style=\"text-align: right;\"> 0.140335 </td></tr>\n",
       "<tr><td>objective_function_d4a14296</td><td>TERMINATED</td><td>127.0.0.1:20440</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.157407</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">8.95704e-07</td><td style=\"text-align: right;\">0.0020676  </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         64.5558</td><td style=\"text-align: right;\"> 0.0465825  </td><td style=\"text-align: right;\">  0.985502</td><td style=\"text-align: right;\"> 0.101211 </td></tr>\n",
       "<tr><td>objective_function_98c3f61d</td><td>TERMINATED</td><td>127.0.0.1:29668</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.308104</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.47736e-05</td><td style=\"text-align: right;\">2.41254e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         23.2886</td><td style=\"text-align: right;\"> 0.515759   </td><td style=\"text-align: right;\">  0.78475 </td><td style=\"text-align: right;\"> 0.458598 </td></tr>\n",
       "<tr><td>objective_function_71bdab5b</td><td>TERMINATED</td><td>127.0.0.1:47852</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.291669</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.27757e-07</td><td style=\"text-align: right;\">9.0598e-05 </td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         13.44  </td><td style=\"text-align: right;\"> 0.392044   </td><td style=\"text-align: right;\">  0.821992</td><td style=\"text-align: right;\"> 0.364737 </td></tr>\n",
       "<tr><td>objective_function_c65380e2</td><td>TERMINATED</td><td>127.0.0.1:20360</td><td style=\"text-align: right;\">         128</td><td>False          </td><td style=\"text-align: right;\">      0.463619</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.89574e-05</td><td style=\"text-align: right;\">0.00650139 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         24.6971</td><td style=\"text-align: right;\"> 0.237155   </td><td style=\"text-align: right;\">  0.957687</td><td style=\"text-align: right;\"> 0.122496 </td></tr>\n",
       "<tr><td>objective_function_baf155df</td><td>TERMINATED</td><td>127.0.0.1:32748</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.446561</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">7.4871e-06 </td><td style=\"text-align: right;\">0.000877883</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         98.7745</td><td style=\"text-align: right;\"> 0.00758835 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.070277 </td></tr>\n",
       "<tr><td>objective_function_a651b302</td><td>TERMINATED</td><td>127.0.0.1:48084</td><td style=\"text-align: right;\">         256</td><td>True           </td><td style=\"text-align: right;\">      0.187102</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">2.30352e-05</td><td style=\"text-align: right;\">1.04795e-05</td><td style=\"text-align: right;\">           3</td><td>GRU       </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         43.4882</td><td style=\"text-align: right;\"> 0.657082   </td><td style=\"text-align: right;\">  0.749616</td><td style=\"text-align: right;\"> 0.518554 </td></tr>\n",
       "<tr><td>objective_function_8ff22b46</td><td>TERMINATED</td><td>127.0.0.1:48492</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.112966</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.80208e-06</td><td style=\"text-align: right;\">1.47434e-05</td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         24.8915</td><td style=\"text-align: right;\"> 0.555232   </td><td style=\"text-align: right;\">  0.72304 </td><td style=\"text-align: right;\"> 0.537786 </td></tr>\n",
       "<tr><td>objective_function_7d2ca9f8</td><td>TERMINATED</td><td>127.0.0.1:30428</td><td style=\"text-align: right;\">         128</td><td>True           </td><td style=\"text-align: right;\">      0.389649</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">0.000510173</td><td style=\"text-align: right;\">0.00123853 </td><td style=\"text-align: right;\">           2</td><td>LSTM      </td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        147.852 </td><td style=\"text-align: right;\"> 0.0900141  </td><td style=\"text-align: right;\">  0.991006</td><td style=\"text-align: right;\"> 0.0591335</td></tr>\n",
       "<tr><td>objective_function_396862f4</td><td>TERMINATED</td><td>127.0.0.1:30080</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.597128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">8.36585e-05</td><td style=\"text-align: right;\">0.00674709 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         37.5628</td><td style=\"text-align: right;\"> 0.156592   </td><td style=\"text-align: right;\">  0.989775</td><td style=\"text-align: right;\"> 0.0710714</td></tr>\n",
       "<tr><td>objective_function_c7bfbdf8</td><td>TERMINATED</td><td>127.0.0.1:50084</td><td style=\"text-align: right;\">         256</td><td>False          </td><td style=\"text-align: right;\">      0.584311</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">0.000119852</td><td style=\"text-align: right;\">0.00906598 </td><td style=\"text-align: right;\">           3</td><td>LSTM      </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         12.0381</td><td style=\"text-align: right;\"> 0.447856   </td><td style=\"text-align: right;\">  0.930816</td><td style=\"text-align: right;\"> 0.261891 </td></tr>\n",
       "<tr><td>objective_function_0b7c8179</td><td>TERMINATED</td><td>127.0.0.1:27932</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.587118</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">3.0488e-07 </td><td style=\"text-align: right;\">0.000955445</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        107.04  </td><td style=\"text-align: right;\"> 0.000317269</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.0422826</td></tr>\n",
       "<tr><td>objective_function_0c832fa6</td><td>TERMINATED</td><td>127.0.0.1:2980 </td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.421138</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">3.69831e-07</td><td style=\"text-align: right;\">0.000895693</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        155.268 </td><td style=\"text-align: right;\"> 0.00121421 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.124685 </td></tr>\n",
       "<tr><td>objective_function_3d7f0315</td><td>TERMINATED</td><td>127.0.0.1:30132</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.414001</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">3.20513e-07</td><td style=\"text-align: right;\">0.000945588</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        177.026 </td><td style=\"text-align: right;\"> 0.000854872</td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.0718001</td></tr>\n",
       "<tr><td>objective_function_e3eb7a36</td><td>TERMINATED</td><td>127.0.0.1:29324</td><td style=\"text-align: right;\">          64</td><td>False          </td><td style=\"text-align: right;\">      0.401517</td><td style=\"text-align: right;\">          384</td><td style=\"text-align: right;\">3.8432e-07 </td><td style=\"text-align: right;\">0.000939146</td><td style=\"text-align: right;\">           2</td><td>GRU       </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        143.116 </td><td style=\"text-align: right;\"> 0.00100502 </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\"> 0.100762 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-08 14:49:21,423 E 48960 49820] (gcs_server.exe) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-08 14:49:26,071 E 49188 23516] (raylet.exe) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "2025-11-08 15:00:39,428\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'c:/Users/Karim Negm/Documents/AN2DL/Challenge 1/ray_results/pirate_pain_optuna_search' in 0.0352s.\n",
      "2025-11-08 15:00:39,451\tINFO tune.py:1041 -- Total run time: 698.77 seconds (698.67 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Search Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define the Search Space for Optuna ---\n",
    "search_space = {\n",
    "    \"rnn_type\": tune.choice(['GRU', 'LSTM']),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),           # Widen the learning rate\n",
    "    \"batch_size\": tune.choice([64, 128, 256]),  \n",
    "    \"hidden_size\": tune.choice([128, 256, 384]),# Let's try bigger models\n",
    "    \"num_layers\": tune.choice([2, 3]),       # Let's try deeper models\n",
    "    \"dropout_rate\": tune.uniform(0.1, 0.6),     # Widen the dropout range\n",
    "    \"bidirectional\": tune.choice([True, False]),\n",
    "    \"l2_lambda\": tune.loguniform(1e-7, 1e-3)      # Widen the L2 range\n",
    "}\n",
    "\n",
    "# --- 2. Define the Optimizer (Optuna) and Scheduler (ASHA) ---\n",
    "optuna_search = OptunaSearch(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_f1\",\n",
    "    mode=\"max\",\n",
    "    grace_period=20,  # Min epochs a trial must run\n",
    "    reduction_factor=2  # How aggressively to stop trials\n",
    ")\n",
    "\n",
    "# --- 3. Initialize Ray (WITH THE BIG HAMMER FIX) ---\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "# --- FIX 2 (The \"Big Hammer\"): Set the Environment Variable ---\n",
    "# This forces Ray to use this short path for *all* its temp files.\n",
    "ray_logs_path = os.path.abspath(\"./ray_results\")\n",
    "os.makedirs(ray_logs_path, exist_ok=True)\n",
    "os.environ[\"RAY_TEMP_DIR\"] = ray_logs_path\n",
    "# --- END FIX ---\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=16, \n",
    "    num_gpus=1, \n",
    "    ignore_reinit_error=True\n",
    ")\n",
    "\n",
    "def short_trial_name(trial):\n",
    "    \"\"\"Creates a short, unique name for each trial folder.\"\"\"\n",
    "    return f\"{trial.trainable_name}_{trial.trial_id}\"\n",
    "\n",
    "\n",
    "# --- 4. Run the Tuner ---\n",
    "print(\"Starting hyperparameter search (1 trial at a time)...\")\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(objective_function, train_ds=train_ds, val_ds=val_ds),\n",
    "    \n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": 0.25}, \n",
    "    \n",
    "    config=search_space,\n",
    "    num_samples=20, # Number of different HPO trials to run\n",
    "    search_alg=optuna_search,\n",
    "    scheduler=scheduler,\n",
    "    name=\"pirate_pain_optuna_search\",\n",
    "\n",
    "    storage_path=ray_logs_path,\n",
    "    \n",
    "    trial_dirname_creator=short_trial_name,\n",
    "    \n",
    "    log_to_file=True,\n",
    "    verbose=1 # 0 = quiet, 1 = table, 2 = detailed\n",
    ")\n",
    "\n",
    "print(\"\\n--- Search Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e2f34508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting best trial from analysis...\n",
      "Best validation F1 score: 0.9895\n",
      "Best hyperparameters found:\n",
      "{'rnn_type': 'GRU', 'lr': 0.000945587937050466, 'batch_size': 64, 'hidden_size': 384, 'num_layers': 2, 'dropout_rate': 0.4140005708856067, 'bidirectional': False, 'l2_lambda': 3.205126834524651e-07}\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Get Best Results (FIX 3) ---\n",
    "# We must *explicitly* tell the analysis object what metric/mode to use\n",
    "print(\"Getting best trial from analysis...\")\n",
    "best_trial = analysis.get_best_trial(metric=\"val_f1\", mode=\"max\", scope=\"all\")\n",
    "if best_trial:\n",
    "    best_config = best_trial.config\n",
    "    best_val_f1 = best_trial.last_result[\"val_f1\"]\n",
    "    \n",
    "    print(f\"Best validation F1 score: {best_val_f1:.4f}\")\n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(best_config)\n",
    "else:\n",
    "    print(\"ERROR: No trials completed successfully. Check the 'ray_results' folder for logs.\")\n",
    "    best_config = None # Handle the case where all trials failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b564a8e",
   "metadata": {
    "id": "markdown-submission-config"
   },
   "source": [
    "## üèÜ **7. Final Model Configuration**\n",
    "\n",
    "After the search is complete, **copy the output from the cell above** and paste it here. This cell now holds our winning configuration, ready for the final training and submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab5802d6",
   "metadata": {
    "id": "code-submission-config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèÜ Final Configuration Set --- \n",
      "Best Val F1 from search: 0.9895\n",
      "{'rnn_type': 'GRU', 'lr': 0.000945587937050466, 'batch_size': 64, 'hidden_size': 384, 'num_layers': 2, 'dropout_rate': 0.4140005708856067, 'bidirectional': False, 'l2_lambda': 3.205126834524651e-07}\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# --- üèÜ FINAL MODEL CONFIGURATION üèÜ ---\n",
    "# ===================================================================\n",
    "# --- 1. Get Best Config from Analysis --- \n",
    "# We can get this automatically from the 'analysis' object\n",
    "FINAL_CONFIG = best_config\n",
    "FINAL_BEST_VAL_F1 = best_val_f1\n",
    "\n",
    "# We need to get the 'best_epoch' for the final training.\n",
    "# We'll re-run the 'fit' function from Lecture 4 (which we must add back)\n",
    "# on the best config to find the optimal number of epochs.\n",
    "\n",
    "print(\"--- üèÜ Final Configuration Set --- \")\n",
    "print(f\"Best Val F1 from search: {FINAL_BEST_VAL_F1:.4f}\")\n",
    "print(FINAL_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03626778",
   "metadata": {},
   "source": [
    "### **Re-run to find Best Epoch**\n",
    "\n",
    "The search trained models for 200 epochs and reported the best score *at any point*. Now we need to re-run the best model *with early stopping* to find the *exact* best epoch for our final submission training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5cfc58f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finding best epoch number for the winning model ---\n",
      "--- Starting Training: final_check ---\n",
      "Will train for 200 epochs with patience=30 monitoring val_f1\n",
      "Epoch   1/200 | Train: Loss=0.6026, F1=0.7457 | Val: Loss=0.4480, F1=0.7934\n",
      "Epoch  10/200 | Train: Loss=0.1089, F1=0.9598 | Val: Loss=0.1203, F1=0.9603\n",
      "Epoch  20/200 | Train: Loss=0.0305, F1=0.9904 | Val: Loss=0.0779, F1=0.9800\n",
      "Epoch  30/200 | Train: Loss=0.0233, F1=0.9924 | Val: Loss=0.0749, F1=0.9833\n",
      "Epoch  40/200 | Train: Loss=0.0002, F1=1.0000 | Val: Loss=0.0818, F1=0.9878\n",
      "Epoch  50/200 | Train: Loss=0.0001, F1=1.0000 | Val: Loss=0.0874, F1=0.9894\n",
      "Epoch  60/200 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.0912, F1=0.9894\n",
      "Epoch  70/200 | Train: Loss=0.0000, F1=1.0000 | Val: Loss=0.0945, F1=0.9894\n",
      "\n",
      "Early stopping triggered after 73 epochs.\n",
      "Restoring best model from epoch 43 with val_f1 0.9894\n",
      "--- Finished Training: final_check ---\n",
      "\n",
      "--- üèÜ Optimal Epochs Found: 43 ---\n",
      "Submission name will be: submission_GRU_H384_L2_BFalse_D0.4140005708856067_Optuna_FINAL.csv\n"
     ]
    }
   ],
   "source": [
    "# --- We need the original 'fit' function back ---\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    model_path = f\"models/{experiment_name}_best_model.pt\"\n",
    "\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"--- Starting Training: {experiment_name} ---\")\n",
    "    print(f\"Will train for {epochs} epochs with patience={patience} monitoring {evaluation_metric}\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
    "            )\n",
    "\n",
    "        if verbose > 0 and (epoch % verbose == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                  f\"Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n",
    "                  f\"Val: Loss={val_loss:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    if restore_best_weights and patience > 0:\n",
    "        print(f\"Restoring best model from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    if patience == 0:\n",
    "        print(\"Training complete. Saving final model.\")\n",
    "        torch.save(model.state_dict(), model_path.replace(\"_best_model.pt\", \"_final_model.pt\"))\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    \n",
    "    print(f\"--- Finished Training: {experiment_name} ---\")\n",
    "    return model, training_history, best_epoch if 'best_epoch' in locals() else epochs\n",
    "\n",
    "# --- 1. Create DataLoaders for the best config ---\n",
    "best_batch_size = FINAL_CONFIG[\"batch_size\"]\n",
    "train_loader_final_check = make_loader(train_ds, batch_size=best_batch_size, shuffle=True, drop_last=True)\n",
    "val_loader_final_check = make_loader(val_ds, batch_size=best_batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# --- 2. Instantiate Fresh Model ---\n",
    "final_check_model = RecurrentClassifier(\n",
    "    input_size=N_FEATURES,\n",
    "    hidden_size=FINAL_CONFIG[\"hidden_size\"],\n",
    "    num_layers=FINAL_CONFIG[\"num_layers\"],\n",
    "    num_classes=N_CLASSES,\n",
    "    dropout_rate=FINAL_CONFIG[\"dropout_rate\"],\n",
    "    bidirectional=FINAL_CONFIG[\"bidirectional\"],\n",
    "    rnn_type=FINAL_CONFIG[\"rnn_type\"]\n",
    ").to(device)\n",
    "\n",
    "if torch.__version__[0] >= \"2\":\n",
    "    final_check_model = torch.compile(final_check_model)\n",
    "\n",
    "final_check_optimizer = torch.optim.AdamW(final_check_model.parameters(), lr=FINAL_CONFIG[\"lr\"], weight_decay=FINAL_CONFIG[\"l2_lambda\"])\n",
    "final_check_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "final_check_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- 3. Run Training with Early Stopping ---\n",
    "print(\"--- Finding best epoch number for the winning model ---\")\n",
    "_, _, FINAL_BEST_EPOCH = fit(\n",
    "    model=final_check_model,\n",
    "    train_loader=train_loader_final_check,\n",
    "    val_loader=val_loader_final_check,\n",
    "    epochs=200, # Max epochs\n",
    "    criterion=final_check_criterion,\n",
    "    optimizer=final_check_optimizer,\n",
    "    scaler=final_check_scaler,\n",
    "    device=device,\n",
    "    writer=None, # No need to log this one\n",
    "    verbose=10,\n",
    "    experiment_name=\"final_check\",\n",
    "    patience=30 # Use a reasonable patience\n",
    ")\n",
    "\n",
    "print(f\"\\n--- üèÜ Optimal Epochs Found: {FINAL_BEST_EPOCH} ---\")\n",
    "\n",
    "# --- 4. Set variables for the submission cell ---\n",
    "# This populates the variables needed for the final submission script\n",
    "FINAL_MODEL_TYPE = FINAL_CONFIG[\"rnn_type\"]\n",
    "FINAL_HIDDEN_SIZE = FINAL_CONFIG[\"hidden_size\"]\n",
    "FINAL_HIDDEN_LAYERS = FINAL_CONFIG[\"num_layers\"]\n",
    "FINAL_BIDIRECTIONAL = FINAL_CONFIG[\"bidirectional\"]\n",
    "FINAL_DROPOUT_RATE = FINAL_CONFIG[\"dropout_rate\"]\n",
    "FINAL_LEARNING_RATE = FINAL_CONFIG[\"lr\"]\n",
    "FINAL_L2_LAMBDA = FINAL_CONFIG[\"l2_lambda\"]\n",
    "FINAL_BATCH_SIZE = FINAL_CONFIG[\"batch_size\"]\n",
    "\n",
    "FINAL_EXPERIMENT_NAME = f\"{FINAL_MODEL_TYPE}_H{FINAL_HIDDEN_SIZE}_L{FINAL_HIDDEN_LAYERS}_B{FINAL_BIDIRECTIONAL}_D{FINAL_DROPOUT_RATE}_Optuna_FINAL\"\n",
    "\n",
    "print(f\"Submission name will be: submission_{FINAL_EXPERIMENT_NAME}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f19754",
   "metadata": {
    "id": "markdown-submission"
   },
   "source": [
    "## üì¨ **8. Create Submission**\n",
    "\n",
    "Now we follow the final plan:\n",
    "1.  Create a **new, full training set** (train + val).\n",
    "2.  Re-scale the data and create a `full_train_loader`.\n",
    "3.  Instantiate a **fresh copy** of our best model (using the `FINAL_` variables).\n",
    "4.  Train it on the **full dataset** for the `FINAL_BEST_EPOCH` we just found.\n",
    "5.  Generate predictions on the test set.\n",
    "6.  Save to a unique `submission_...csv` file in the `submissions/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "acc486f2",
   "metadata": {
    "id": "code-submission"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing full dataset for final training ---\n",
      "Fitting FINAL Scaler on X_train_full_2d shape: (105760, 35)\n",
      "Final scaling complete.\n",
      "--- Applying sliding windows to final dataset ---\n",
      "Full train windowed shape: (3305, 80, 35)\n",
      "Test windowed shape: (6620, 80, 35)\n",
      "Test window indices shape: (6620,)\n",
      "Final DataLoaders created.\n",
      "\n",
      "--- Building FINAL model for submission: GRU_H384_L2_BFalse_D0.4140005708856067_Optuna_FINAL ---\n",
      "Compiling final model...\n",
      "Training final model for 43 epochs on ALL data...\n",
      "Final Training Epoch   1/43 | Train: Loss=0.5865, F1=0.7627\n",
      "Final Training Epoch   5/43 | Train: Loss=0.2152, F1=0.9265\n",
      "Final Training Epoch  10/43 | Train: Loss=0.0902, F1=0.9725\n",
      "Final Training Epoch  15/43 | Train: Loss=0.0792, F1=0.9774\n",
      "Final Training Epoch  20/43 | Train: Loss=0.0426, F1=0.9914\n",
      "Final Training Epoch  25/43 | Train: Loss=0.0327, F1=0.9932\n",
      "Final Training Epoch  30/43 | Train: Loss=0.0157, F1=0.9994\n",
      "Final Training Epoch  35/43 | Train: Loss=0.0321, F1=0.9960\n",
      "Final Training Epoch  40/43 | Train: Loss=0.0192, F1=0.9982\n",
      "Final Training Epoch  43/43 | Train: Loss=0.0140, F1=1.0000\n",
      "Final training complete.\n",
      "\n",
      "--- Generating predictions on test set (windowed) ---\n",
      "Generated 6620 predictions (from 6620 windows).\n",
      "Aggregating window predictions to sample predictions...\n",
      "Aggregated to 1324 final predictions.\n",
      "Loading sample submission file for correct formatting...\n",
      "Prediction count matches. Creating submission.\n",
      "\n",
      "Successfully saved to submissions\\submission_GRU_H384_L2_BFalse_D0.4140005708856067_Optuna_FINAL_windowed_w80_s20.csv!\n",
      "This file is correctly formatted for Kaggle:\n",
      "  sample_index    label\n",
      "0          000  no_pain\n",
      "1          001  no_pain\n",
      "2          002  no_pain\n",
      "3          003  no_pain\n",
      "4          004  no_pain\n"
     ]
    }
   ],
   "source": [
    "# --- 1. & 2. Create Full Training Set & Loader (with windows) ---\n",
    "print(\"\\n--- Preparing full dataset for final training ---\")\n",
    "\n",
    "scaler_final = StandardScaler()\n",
    "ns, ts, f = X_train_full.shape\n",
    "X_train_full_2d = X_train_full.reshape(ns * ts, f)\n",
    "\n",
    "print(f\"Fitting FINAL Scaler on X_train_full_2d shape: {X_train_full_2d.shape}\")\n",
    "scaler_final.fit(X_train_full_2d)\n",
    "\n",
    "X_train_full_scaled_2d = scaler_final.transform(X_train_full_2d)\n",
    "X_train_full_scaled = X_train_full_scaled_2d.reshape(ns, ts, f)\n",
    "\n",
    "ns_test, ts_test, f_test = X_test.shape\n",
    "X_test_2d = X_test.reshape(ns_test * ts_test, f_test)\n",
    "X_test_final_scaled_2d = scaler_final.transform(X_test_2d)\n",
    "X_test_final_scaled = X_test_final_scaled_2d.reshape(ns_test, ts_test, f_test)\n",
    "\n",
    "print(\"Final scaling complete.\")\n",
    "print(\"--- Applying sliding windows to final dataset ---\")\n",
    "\n",
    "# --- NEW: Apply windowing to the final training and test sets ---\n",
    "X_train_full_windowed, y_train_full_windowed, _ = create_sliding_windows(\n",
    "    X_train_full_scaled,\n",
    "    y_train_full,\n",
    "    window_size=NEW_WINDOW_SIZE,\n",
    "    stride=NEW_STRIDE\n",
    ")\n",
    "\n",
    "X_test_final_windowed, test_window_indices = create_sliding_windows(\n",
    "    X_test_final_scaled,\n",
    "    y=None, # No labels for the test set\n",
    "    window_size=NEW_WINDOW_SIZE,\n",
    "    stride=NEW_STRIDE\n",
    ")\n",
    "print(f\"Full train windowed shape: {X_train_full_windowed.shape}\")\n",
    "print(f\"Test windowed shape: {X_test_final_windowed.shape}\")\n",
    "print(f\"Test window indices shape: {test_window_indices.shape}\")\n",
    "\n",
    "\n",
    "# --- Create Tensors and DataLoaders from WINDOWED data ---\n",
    "full_train_features = torch.from_numpy(X_train_full_windowed).float()\n",
    "full_train_targets = torch.from_numpy(y_train_full_windowed).long()\n",
    "final_test_features = torch.from_numpy(X_test_final_windowed).float()\n",
    "\n",
    "full_train_ds = TensorDataset(full_train_features, full_train_targets)\n",
    "final_test_ds = TensorDataset(final_test_features) # No labels\n",
    "\n",
    "def make_final_loader(ds, batch_size, shuffle, drop_last):\n",
    "    return DataLoader(\n",
    "        ds, batch_size=int(batch_size), shuffle=shuffle, drop_last=drop_last,\n",
    "        num_workers=0, pin_memory=True, pin_memory_device=\"cuda\", prefetch_factor=None\n",
    "    )\n",
    "\n",
    "full_train_loader = make_final_loader(full_train_ds, batch_size=FINAL_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = make_final_loader(final_test_ds, batch_size=FINAL_BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "print(\"Final DataLoaders created.\")\n",
    "\n",
    "# --- 3. Instantiate Fresh Model (using FINAL_... vars) ---\n",
    "print(f\"\\n--- Building FINAL model for submission: {FINAL_EXPERIMENT_NAME} ---\")\n",
    "final_model = RecurrentClassifier(\n",
    "    input_size=N_FEATURES,\n",
    "    hidden_size=FINAL_HIDDEN_SIZE,\n",
    "    num_layers=FINAL_HIDDEN_LAYERS,\n",
    "    num_classes=N_CLASSES,\n",
    "    dropout_rate=FINAL_DROPOUT_RATE,\n",
    "    bidirectional=FINAL_BIDIRECTIONAL,\n",
    "    rnn_type=FINAL_MODEL_TYPE\n",
    ").to(device)\n",
    "\n",
    "if torch.__version__[0] >= \"2\":\n",
    "    print(\"Compiling final model...\")\n",
    "    final_model = torch.compile(final_model)\n",
    "\n",
    "final_optimizer = torch.optim.AdamW(final_model.parameters(), lr=FINAL_LEARNING_RATE, weight_decay=FINAL_L2_LAMBDA)\n",
    "final_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# --- 4. Train on Full Dataset ---\n",
    "print(f\"Training final model for {FINAL_BEST_EPOCH} epochs on ALL data...\")\n",
    "\n",
    "final_model.train() \n",
    "for epoch in range(1, FINAL_BEST_EPOCH + 1):\n",
    "    train_loss, train_f1 = train_one_epoch(\n",
    "        final_model, full_train_loader, final_check_criterion, final_optimizer, final_scaler, device, FINAL_L2_LAMBDA\n",
    "    )\n",
    "    if epoch % 5 == 0 or epoch == 1 or epoch == FINAL_BEST_EPOCH:\n",
    "        print(f\"Final Training Epoch {epoch:3d}/{FINAL_BEST_EPOCH} | Train: Loss={train_loss:.4f}, F1={train_f1:.4f}\")\n",
    "\n",
    "print(\"Final training complete.\")\n",
    "\n",
    "# --- 5. Generate Predictions ---\n",
    "print(\"\\n--- Generating predictions on test set (windowed) ---\")\n",
    "final_model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (inputs,) in test_loader: \n",
    "        inputs = inputs.to(device)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = final_model(inputs)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_predictions.append(preds.cpu().numpy())\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "print(f\"Generated {len(all_predictions)} predictions (from {len(test_window_indices)} windows).\")\n",
    "\n",
    "\n",
    "# --- 6. NEW: Aggregate Predictions (Majority Vote) ---\n",
    "print(\"Aggregating window predictions to sample predictions...\")\n",
    "\n",
    "# Use pandas for easy aggregation\n",
    "df_preds = pd.DataFrame({\n",
    "    'original_index': test_window_indices, # Map from window to original sample\n",
    "    'prediction': all_predictions\n",
    "})\n",
    "\n",
    "from scipy.stats import mode \n",
    "# Group by the original sample index and find the most common prediction\n",
    "# 'mode(x)[0]' gets the most frequent value (the 'mode')\n",
    "agg_preds = df_preds.groupby('original_index')['prediction'].apply(lambda x: mode(x)[0]).values\n",
    "\n",
    "# 'agg_preds' now has one prediction per original sample (length 1324)\n",
    "print(f\"Aggregated to {len(agg_preds)} final predictions.\")\n",
    "\n",
    "# Inverse transform these aggregated predictions to labels\n",
    "predicted_labels = le.inverse_transform(agg_preds)\n",
    "\n",
    "\n",
    "# --- 7. Save Submission File (This part now works) ---\n",
    "print(\"Loading sample submission file for correct formatting...\")\n",
    "X_test_long = pd.read_csv(X_TEST_PATH)\n",
    "test_sample_indices = sorted(X_test_long['sample_index'].unique())\n",
    "\n",
    "if len(predicted_labels) != len(test_sample_indices):\n",
    "    print(f\"ERROR: Prediction count mismatch! Predictions: {len(predicted_labels)}, Test Indices: {len(test_sample_indices)}\")\n",
    "else:\n",
    "    print(\"Prediction count matches. Creating submission.\")\n",
    "    \n",
    "    final_submission_df = pd.DataFrame({\n",
    "        'sample_index': test_sample_indices,\n",
    "        'label': predicted_labels \n",
    "    })\n",
    "    \n",
    "    final_submission_df['sample_index'] = final_submission_df['sample_index'].apply(lambda x: f\"{x:03d}\")\n",
    "\n",
    "    SUBMISSIONS_DIR = \"submissions\"\n",
    "    os.makedirs(SUBMISSIONS_DIR, exist_ok=True)\n",
    "    \n",
    "    submission_filename = f\"submission_{FINAL_EXPERIMENT_NAME}_windowed_w{NEW_WINDOW_SIZE}_s{NEW_STRIDE}.csv\"\n",
    "    submission_filepath = os.path.join(SUBMISSIONS_DIR, submission_filename)\n",
    "    \n",
    "    final_submission_df.to_csv(submission_filepath, index=False)\n",
    "\n",
    "    print(f\"\\nSuccessfully saved to {submission_filepath}!\")\n",
    "    print(\"This file is correctly formatted for Kaggle:\")\n",
    "    print(final_submission_df.head())\n",
    "\n",
    "del final_model, full_train_loader, test_loader, full_train_features, final_test_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an2dl-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
